<!DOCTYPE html>
<html dir="ltr" lang="en">
 <head>
  <meta charset="utf-8"/>
  <!-- <meta name="viewport" content="width=device-width"> -->
  <title>
  </title>
  <style media="screen">
   p{
                font-size: 20px;
                font-weight: bold;
                line-height: 1.5;
                font-family: 'Courier';
            }
            @media screen and (max-width: 680px) {
                html {
                    width: 100%;
                }
            }
  </style>
 </head>
 <body>
  <p>
   Data-Driven Shape Analysis and Processing
  </p>
  <p>
   Kai Xu1,2 Vladimir G. Kim3 Qixing Huang4 Niloy Mitra5 Evangelos Kalogerakis6
  </p>
  <p>
   1 National University of Defense Technology 2 Shenzhen VisuCA Key Lab / SIAT 3 Adobe Research
  </p>
  <p>
   4 Toyota Technological Institute at Chicago 5 University College London 6 University of Massachusetts Amherst
  </p>
  <p>
   Abstract
  </p>
  <p>
   Data-driven methods serve an increasingly important role in discovering
  </p>
  <p>
   geometric, structural, and semantic relationships between
  </p>
  <p>
   shapes. In contrast to traditional approaches that process shapes
  </p>
  <p>
   in isolation of each other, data-driven methods aggregate information
  </p>
  <p>
   from 3D model collections to improve the analysis, modeling
  </p>
  <p>
   and editing of shapes. Through reviewing the literature, we provide
  </p>
  <p>
   an overview of the main concepts and components of these
  </p>
  <p>
   methods, as well as discuss their application to classification, segmentation,
  </p>
  <p>
   matching, reconstruction, modeling and exploration, as
  </p>
  <p>
   well as scene analysis and synthesis. We conclude our report with
  </p>
  <p>
   ideas that can inspire future research in data-driven shape analysis
  </p>
  <p>
   and processing.
  </p>
  <p>
   Keywords: machine learning, geometry processing, geometry
  </p>
  <p>
   analysis
  </p>
  <p>
   Concepts: Computing methodologies!Image manipulation;
  </p>
  <p>
   Computational photography;
  </p>
  <p>
   1 Introduction
  </p>
  <p>
   As the availability of 3D data increases, due to the developments
  </p>
  <p>
   in both 3D sensing technology as well as 3D modeling software,
  </p>
  <p>
   data-driven approaches become increasingly applicable and useful
  </p>
  <p>
   to 3D shape processing. In contrast to traditional approaches [Levy
  </p>
  <p>
   and Zhang 2011], data-driven methods look beyond single objects,
  </p>
  <p>
   instead analyzing sets of shapes jointly to extract meaningful mappings
  </p>
  <p>
   and correlations between them. In addition, these methods are
  </p>
  <p>
   able to learn from data computational models that effectively reason
  </p>
  <p>
   about properties and relationships of shapes without relying on
  </p>
  <p>
   hard-coded rules or explicitly programmed instructions. Leveraging
  </p>
  <p>
   shared information across multiple objects, data-driven methods
  </p>
  <p>
   are able to facilitate high-level shape understanding through
  </p>
  <p>
   discovering geometric and structural patterns among collections of
  </p>
  <p>
   shapes, patterns which serve as strong priors in various geometry
  </p>
  <p>
   processing applications.
  </p>
  <p>
   The idea of utilizing data to support geometry processing has been
  </p>
  <p>
   exploited and practiced for many years. However, most existing
  </p>
  <p>
   works based on this idea are confined to the example-based
  </p>
  <p>
   paradigm, mostly leveraging only one core concept of data-driven
  </p>
  <p>
   techniques  information transfer. Typically, the input to these
  </p>
  <p>
   problems includes one or multiple exemplar shapes with some prescribed
  </p>
  <p>
   or precomputed information of interest, and a target shape
  </p>
  <p>
   that needs to be analyzed or processed. These techniques usually
  </p>
  <p>
   establish a correlation between the source and the target shapes and
  </p>
  <p>
   transfer the interesting information from the source to the target.
  </p>
  <p>
   The applications of such approaches include a variety of methods
  </p>
  <p>
   Permission to make digital or hard copies of part or all of this work for
  </p>
  <p>
   personal or classroom use is granted without fee provided that copies are
  </p>
  <p>
   not made or distributed for profit or commercial advantage and that copies
  </p>
  <p>
   bear this notice and the full citation on the first page. Copyrights for thirdparty
  </p>
  <p>
   components of this work must be honored. For all other uses, contact
  </p>
  <p>
   the owner/author(s). c
  </p>
  <p>
   2016 Copyright held by the owner/author(s).
  </p>
  <p>
   SIGGRAPH Asia 2016 Courses, July 24-28, 2016, Anaheim, CA
  </p>
  <p>
   ISBN: 978-1-4503-ABCD-E/16/07
  </p>
  <p>
   DOI: http://doi.acm.org/10.1145/9999997.9999999
  </p>
  <p>
   Data-driven
  </p>
  <p>
   shape analysis &amp;
  </p>
  <p>
   processing
  </p>
  <p>
   3D content
  </p>
  <p>
   3D sensing creation
  </p>
  <p>
   3D model database
  </p>
  <p>
   Data analysis &amp;
  </p>
  <p>
   processing
  </p>
  <p>
   Data collection
  </p>
  <p>
   Data
  </p>
  <p>
   management
  </p>
  <p>
   Machine
  </p>
  <p>
   learning
  </p>
  <p>
   Manual
  </p>
  <p>
   Segmentation
  </p>
  <p>
   Labeling
  </p>
  <p>
  </p>
  <p>
   Figure 1: Data-driven shape processing and modeling provides
  </p>
  <p>
   a promising solution to the development of big 3D data. The
  </p>
  <p>
   two major ways of 3D data generation, 3D sensing and 3D content
  </p>
  <p>
   creation, populate 3D databases with fast growing amount of 3D
  </p>
  <p>
   models. The database models are sparsely augmented with manual
  </p>
  <p>
   segmentation and labeling, as well as reasonably organized, to
  </p>
  <p>
   support data-driven shape analysis and processing, based on, e.g.,
  </p>
  <p>
   machine learning techniques. The learned knowledge can in turn
  </p>
  <p>
   support efficient 3D reconstruction and 3D content creation, during
  </p>
  <p>
   which the knowledge can be transferred to the newly generated
  </p>
  <p>
   data. Such 3D data with semantic information can be included into
  </p>
  <p>
   the database to enrich it and facilitate further data-driven applications.
  </p>
  <p>
   in shape analysis (e.g. [Schaefer and Yuksel 2007a]) and shape synthesis
  </p>
  <p>
   (e.g. [Merrell 2007; Ma et al. 2014]).
  </p>
  <p>
   As the availability of 3D data increases, several new concepts in
  </p>
  <p>
   data-driven methods are emerging, opening space for new developments
  </p>
  <p>
   in shape analysis and content creation. First, the rich variability
  </p>
  <p>
   of 3D content in existing shape repositories makes it possible
  </p>
  <p>
   to directly reuse the shapes or parts for constructing new 3D
  </p>
  <p>
   models [Funkhouser et al. 2004]. Content reuse for 3D modeling
  </p>
  <p>
   is perhaps the most straightforward application of big 3D geometric
  </p>
  <p>
   data, providing a promising approach to address the challenging
  </p>
  <p>
   3D content creation problem. Second, high-level shape understanding
  </p>
  <p>
   can benefit from co-analyzing collections of shapes. Several
  </p>
  <p>
   analysis tools demonstrate that shape analysis is more reliable if it
  </p>
  <p>
   is supported by observing certain attributes across a set of semantically
  </p>
  <p>
   related shapes instead of just focusing on a single object.
  </p>
  <p>
   Co-analysis requires a critical step of finding correlations between
  </p>
  <p>
   multiple shapes in the input set, which is substantially different
  </p>
  <p>
   from building pair-wise correlation. A key concept to co-analysis is
  </p>
  <p>
   the consistency of the correlations across the entire set, which has
  </p>
  <p>
   both semantic [Kalogerakis et al. 2010; Sidi et al. 2011;Wang et al.
  </p>
  <p>
   2012] and mathematical [Huang and Guibas 2013a] justifications.
  </p>
  <p>
   Third, aside from analyzing patterns from a set of shapes, it is also
  </p>
  <p>
   possible to endorse a subset of the shapes with some semantic information
  </p>
  <p>
   (e.g., part labeling), which can be propogated to the other
  </p>
  <p>
   shapes through learned mappings. This information propogation
  </p>
  <p>
   evolves the concept of knowledge transfer between shapes.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Relation to knowledge-driven shape processing. Prior to the
  </p>
  <p>
   emergence of data-driven techniques, high-level shape understanding
  </p>
  <p>
   and modeling was usually achieved with knowledge-driven
  </p>
  <p>
   methods. In the knowledge-driven paradigm, geometric and structural
  </p>
  <p>
   patterns are extracted and interpreted with the help of explicit
  </p>
  <p>
   rules or hand-crafted parameters. Such examples include heuristicsbased
  </p>
  <p>
   shape segmentation [Shamir 2008] and procedural shape
  </p>
  <p>
   modeling [Muller et al. 2006]. Although these approaches have
  </p>
  <p>
   certain empirical success, they exhibit several inherent limitations.
  </p>
  <p>
   First, it is extremely difficult to hard-code explicit rules and heuristics
  </p>
  <p>
   that can handle the enormous geometric and structural variability
  </p>
  <p>
   of 3D shapes and scenes in general. As a result, knowledgedriven
  </p>
  <p>
   approaches are often hard to generalize well to large and
  </p>
  <p>
   diverse shape collections. Another issue is that non-experts find it
  </p>
  <p>
   difficult to interact with knowledge-driven techniques that require
  </p>
  <p>
   as input low-level geometric parameters or instructions.
  </p>
  <p>
   In contrast to knowledge driven methods, data-driven techniques
  </p>
  <p>
   learn representations and parameters from data. They usually do
  </p>
  <p>
   not depend on hard-coded prior knowledge, and consequently do
  </p>
  <p>
   not rely on hand-crafted parameters, making these techniques more
  </p>
  <p>
   data-adaptive and thus lead to significantly improved performance
  </p>
  <p>
   in many practical settings. The success of data-driven approaches,
  </p>
  <p>
   backed by machine learning techniques, heavily relies on the accessibility
  </p>
  <p>
   of large data collections. We have witnessed the successful
  </p>
  <p>
   performance improvement of machine learning algorithms by increasing
  </p>
  <p>
   the training set size [Banko and Brill 2001]. In light of
  </p>
  <p>
   this, the recent developments in 3D modeling tools and acquisition
  </p>
  <p>
   techniques for 3D geometry, as well as availability of large repositories
  </p>
  <p>
   of 3D shapes (e.g., Trimble 3D Warehouse, Yobi3D , etc.),
  </p>
  <p>
   offer great opportunities for developing data-driven approaches for
  </p>
  <p>
   3D shape analysis and processing.
  </p>
  <p>
   Relation to structure-aware shape processing. This report is
  </p>
  <p>
   closely related to the recent survey on structure-aware shape processing
  </p>
  <p>
   by Mitra and co-workers [Mitra et al. 2014], which concentrates
  </p>
  <p>
   on techniques for structural analysis of 3D shapes, as well
  </p>
  <p>
   as high-level shape processing guided by structure-preservation. In
  </p>
  <p>
   that survey, shape structure is defined as the arrangement and relations
  </p>
  <p>
   between shape parts, which is analyzed through identifying
  </p>
  <p>
   shape parts, part parameters, and part relations. Each of the three
  </p>
  <p>
   can be determined through manual assignment, predefined model
  </p>
  <p>
   fitting and data-driven learning.
  </p>
  <p>
   In contrast, our report takes a very different perspectivewe focus
  </p>
  <p>
   on how the increasing availability of geometric data has changed
  </p>
  <p>
   the field of shape analysis and processing. In particular, we want
  </p>
  <p>
   to highlight several key distinctions: First, data-driven shape processing
  </p>
  <p>
   goes beyond structure analysis. For example, leveraging
  </p>
  <p>
   large shape collections may benefit a wider variety of problems in
  </p>
  <p>
   shape understanding and processing, such as parametric modeling
  </p>
  <p>
   of shape space [Allen et al. 2003], hypothesis generation for object
  </p>
  <p>
   and scene understanding [Zia et al. 2013; Satkin et al. 2012], and
  </p>
  <p>
   information transfer between multi-modal data [Wang et al. 2013b;
  </p>
  <p>
   Su et al. 2014]. Data-driven shape processing may also exploit the
  </p>
  <p>
   data-centered techniques in machine learning such as sparse representation
  </p>
  <p>
   [Ren and Ramanan 2013] and feature learning [Hinton
  </p>
  <p>
   et al. 2006; Bengio 2009; Yu and Ng 2010; Krizhevsky et al. 2012],
  </p>
  <p>
   which are not pre-conditioned on any domain-specific or structural
  </p>
  <p>
   prior beyond raw data. Second, even within the realm of structureaware
  </p>
  <p>
   shape processing, data-driven approaches are arguably becoming
  </p>
  <p>
   dominant due to their theoretical and practical advantages,
  </p>
  <p>
   as well as the availability of large shape repositories and recent developments
  </p>
  <p>
   in machine learning.
  </p>
  <p>
   Vision and motivation. With the emergence of big data, many
  </p>
  <p>
   scientific disciplines have shifted their focus to data-driven techniques.
  </p>
  <p>
   Although 3D geometry data is still far from being as ubiquitous
  </p>
  <p>
   as some other data formats (e.g., photographs), the rapidly
  </p>
  <p>
   growing number of 3D models, the recent developments in fusing
  </p>
  <p>
   2D and 3D data, and the invention of commodity depth sensors,
  </p>
  <p>
   have made the era of big 3D data more promising than ever. At
  </p>
  <p>
   the same time, we expect data-driven approaches to take one of the
  </p>
  <p>
   leading roles in the reconstruction and understanding of acquired
  </p>
  <p>
   3D data, as well as the synthesis of new shapes. Data-driven geometry
  </p>
  <p>
   processing will close the loop starting from acquisition, analysis,
  </p>
  <p>
   and processing all the way to the generation of 3D shapes (see
  </p>
  <p>
   Figure 1), and will be a key tool for manipulating big visual data.
  </p>
  <p>
   Recent years have witnessed a rapid development of data-driven
  </p>
  <p>
   geometry processing algorithms, both in the computer graphics and
  </p>
  <p>
   computer vision communities. Given the research efforts and wide
  </p>
  <p>
   interests in the subject, we believe many researchers would benefit
  </p>
  <p>
   from a comprehensive and systematic survey. We also hope such a
  </p>
  <p>
   survey can stimulate new theories, problems, and applications.
  </p>
  <p>
   Organization. This survey is organized as follows. Section 2
  </p>
  <p>
   gives a high-level overview of data-driven approaches and classifies
  </p>
  <p>
   data-driven methods with respect to their application domains.
  </p>
  <p>
   This section also provides two representative examples for the readers
  </p>
  <p>
   to understand the general work-flow of data-driven geometry
  </p>
  <p>
   processing. The sections following survey the various data-driven
  </p>
  <p>
   shape processing problems in detail, and try to correlate the different
  </p>
  <p>
   methods through comparisons in various aspects. Finally, we
  </p>
  <p>
   conclude our survey by discussing a few key problems involved in
  </p>
  <p>
   designing a data-driven method for shape processing, listing a set
  </p>
  <p>
   of open challenges in this direction, as well as providing a vision
  </p>
  <p>
   on future research.
  </p>
  <p>
   Accompanying online resources. In order to assist the reader
  </p>
  <p>
   in learning and leveraging the basic algorithms, we provide an online
  </p>
  <p>
   wikipage [Xu et al. 2016], which collects tools and source code,
  </p>
  <p>
   together with benchmark data for typical problems and applications
  </p>
  <p>
   of data-driven shape processing. This page will also maintain links
  </p>
  <p>
   and data mining tools for obtaining large data collections of shapes
  </p>
  <p>
   and scenes. This website could serve as a starting point for those
  </p>
  <p>
   who are conducting research in this direction. We also expect it to
  </p>
  <p>
   benefit a wide spectrum of researchers from related fields.
  </p>
  <p>
   2 Overview
  </p>
  <p>
   In this section, we provide a high-level overview of the main components
  </p>
  <p>
   and steps of data-driven approaches for processing 3D
  </p>
  <p>
   shapes and scenes. Although the pipeline of these methods vary
  </p>
  <p>
   significantly depending on their particular applications and goals,
  </p>
  <p>
   a number of components tend to be common: the input data collection
  </p>
  <p>
   and processing, data representations and feature extraction,
  </p>
  <p>
   as well as learning and inference. Representation, learning and
  </p>
  <p>
   inference are critical components of machine learning approaches
  </p>
  <p>
   in general [Koller and Friedman 2009b]. In the case of shape
  </p>
  <p>
   and scene processing, each of these components poses several interesting
  </p>
  <p>
   and unique problems when dealing with 3D geometric
  </p>
  <p>
   data. These problems have greatly motivated the research on datadriven
  </p>
  <p>
   geometry processing, and in turn have brought new challenges
  </p>
  <p>
   to the computer vision and machine learning communities,
  </p>
  <p>
   as reflected by the increased interest in 3D visual data from these
  </p>
  <p>
   fields. Below, we discuss particular characteristics and challenges
  </p>
  <p>
   of data-driven 3D shape and scene processing algorithms. Figure 2
  </p>
  <p>
   provides a schematic overview of the most common components of
  </p>
  <p>
   these algorithms.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Data collection
  </p>
  <p>
   &amp; preprocessing
  </p>
  <p>
   Feature
  </p>
  <p>
   extraction
  </p>
  <p>
   Learning
  </p>
  <p>
   Classification
  </p>
  <p>
   Regression
  </p>
  <p>
   Retrieval
  </p>
  <p>
   Segmentation
  </p>
  <p>
   Reconstruction
  </p>
  <p>
   Synthesis
  </p>
  <p>
   Processed or generated data with semantic information
  </p>
  <p>
   Clustering
  </p>
  <p>
   Inference
  </p>
  <p>
  </p>
  <p>
   Feature learning
  </p>
  <p>
   Belief propagation
  </p>
  <p>
   Bayesian inference
  </p>
  <p>
   Gibbs sampling
  </p>
  <p>
   Figure 2: The general pipeline of data-driven geometry processing contains four major stages: data collection and preprocessing, feature
  </p>
  <p>
   extraction (or feature learning), learning and inference. The inference supports many applications which would produce new shapes or
  </p>
  <p>
   scenes through reconstruction modeling or synthesis. These new data, typically possessing labels for shapes or parts, can be used to enrich
  </p>
  <p>
   the input datasets and enhance the learning tasks in future, forming a data-driven geometry processing loop.
  </p>
  <p>
   Training shapes
  </p>
  <p>
   with labeled parts Geometric feature space
  </p>
  <p>
   x1
  </p>
  <p>
   x2
  </p>
  <p>
   Test shape
  </p>
  <p>
   Test shape
  </p>
  <p>
   with labeled parts
  </p>
  <p>
   Figure 3: Pipeline of a supervised segmentation algorithm [Kalogerakis et al. 2010]. Given a set of shapes with labeled parts, the points of
  </p>
  <p>
   each shape are embedded in a common feature space based on their local geometric descriptors (a color is assigned to points depending on
  </p>
  <p>
   their given part label). A classifier is learned to split the feature space into regions corresponding to each part label. Given a test shape, its
  </p>
  <p>
   points (shown in grey) are first embedded in the same space. Then part labels are inferred for all its points based on the learned classifier
  </p>
  <p>
   and an underlying structured probabilistic model (Section 4).
  </p>
  <p>
   2.1 3D data collection
  </p>
  <p>
   Shape representation. A main component of data-driven approaches
  </p>
  <p>
   for shape and scene processing is data collection, where
  </p>
  <p>
   the goal is acquire a number of 3D shapes and scenes depending on
  </p>
  <p>
   the application. When shapes and scenes are captured with scanners
  </p>
  <p>
   or depth sensors, their initial representation is in the form of range
  </p>
  <p>
   data or unorganized point clouds. Several data-driven methods
  </p>
  <p>
   for reconstruction, segmentation and recognition directly work on
  </p>
  <p>
   these representations and do not require any further processing. On
  </p>
  <p>
   the other hand, online repositories, such as the Trimble 3D Warehouse,
  </p>
  <p>
   contain millions of shapes and scenes that are represented
  </p>
  <p>
   as polygon meshes. A large number of data-driven techniques are
  </p>
  <p>
   designed to handle complete shapes in the form of polygon meshes
  </p>
  <p>
   created by 3D modeling tools or re-constructed from point clouds.
  </p>
  <p>
   Choosing which representation to use depends on the application.
  </p>
  <p>
   For example, data-driven reconstruction techniques aim for generating
  </p>
  <p>
   complete shapes and scenes from noisy point clouds with
  </p>
  <p>
   missing data. The reconstructed shapes can then be processed with
  </p>
  <p>
   other data-driven methods for categorization, segmentation, matching
  </p>
  <p>
   and so on. Developing methods that can handle any 3D data representation,
  </p>
  <p>
   as well as jointly reconstructing and analyzing shapes
  </p>
  <p>
   is a potential direction for future research we discuss in Section 11.
  </p>
  <p>
   When polygon meshes are used as the input representation, an important
  </p>
  <p>
   aspect to consider is whether and how data-driven methods
  </p>
  <p>
   will deal with possible defects, such as non-manifold and
  </p>
  <p>
   non-orientable sets of polygons, inverted faces, isolated elements,
  </p>
  <p>
   self-intersections, holes and topological noise. The vast majority of
  </p>
  <p>
   meshes available in online repositories have these problems. Although
  </p>
  <p>
   there is a number of mesh repairing tools (see [Campen
  </p>
  <p>
   et al. 2012] for a survey), they may not handle all different types
  </p>
  <p>
   of defects, and can take a significant amount of time to process
  </p>
  <p>
   each shape in a large dataset. To avoid the issues caused by these
  </p>
  <p>
   defects, some data-driven methods uniformly sample the input
  </p>
  <p>
   meshes and work on the resulting point-based representation instead
  </p>
  <p>
   (e.g., [Chaudhuri et al. 2011a; Kim et al. 2013a]).
  </p>
  <p>
   Datasets. Although it is desirable to develop data-driven methods
  </p>
  <p>
   that can learn from a handful of training shapes or scenes, this is
  </p>
  <p>
   generally a challenging problem in machine learning [Fei-Fei et al.
  </p>
  <p>
   2006]. Several data-driven methods in computer vision have been
  </p>
  <p>
   particularly successful due to the use of very large datasets that can
  </p>
  <p>
   reach the size of several millions of images [Torralba et al. 2008].
  </p>
  <p>
   In contrast, data-driven approaches for 3D shape and scene processing
  </p>
  <p>
   approaches have mostly relied on datasets that reach the order
  </p>
  <p>
   of a few thousands so far (e.g., Princeton Shape Benchmark [Shilane
  </p>
  <p>
   et al. 2004b], or datasets collected from the web [Kim et al.
  </p>
  <p>
   2013a]). Online repositories contain large amount of shapes, which
  </p>
  <p>
   can lead to the development of methods that will leverage datasets
  </p>
  <p>
   that are orders of magnitudes larger than the ones currently used.
  </p>
  <p>
   One significant example is the recently available ShapeNet [Su et al.
  </p>
  <p>
   2015c], which provides a richly-annotated, large-scale dataset of
  </p>
  <p>
   3D shapes. Similar to ImageNet, a well-known image database in
  </p>
  <p>
   the computer vision community, ShapeNet is organized based on
  </p>
  <p>
   the WordNet hierarchy. It has indexed about 3 million models, out
  </p>
  <p>
   of which 220 thousand models are classified into 3,135 WordNet
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   synsets (a synset refers to a meaningful concept in WordNet).
  </p>
  <p>
   Another possibility is to develop synthetic datasets. A notable example
  </p>
  <p>
   is the pose and part recognition algorithm used in Microsofts
  </p>
  <p>
   Kinect that relies on 500K synthesized shapes of human bodies in
  </p>
  <p>
   different poses [Shotton et al. 2011]. In general, large datasets are
  </p>
  <p>
   important to capture the enormous 3D shape and scene variability,
  </p>
  <p>
   and can significantly increase the predictive performance and usability
  </p>
  <p>
   of learning methods. A more comprehensive summary of
  </p>
  <p>
   existing online data collections can be found on our wikipage [Xu
  </p>
  <p>
   et al. 2016].
  </p>
  <p>
   2.2 3D data processing and feature representation
  </p>
  <p>
   It is common to perform some additional processing on the input
  </p>
  <p>
   representations of shapes and scenes before executing the main
  </p>
  <p>
   learning step. The reason is that the input representations of 3D
  </p>
  <p>
   shapes and scenes can have different resolutions (e.g., number of
  </p>
  <p>
   points or faces), scale, orientation, and structure. In other words,
  </p>
  <p>
   the input shapes and scenes do not initially have any type of common
  </p>
  <p>
   parameterization or alignment. This is significantly different
  </p>
  <p>
   from other domains, such as natural language processing or vision,
  </p>
  <p>
   where text or image datasets frequently come with a common parameterization
  </p>
  <p>
   beforehand (e.g., images with the same number of
  </p>
  <p>
   pixels and objects of consistent orientation).
  </p>
  <p>
   To achieve a common parameterization of the input shapes and
  </p>
  <p>
   scenes, one popular approach is to embed them in a common geometric
  </p>
  <p>
   feature space. For this purpose a variety of shape descriptors
  </p>
  <p>
   have been developed. These descriptors can be classified into
  </p>
  <p>
   two main categories: global shape descriptors that convert each
  </p>
  <p>
   shape to a feature vector, and local shape descriptors that convert
  </p>
  <p>
   each point to a feature vector. Examples of global shape descriptors
  </p>
  <p>
   are Extended Gaussian Images [Horn 1984], 3D shape histograms
  </p>
  <p>
   [Ankerst et al. 1999; Chaudhuri and Koltun 2010], spherical functions
  </p>
  <p>
   [Saupe and Vranic 2001], lightfield descriptors [Chen et al.
  </p>
  <p>
   2003a], shape distributions [Osada et al. 2002], symmetry descriptors
  </p>
  <p>
   [Kazhdan et al. 2004a], spherical harmonics [Kazhdan et al.
  </p>
  <p>
   2003b], 3D Zernicke moments [Novotni and Klein 2003], and bagsof-
  </p>
  <p>
   words created out of local descriptors [Bronstein et al. 2011].
  </p>
  <p>
   Local shape descriptors include surface curvature, PCA descriptors,
  </p>
  <p>
   local shape diameter [Shapira et al. 2008], shape contexts [Belongie
  </p>
  <p>
   et al. 2002; Kalogerakis et al. 2010; Kokkinos et al. 2012],
  </p>
  <p>
   spin images [Johnson and Hebert 1999b], geodesic distance features
  </p>
  <p>
   [Zhang et al. 2005], heat-kernel descriptors [Bronstein et al.
  </p>
  <p>
   2011], and depth features [Shotton et al. 2011]. Global shape descriptors
  </p>
  <p>
   are particularly useful for shape classification, retrieval
  </p>
  <p>
   and organization. Local shape descriptors are useful for partial
  </p>
  <p>
   shape matching, segmentation, and point correspondence estimation.
  </p>
  <p>
   Before using any type of global or local descriptor, it is important
  </p>
  <p>
   to consider whether the descriptor will be invariant to different
  </p>
  <p>
   shape orientations, scales, or poses. In the presence of noise
  </p>
  <p>
   and irregular mesh tessellations, it is important to robustly estimate
  </p>
  <p>
   local descriptors, since surface derivatives are particularly susceptible
  </p>
  <p>
   to surface and sampling noise [Kalogerakis et al. 2007]. It is
  </p>
  <p>
   also possible to use several different descriptors as input, and let the
  </p>
  <p>
   learning step decide which ones are more relevant for each class of
  </p>
  <p>
   shapes [Kalogerakis et al. 2010].
  </p>
  <p>
   A different approach, which has attracted large attention in the computer
  </p>
  <p>
   vision community, is to avoid manually engineered features
  </p>
  <p>
   and instead directly learn features them from raw data. This approach
  </p>
  <p>
   has been enlightened by the recent developments in deep
  </p>
  <p>
   learning [Bengio 2009; Yu and Ng 2010], and in particular Convolutional
  </p>
  <p>
   Neural Networks (CNNs) [Krizhevsky et al. 2012; Szegedy
  </p>
  <p>
   et al. 2015]. A number of deep learning architectures have been recently
  </p>
  <p>
   proposed to learn 3D shape and scene descriptors, operating
  </p>
  <p>
   on either voxel-based representations [Wu et al. 2015], view-based
  </p>
  <p>
   projections [Su et al. 2015a; Xie et al. 2015], spectral representations
  </p>
  <p>
   [Boscaini et al. 2015], or RGB-D data [Socher et al. 2012;
  </p>
  <p>
   Blum et al. 2012; Lai et al. 2013; Bo et al. 2014].
  </p>
  <p>
   Instead of embedding shapes in a common geometric feature space,
  </p>
  <p>
   several methods instead try to directly align shapes in Euclidean
  </p>
  <p>
   space. We refer the reader to the survey on dynamic geometry processing
  </p>
  <p>
   for a tutorial on rigid and non-rigid registration techniques
  </p>
  <p>
   [Chang et al. 2012a]. An interesting extension of these techniques
  </p>
  <p>
   is to include the alignment process in the learning step of datadriven
  </p>
  <p>
   methods, since it is inter-dependent with other shape analysis
  </p>
  <p>
   tasks such as shape segmentation and correspondences [Kim
  </p>
  <p>
   et al. 2013a].
  </p>
  <p>
   Some data-driven methods require additional processing steps on
  </p>
  <p>
   the input. For example, learning deformation handles or fully
  </p>
  <p>
   generative models of shapes usually rely on segmenting the input
  </p>
  <p>
   shapes into parts with automatic algorithms [Huang et al. 2011; Sidi
  </p>
  <p>
   et al. 2011] and representing these parts with surface abstractions
  </p>
  <p>
   [Yumer and Kara 2012a] or descriptors [Kalogerakis et al. 2012a].
  </p>
  <p>
   To decrease the amount of computation required during learning, it
  </p>
  <p>
   is also common to represent the shapes as a set of patches (superfaces)
  </p>
  <p>
   [Huang et al. 2011] inspired by the computation of superpixels
  </p>
  <p>
   in image segmentation.
  </p>
  <p>
   2.3 Learning and Inference
  </p>
  <p>
   The processed representations of shapes and scenes are used to perform
  </p>
  <p>
   learning and inference for a variety of applications: shape
  </p>
  <p>
   classification, segmentation, matching, reconstruction, modeling,
  </p>
  <p>
   synthesis, and scene analysis. The learning procedures significantly
  </p>
  <p>
   vary depending on the application, thus we discuss them individually
  </p>
  <p>
   in each of the following sections on these applications. As
  </p>
  <p>
   a common theme, learning is viewed as an optimization problem
  </p>
  <p>
   that runs on a set of variables representing geometric, structural,
  </p>
  <p>
   semantic or functional properties of shapes and scenes. There is
  </p>
  <p>
   usually a single or multiple objective (or loss) functions for quantifying
  </p>
  <p>
   preferences for different models or patterns governing the
  </p>
  <p>
   3D data. After learning a model from the training data, inference
  </p>
  <p>
   procedures are used to predict values of variables for new shapes
  </p>
  <p>
   or scenes. Again, the inference procedures vary depending on the
  </p>
  <p>
   application, and are discussed separately in the following sections.
  </p>
  <p>
   It is common that inference itself is an optimization problem, and
  </p>
  <p>
   sometimes is part of the learning process when there are latent variables
  </p>
  <p>
   or partially observed input shapes or scene data.
  </p>
  <p>
   A general classification of the different types of algorithms used in
  </p>
  <p>
   data-driven approaches for shape and scene processing can be derived
  </p>
  <p>
   from the type of input information available during learning:
  </p>
  <p>
   Supervised learning algorithms are trained on a set of shapes
  </p>
  <p>
   or scenes annotated with labeled data. For example, in the
  </p>
  <p>
   case of shape classification, these labeled data can have the
  </p>
  <p>
   form of tags, while in the case of segmentation, the labeled
  </p>
  <p>
   data have the form of segmentation boundaries or part labels.
  </p>
  <p>
   The labeled data can be provided by humans or generated synthetically.
  </p>
  <p>
   After learning, the learned models are applied on
  </p>
  <p>
   different sets of shapes (test shapes) to produce results relevant
  </p>
  <p>
   to the task.
  </p>
  <p>
   Unsupervised algorithms co-analyze the input shapes or
  </p>
  <p>
   scenes without any additional labeled data i.e., the desired
  </p>
  <p>
   output is unknown beforehand. The goal of these methods
  </p>
  <p>
   is to discover correlations in the geometry and structure of the
  </p>
  <p>
   input shape or scene data. For example, unsupervised shape
  </p>
  <p>
   segmentation methods usually perform some type of clustering
  </p>
  <p>
   in the feature space of points or patches belonging to the
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   input shapes.
  </p>
  <p>
   Semi-supervised algorithms make use of shapes (or scenes)
  </p>
  <p>
   with and without any labeled data. Active learning is a special
  </p>
  <p>
   case of semi-supervised learning in which a learning algorithm
  </p>
  <p>
   interactively queries the user to obtain desired outputs
  </p>
  <p>
   for more data points related to shapes.
  </p>
  <p>
   In general, supervised methods tend to output results that are closer
  </p>
  <p>
   to what a human would expect given the provided labeled data.
  </p>
  <p>
   However, they may fail to produce desirable results when the training
  </p>
  <p>
   shapes (or scenes) are geometrically and structurally dissimilar
  </p>
  <p>
   from the test shapes (or scenes). They also tend to require a substantial
  </p>
  <p>
   amount of labeled information as input, which can become a
  </p>
  <p>
   significant burden for the user. Unsupervised methods can deal with
  </p>
  <p>
   collections of shapes and scenes with larger variability and require
  </p>
  <p>
   no human supervision. However, they sometimes require parameter
  </p>
  <p>
   tuning to yield the desired results. Semi-supervised methods represent
  </p>
  <p>
   a trade-off between supervised and unsupervised methods:
  </p>
  <p>
   they provide more direct control to the user about the desired result
  </p>
  <p>
   compared to unsupervised methods, and often produce considerable
  </p>
  <p>
   improvements in the results by making use of both labeled and
  </p>
  <p>
   unlabeled shapes or scenes compared to supervised methods.
  </p>
  <p>
   The data-driven loop. An advantageous feature of data-driven
  </p>
  <p>
   shape processing is that the output data, produced by learning and
  </p>
  <p>
   inference, typically come with rich semantic information. For example,
  </p>
  <p>
   data-driven shape segmentation produces parts with semantic
  </p>
  <p>
   labels [Kalogerakis et al. 2010]; data-driven reconstruction is
  </p>
  <p>
   commonly coupled with semantic part or shape recognition [Shen
  </p>
  <p>
   et al. 2012; Nan et al. 2012]; data-driven shape modeling can generate
  </p>
  <p>
   readily usable shapes inheriting the semantic information from
  </p>
  <p>
   the input data [Xu et al. 2011]. These processed and generated
  </p>
  <p>
   data can be used to enrich the existing shape collections with both
  </p>
  <p>
   training labels and reusable contents, which in turn benefit subsequent
  </p>
  <p>
   learning. In a sense, data-driven methods close the loop of
  </p>
  <p>
   data generation and data analysis for 3D shapes and scenes; see
  </p>
  <p>
   Figure 2. Such concept has been practiced in several prior works,
  </p>
  <p>
   such as the data-driven shape reconstruction framework proposed
  </p>
  <p>
   in [Pauly et al. 2005a] (Figure 12).
  </p>
  <p>
   Pipeline example. To help the reader grasp the pipeline of datadriven
  </p>
  <p>
   methods, a schematic overview of the components is given
  </p>
  <p>
   in Figure 2. Depending on the particular application, the pipeline
  </p>
  <p>
   can have several variations, or some components might be skipped.
  </p>
  <p>
   We discuss the main components and steps of algorithms for each
  </p>
  <p>
   application in more detail in the following sections. A didactic example
  </p>
  <p>
   of the pipeline in the case of supervised shape segmentation
  </p>
  <p>
   is shown in Figure 3. The input shapes are annotated with labeled
  </p>
  <p>
   part information. A geometric descriptor is extracted for each point
  </p>
  <p>
   on the training shapes, and the points are embedded in a common
  </p>
  <p>
   feature space. The learning step uses a classification algorithm that
  </p>
  <p>
   non-linearly separates the input space into a set of regions corresponding
  </p>
  <p>
   to part labels in order to optimize classification performance
  </p>
  <p>
   (more details are provided in Section 4). Given a test shape,
  </p>
  <p>
   a probabilistic model is used to infer part labels for each point on
  </p>
  <p>
   that shape based on its geometric descriptor in the feature space.
  </p>
  <p>
   2.4 A comparative overview
  </p>
  <p>
   Before reviewing the related works in detail, we provide a comparative
  </p>
  <p>
   overview of them in Table 5, and correlate them under a set of
  </p>
  <p>
   criteria:
  </p>
  <p>
   Training data. Data-driven methods can be categorized according
  </p>
  <p>
   to the shape or scene representations they operate
  </p>
  <p>
   Figure 4: Fine-grained classification of 3D models [Huang et al.
  </p>
  <p>
   2013a], where text labels are propagated from brown to blue models.
  </p>
  <p>
   on, the scale (size) of the training datasets they use, and
  </p>
  <p>
   the type of pre-processing applied to these datasets. The
  </p>
  <p>
   most common representation for shapes are polygon meshes
  </p>
  <p>
   and point clouds. 3D scenes are typically represented as
  </p>
  <p>
   an arrangement of individual shapes, usually organized in a
  </p>
  <p>
   scene graph. Pre-processing includes pre-segmentation, oversegmentation,
  </p>
  <p>
   pre-alignment, initial correspondence, or/and
  </p>
  <p>
   labeling.
  </p>
  <p>
   Features. Roughly speaking, there are two types of feature
  </p>
  <p>
   representations involved in data-driven shape processing. The
  </p>
  <p>
   most commonly used feature representations are low-level
  </p>
  <p>
   ones, such as local geometric features (e.g., local curvature)
  </p>
  <p>
   and global shape descriptors (e.g. shape distribution [Osada
  </p>
  <p>
   et al. 2002]). If the input shapes are pre-segmented into
  </p>
  <p>
   meaningful parts, high-level structural representations (spatial
  </p>
  <p>
   relationships of parts) can be derived. Generally, working
  </p>
  <p>
   with high-level feature representations enables the learning
  </p>
  <p>
   of more powerful models for more advanced inference tasks,
  </p>
  <p>
   such as structural analysis [Mitra et al. 2014], on complex
  </p>
  <p>
   man-made objects and scenes.
  </p>
  <p>
   Learning model/approach. The specific choice of learning
  </p>
  <p>
   method is often application-dependent. In most cases,
  </p>
  <p>
   machine learning techniques are adapted or developed from
  </p>
  <p>
   scratch to process geometric data. For some problems, such
  </p>
  <p>
   as shape correspondence, the core problem is to extract geometric
  </p>
  <p>
   correlations between different shapes in an unsupervised
  </p>
  <p>
   manner, which itself can be seen as a learning problem
  </p>
  <p>
   specific to geometry processing.
  </p>
  <p>
   Learning type. As discussed above, there are three basic
  </p>
  <p>
   types of data-driven methods, depending on the use of labeled
  </p>
  <p>
   training data: supervised, semi-supervised and unsupervised
  </p>
  <p>
   methods.
  </p>
  <p>
   Learning outcome. Learning can produce different types
  </p>
  <p>
   of outputs: parametric or non-parametric models (classifiers,
  </p>
  <p>
   clusterings, regressors, etc.), distance metrics which can be
  </p>
  <p>
   utilized for further analysis, and/or feature representations
  </p>
  <p>
   learned from raw data.
  </p>
  <p>
   Application. The main applications of data-driven shape
  </p>
  <p>
   analysis and processing include classification, segmentation,
  </p>
  <p>
   correspondence, modeling, synthesis, reconstruction, exploration
  </p>
  <p>
   and organization.
  </p>
  <p>
   3 Shape classification
  </p>
  <p>
   Data-driven techniques commonly make assumptions about the size
  </p>
  <p>
   and homogeneity of the input data set. In particular, existing analysis
  </p>
  <p>
   techniques often assume that all models belong to the same class
  </p>
  <p>
   of objects [Kim et al. 2013a] or scenes [Fisher et al. 2011], and
  </p>
  <p>
   cannot directly scale to entire repositories such as the Trimble 3D
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Method Input Data Shapes Classes Acc
  </p>
  <p>
   [Huang et al. 2013a] 3D Warehouse 1206-5850 26 86
  </p>
  <p>
   [Golovinskiy et al. 2009] LIDAR 1063 16 58
  </p>
  <p>
   [Shilane and Funkhouser 2007] PSB 1814 90 75
  </p>
  <p>
   [Funkhouser and Shilane 2006] PSB 1814 90 83
  </p>
  <p>
   [Barutcuoglu and DeCoro 2006] PSB 1814 90 84
  </p>
  <p>
   [Bronstein et al. 2011] SG 715 13 89
  </p>
  <p>
   [Litman et al. 2014] SG 715 13 91
  </p>
  <p>
   [Li et al. 2012] SHREC12 1200 60 88
  </p>
  <p>
   [Li et al. 2014] SHREC14 400-104 1352 87
  </p>
  <p>
   [Wu et al. 2015] ModelNet40 48,000 40 77
  </p>
  <p>
   [Su et al. 2015a] ModelNet40 48,000 40 90
  </p>
  <p>
   [Su et al. 2016] ModelNet40 48,000 40 94
  </p>
  <p>
   Table 1: Performance of several methods for shape classification
  </p>
  <p>
   (the accuracy in the right-most column as measured as fraction
  </p>
  <p>
   of correctly-labeled shapes). Huang et al. [Huang et al. 2013a]
  </p>
  <p>
   predict fine-grained tag attributes for big collections of similar
  </p>
  <p>
   shapes. Golovinskiy et al. [Golovinskiy et al. 2009] propose a
  </p>
  <p>
   method for classifying point clouds of objects in urban environments.
  </p>
  <p>
   The methods aimed at classifying meshes are evaluated on
  </p>
  <p>
   Princeton Shape Benchmark (PSB) [Shilane and Funkhouser 2007;
  </p>
  <p>
   Funkhouser and Shilane 2006; Barutcuoglu and DeCoro 2006].
  </p>
  <p>
   To evaluate performance of the method in the presence of nonrigid
  </p>
  <p>
   deformations ShapeGoogle (SG) dataset is also commonly
  </p>
  <p>
   used [Bronstein et al. 2011; Litman et al. 2014]. Several recent
  </p>
  <p>
   techniques use uniformly sampled representations (volumetric and
  </p>
  <p>
   view-based images) of 3D shapes in conjunction with neural networks
  </p>
  <p>
   [Wu et al. 2015; Su et al. 2015a; Su et al. 2016].
  </p>
  <p>
   Warehouse [Trimble 2014]. Similarly, techniques for data-driven
  </p>
  <p>
   reconstruction of indoor environments assume that the input data
  </p>
  <p>
   set only has furniture models [Nan et al. 2012], while modeling
  </p>
  <p>
   and synthesis interfaces restrict the input data to particular object
  </p>
  <p>
   or scene classes [Chaudhuri et al. 2011a; Kalogerakis et al. 2012a;
  </p>
  <p>
   Fisher et al. 2012]. Thus, as a first step these methods need to query
  </p>
  <p>
   a 3D model repository to retrieve a subset of relevant models.
  </p>
  <p>
   Most public shape repositories such as 3D Warehouse [Trimble
  </p>
  <p>
   2014] rely on the users to provide tags and names of the shapes with
  </p>
  <p>
   little additional quality control measures. As a result, the shapes
  </p>
  <p>
   are sparsely labeled with inconsistent and noisy tags. This motivates
  </p>
  <p>
   developing automatic algorithms to infer text associated with
  </p>
  <p>
   models. Existing work focuses on establishing class memberships
  </p>
  <p>
   for an entire shape (e.g. this shape is a chair), as well as inferring
  </p>
  <p>
   finer-scale attributes (e.g. this chair has a rocking leg).
  </p>
  <p>
   Classification methods assign a class membership for unlabeled
  </p>
  <p>
   shapes. One approach is to retrieve for each unlabeled shape the
  </p>
  <p>
   most similar shape from a database of 3D models with known shape
  </p>
  <p>
   classes. There has been a large number of shape descriptors proposed
  </p>
  <p>
   in recent years that can be used in such a retrieval task, and
  </p>
  <p>
   one can refer to various surveys (e.g., [Tangelder and Veltkamp
  </p>
  <p>
   2008b]) for a thorough overview and comparisons. One can further
  </p>
  <p>
   improve classification results by leveraging machine learning
  </p>
  <p>
   techniques to learn classifiers that are based on global shape descriptors
  </p>
  <p>
   [Frome et al. 2004; Golovinskiy et al. 2009]. Barutcuoglu
  </p>
  <p>
   et al. [Barutcuoglu and DeCoro 2006] demonstrate that Bayesian
  </p>
  <p>
   aggregation can be used to improve classification of shapes that are
  </p>
  <p>
   a part of a hierarchical ontology of objects. Geometry matching
  </p>
  <p>
   algorithms also facilitate distinguishing important features for classification
  </p>
  <p>
   [Funkhouser and Shilane 2006; Shilane and Funkhouser
  </p>
  <p>
   2007]. Bronstein et al.[Bronstein et al. 2011] leverage bag of
  </p>
  <p>
   features to learn powerful descriptor-space metrics for non-rigid
  </p>
  <p>
   shapes. These technique can be further improved by using sparse
  </p>
  <p>
   Figure 5: Ranking of parts with respect to dangerous attribute
  </p>
  <p>
   (image from [Chaudhuri et al. 2013])
  </p>
  <p>
   coding techniques [Litman et al. 2014] and perform well on benchmarks
  </p>
  <p>
   [Li et al. 2012; Li et al. 2014]. Motivated by the success of
  </p>
  <p>
   deep neural networks in image classification, Wu et al. [Wu et al.
  </p>
  <p>
   2015] represent a shape by a volumetric occupancy grid and train
  </p>
  <p>
   neural network to classify them. Su et al. [Su et al. 2015a] demonstrate
  </p>
  <p>
   that by rendering the shape from multiple viewpoints and analyzing
  </p>
  <p>
   the views as images enables leveraging the power of existing
  </p>
  <p>
   neural networks that were trained for image analysis. Su et al. [Su
  </p>
  <p>
   et al. 2016] further combine both shape representations by sampling
  </p>
  <p>
   volumetric occupancy grids with anisotropic view-dependent
  </p>
  <p>
   kernels. See Table 1 for a brief summary of some methods.
  </p>
  <p>
   Tag attributes often capture fine-scale attributes of shapes that
  </p>
  <p>
   belong to the same class. These attributes can include presence or
  </p>
  <p>
   absence of particular parts, object style, or comparative adjectives.
  </p>
  <p>
   Huang et al. [Huang et al. 2013a] developed a framework for propagating
  </p>
  <p>
   these attributes in a collection of partially annotated 3D models.
  </p>
  <p>
   For example, only brown models in Figure 4 were labeled, and
  </p>
  <p>
   blue models were annotated automatically. To achieve automatic labeling,
  </p>
  <p>
   they start by co-aligning all models to a canonical domain,
  </p>
  <p>
   and generate a voxel grid around the co-aligned models. For each
  </p>
  <p>
   voxel they compute local shape features, such as spin images, for
  </p>
  <p>
   each shape. Then, they learn a distance metric that best discriminates
  </p>
  <p>
   between different tags. All shapes are finally embedded in a
  </p>
  <p>
   weighted feature space where nearest neighbors are connected in a
  </p>
  <p>
   graph. A graph cut clustering is used to assign tags to unlabeled
  </p>
  <p>
   shapes. Tag attributes can also be used to describe semantics, function,
  </p>
  <p>
   or style of parts in shapes. Data-driven consistent segmentation
  </p>
  <p>
   and labeling techniques can be applied to propagate part tags
  </p>
  <p>
   across shapes (see Section 4). An alternative approach is to partition
  </p>
  <p>
   shapes into multiple sets of parts, then extract descriptors to
  </p>
  <p>
   define part similarity. A characteristic example of such an approach
  </p>
  <p>
   was demonstrated in Shapira et al. [Shapira et al. 2010]. Given the
  </p>
  <p>
   hierarchical segmentations of 3D shapes as input, part tagging was
  </p>
  <p>
   achieved by comparing local geometric features of parts as well as
  </p>
  <p>
   their context within the whole shape.
  </p>
  <p>
   While the above method works well for discrete tags, they do not
  </p>
  <p>
   capture more continuous relations, such as animal A is more dangerous
  </p>
  <p>
   than animal B. Chaudhuri et al. [Chaudhuri et al. 2013] focus
  </p>
  <p>
   on estimating ranking based on comparative adjectives. They
  </p>
  <p>
   use crowdsourcing to gather pairwise comparisons of shape parts
  </p>
  <p>
   with respect to different adjectives, and use a Support Vector Machine
  </p>
  <p>
   ranking method to predict attribute strengths from shape features
  </p>
  <p>
   for novel shape parts (Figure 5).
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Figure 6: A random forest classifier applied on depth data representing
  </p>
  <p>
   a human body shape (image from [Fossati et al. 2013])
  </p>
  <p>
   Style similarity methods have recently been proposed to classify
  </p>
  <p>
   shapes into style-related categories e.g., buildings can be classified
  </p>
  <p>
   into architectural styles, such as Gothic, Baroque, Byzantine and so
  </p>
  <p>
   on. In contrast to the previously discussed approaches that rely on
  </p>
  <p>
   generic visual similarity measures to compare shapes, these methods
  </p>
  <p>
   learn distance functions for style elements [Lun et al. 2015] or
  </p>
  <p>
   common feature spaces [Liu et al. 2015] to quantify the stylistic
  </p>
  <p>
   similarity of shapes. The methods can be used to compare the style
  </p>
  <p>
   similarity of shapes, even when these belong to different classes
  </p>
  <p>
   (e.g., chairs and lamps). To align the style similarity measures with
  </p>
  <p>
   the human perception of style, style comparisons of shapes are gathered
  </p>
  <p>
   through crowdsourcing. The learned similarity measures can
  </p>
  <p>
   be used to retrieve stylistically similar shapes to populate a scene,
  </p>
  <p>
   or associate shapes with style-related tags.
  </p>
  <p>
   While the techniques described above are suitable for retrieving and
  </p>
  <p>
   classifying shapes, a large number of applications require a more
  </p>
  <p>
   involved structural analysis to infer semantic and functional properties
  </p>
  <p>
   of shapes or their parts. The following two sections will discuss
  </p>
  <p>
   methods that perform structural analysis in collections of shapes
  </p>
  <p>
   based on segmentation and local matching.
  </p>
  <p>
   4 Shape segmentation
  </p>
  <p>
   The goal of data-driven shape segmentation is to partition the
  </p>
  <p>
   shapes of an input collection into parts, and also estimate part correspondences
  </p>
  <p>
   across these shapes. We organize the literature on shape
  </p>
  <p>
   segmentation into the following three categories: supervised segmentation,
  </p>
  <p>
   unsupervised segmentation, and semi-supervised segmentation
  </p>
  <p>
   following the main classification discussed in Section
  </p>
  <p>
   2. Table 2 summarizes representative techniques and reports their
  </p>
  <p>
   segmentation and part labeling performance based on established
  </p>
  <p>
   benchmarks. Table 3 reports characteristic running times for the
  </p>
  <p>
   same techniques.
  </p>
  <p>
   4.1 Supervised shape segmentation
  </p>
  <p>
   Classification techniques. Supervised shape segmentation is
  </p>
  <p>
   frequently formulated as a classification problem. Given a training
  </p>
  <p>
   set of shapes containing points, faces or patches that are labeled
  </p>
  <p>
   according to a part category (see Figure 3), the goal of a classifier is
  </p>
  <p>
   to identify which part category other points, faces, or patches from
  </p>
  <p>
   different shapes belong to. Supervised shape segmentation is executed
  </p>
  <p>
   in two steps: during the first step, the parameters of the classifier
  </p>
  <p>
   are learned from the training data. During the second step,
  </p>
  <p>
   the classifier is applied on new shapes. A simple linear classifier
  </p>
  <p>
   has the form:
  </p>
  <p>
   c = f(
  </p>
  <p>
   X
  </p>
  <p>
   j
  </p>
  <p>
   .j  xj) (1)
  </p>
  <p>
   where xj is a geometric feature of a point (face, or patch), such
  </p>
  <p>
   as the ones discussed in Section 2. The parameters .j serve as
  </p>
  <p>
   weights for each geometric feature. The function f is non-linear
  </p>
  <p>
   and maps to a discrete value (label), which is a part category, or
  </p>
  <p>
   to probabilities per category. In general, choosing a good set of
  </p>
  <p>
   geometric features that help predicting part labels, and employing
  </p>
  <p>
   classifiers that can discriminate the input data points correctly are
  </p>
  <p>
   important design choices. There is no rule of thumb on which is the
  </p>
  <p>
   best classifier for a problem. This depends on the underlying distribution
  </p>
  <p>
   and characteristics of the input geometric features, their
  </p>
  <p>
   dimensionality, amount of labeled data, existence of noise in the labeled
  </p>
  <p>
   data or shapes, training and test time constraints - for a related
  </p>
  <p>
   discussion on how to choose a classifier for a problem, we refer the
  </p>
  <p>
   reader to [Manning et al. 2008]. Due to the large dimensionality
  </p>
  <p>
   and complexity of geometric feature spaces, non-linear classifiers
  </p>
  <p>
   are more commonly used. For example, to segment human bodies
  </p>
  <p>
   into parts and recognize poses, the Microsofts Kinect uses a random
  </p>
  <p>
   forest classifier trained on synthetic depth images of humans
  </p>
  <p>
   of many shapes and sizes in highly varied poses sampled from a
  </p>
  <p>
   large motion capture database [Shotton et al. 2011] (Figure 6).
  </p>
  <p>
   Structured models. For computer graphics applications, it is important
  </p>
  <p>
   to segment shapes with accurate and smooth boundaries.
  </p>
  <p>
   For example, to help the user create a new shape by re-combining
  </p>
  <p>
   parts from other shapes [Funkhouser et al. 2004], irregular and
  </p>
  <p>
   noisy segmentation boundaries can cause problems in the part attachment.
  </p>
  <p>
   From this aspect, using a classifier per point/face independently
  </p>
  <p>
   is usually not enough. Thus, it is more common to
  </p>
  <p>
   formulate the shape segmentation problem as an energy minimization
  </p>
  <p>
   problem that involves a unary term assessing the consistency
  </p>
  <p>
   of each point/face with each part label, as well as a pairwise term
  </p>
  <p>
   assessing the consistency of neighboring points/faces with pairs of
  </p>
  <p>
   labels. For example, pairs of points that have low curvature (i.e.,
  </p>
  <p>
   are on flat surface) are more likely to have the same part label. This
  </p>
  <p>
   energy minimization formulation has been used in several singleshape
  </p>
  <p>
   and data-driven segmentations (unsupervised or supervised)
  </p>
  <p>
   [Katz and Tal 2003; Anguelov et al. 2005c; Shapira et al. 2010;
  </p>
  <p>
   Kalogerakis et al. 2010]. In the case of supervised segmentation
  </p>
  <p>
   [Kalogerakis et al. 2010], the energy can be written as:
  </p>
  <p>
   E(c; .) =
  </p>
  <p>
   X
  </p>
  <p>
   i
  </p>
  <p>
   Eunary(ci; xi, .1)+
  </p>
  <p>
   X
  </p>
  <p>
   i,j
  </p>
  <p>
   Epairwise(ci, cj ; yij, .2)
  </p>
  <p>
   (2)
  </p>
  <p>
   where c = {ci} is a vector of random variables representing the
  </p>
  <p>
   part label per point (or face) i, xi is its geometric feature vector,
  </p>
  <p>
   i, j are indices to points (or faces) that are considered neighbors,
  </p>
  <p>
   yij is a geometric feature vector representing dihedral angle, angle
  </p>
  <p>
   between normals, or other features, and . = {.1, .2} are the energy
  </p>
  <p>
   parameters. The important difference of supervised data-driven
  </p>
  <p>
   methods with previous single-shape segmentation methods is that
  </p>
  <p>
   the parameters . are automatically learned from the training shapes
  </p>
  <p>
   to capture complex feature space patterns per part [Anguelov et al.
  </p>
  <p>
   2005c; Kalogerakis et al. 2010]. We also note that the above energy
  </p>
  <p>
   of Equation 2, when written in an exponentiated form and normalized,
  </p>
  <p>
   can be treated as a probabilistic graphical model [Koller and
  </p>
  <p>
   Friedman 2009b], called Conditional Random Field [Lafferty et al.
  </p>
  <p>
   2001] that represents the joint probability distribution over part labels
  </p>
  <p>
   conditioned on the input features:
  </p>
  <p>
   P(c|x, y, .) = exp(.E(c; .))/Z(x, y, .) (3)
  </p>
  <p>
   where Z(x, y, .) is a normalization factor, also known as partition
  </p>
  <p>
   function. Minimizing the energy of Equation 2, or correspondingly
  </p>
  <p>
   finding the assignment c that maximizes the above probability
  </p>
  <p>
   distribution is known as a Maximum A Posteriori inference problem
  </p>
  <p>
   that can be solved in various manners, such as graph cuts, belief
  </p>
  <p>
   propagation, variational or linear programming relaxation techniques
  </p>
  <p>
   [Koller and Friedman 2009b].
  </p>
  <p>
   The parameters . can be jointly learned through maximum likelihood
  </p>
  <p>
   (ML) or maximum a posteriori (MAP) estimates [Koller and
  </p>
  <p>
   Friedman 2009b]. However, due to high computational complexity
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Segmentation Learning Type of PSB rand index (# train. L-PSB accuracy (# train. COSEG
  </p>
  <p>
   method type manual input shapes if applicable) shapes if applicable) accuracy
  </p>
  <p>
   [Kalogerakis et al. 2010] supervised labeled shapes 9.4% (19) / 14.8% (3) 95.3% (19) / 89.2% (3) 91.9% (12) / 89.0% (3)
  </p>
  <p>
   [Benhabiles et al. 2011] supervised segmented shapes 8.8% (19) / 9.7% (6) not applicable not applicable
  </p>
  <p>
   [Huang et al. 2011] unsupervised none 10.1% not applicable not applicable
  </p>
  <p>
   [Sidi et al. 2011] unsupervised none unknown unknown 87.7%
  </p>
  <p>
   [van Kaick et al. 2011a] supervised labeled shapes unknown 88.7% (12), see caption unknown
  </p>
  <p>
   [Hu et al. 2012a] unsupervised none unknown 88.5% 91.4%
  </p>
  <p>
   [Lv et al. 2012] semi-supervised labeled shapes unknown 92.3% (3) unknown
  </p>
  <p>
   [Wang et al. 2013b] supervised labeled images unknown 88.0% (19), see caption unknown
  </p>
  <p>
   [Kim et al. 2013a] semi-/unsupervised box templates unknown unknown 92.7% (semi-superv.)
  </p>
  <p>
   [Huang et al. 2014b] unsupervised none unknown unknown 90.1%
  </p>
  <p>
   [Xu et al. 2014b] supervised labeled shapes 10.0% 86.0% unknown
  </p>
  <p>
   [Xie et al. 2014] supervised labeled shapes 10.2% (19) 94.2 (19) / 88.6 (5) unknown
  </p>
  <p>
   Table 2: Performance of data-driven methods for segmentation in the Princeton Segmentation Benchmark (PSB) and COSEG datasets. Left
  </p>
  <p>
   to right: segmentation method, learning type depending on the nature of data required as input to the method, type of manual input if such
  </p>
  <p>
   required, segmentation performance expressed by the rand index metric [Chen et al. 2009], labeling accuracy [Kalogerakis et al. 2010] based
  </p>
  <p>
   on the PSB and COSEG datasets. We report the rand index segmentation error metric averaged over all classes of the PSB benchmark. The
  </p>
  <p>
   labeling accuracy is averaged over the Labeled PSB (L-PSB) benchmark excluding the Bust, Mech, and Bearing classes. The reason
  </p>
  <p>
   is that there are no clear semantic correspondences between parts in these classes, or the ground-truth segmentations do not sufficiently
  </p>
  <p>
   capture semantic parts in their shapes. We report the labeling accuracy averaged over the categories of the COSEG dataset used in [Sidi
  </p>
  <p>
   et al. 2011]. The COSEG classes iron, large chairs, large vases, tele-aliens were added later and are excluded here since most
  </p>
  <p>
   papers frequently do not report performance in those. We note that van Kaick et al. [van Kaick et al. 2011a] reported the labeling accuracy
  </p>
  <p>
   in ten of the L-PSB classes, while Wang et al. [Wang et al. 2013b] reported the labeling accuracy in seven of the L-PSB classes. The method
  </p>
  <p>
   by Kim et al. [Kim et al. 2013a] can run in either semi-supervised or unsupervised mode. In unsupervised mode, the corresponding labeling
  </p>
  <p>
   accuracy is 89.9% in the COSEG dataset on average.
  </p>
  <p>
   of ML or MAP learning and the non-linearity of classifiers used in
  </p>
  <p>
   shape segmentation, it is common to train the parameters .1 and .2
  </p>
  <p>
   of the model separately i.e., train the classifiers of the unary and
  </p>
  <p>
   pairwise term separately [Sutton and Mccallum 2005]. The exact
  </p>
  <p>
   form of the unary and pairwise terms vary across supervised shape
  </p>
  <p>
   segmentation methods: the unary term can have the form of a loglinear
  </p>
  <p>
   model [Anguelov et al. 2005c], cascade of JointBoost classifiers
  </p>
  <p>
   [Kalogerakis et al. 2010], Gentleboost [van Kaick et al. 2011a],
  </p>
  <p>
   or feedforward neural networks [Xie et al. 2014]. The pairwise term
  </p>
  <p>
   can have the form of a learned log-linear model [Anguelov et al.
  </p>
  <p>
   2005c], label-dependent GentleBoost classifier [Kalogerakis et al.
  </p>
  <p>
   2010], or a smoothness term based on dihedral angles and edge
  </p>
  <p>
   length tuned by experimentation [Shapira et al. 2010; van Kaick
  </p>
  <p>
   et al. 2011a; Xie et al. 2014]. Again the form of the unary and pairwise
  </p>
  <p>
   terms depend on the amount of training data, dimensionality
  </p>
  <p>
   and underlying distribution of geometric features used, and computational
  </p>
  <p>
   cost.
  </p>
  <p>
   Joint labeling. Instead of applying the learned probabilistic
  </p>
  <p>
   model to a single shape, an alternative approach is to find correspondences
  </p>
  <p>
   between faces of pairs of shapes, and incorporate a
  </p>
  <p>
   third inter-shape term in the energy of Equation 2 [van Kaick
  </p>
  <p>
   et al. 2011a]. The inter-shape term favors pairs of corresponding
  </p>
  <p>
   faces on different shapes to have the same label. As a result, the
  </p>
  <p>
   energy can be minimized jointly over a set of shapes to take into
  </p>
  <p>
   account any additional correspondences.
  </p>
  <p>
   Boundary learning. Instead of applying a classifier per mesh
  </p>
  <p>
   point, face or patch to predict a part label, a different approach is to
  </p>
  <p>
   predict the probability that a polygon mesh edge is a segmentation
  </p>
  <p>
   boundary [Benhabiles et al. 2011]. The problem can be formulated
  </p>
  <p>
   as a binary classifier (e.g., Adaboost) that is trained from human
  </p>
  <p>
   segmentation boundaries. The input to the classifier are geometric
  </p>
  <p>
   features of edges, such as dihedral angles, curvature, and shape
  </p>
  <p>
   diameter. The output is a probability for an edge to be a segmentation
  </p>
  <p>
   boundary. Since the predicted probabilities over the mesh do
  </p>
  <p>
   not correspond to closed smooth boundaries, a thinning and an active
  </p>
  <p>
   contour model [Kass et al. 1988] are used in post-processing to
  </p>
  <p>
   produce the final segmentations.
  </p>
  <p>
   Transductive segmentation. Another way to formulate the
  </p>
  <p>
   shape segmentation problem is to group patches on a mesh such
  </p>
  <p>
   that the segment similarity is maximized between the resulting segments
  </p>
  <p>
   and the provided segments in the training database. The segment
  </p>
  <p>
   similarity can be measured as the reconstruction cost of the
  </p>
  <p>
   resulting segment from the training ones. The grouping of patches
  </p>
  <p>
   can be solved as an integer programming problem [Xu et al. 2014b].
  </p>
  <p>
   Shape segmentation from labeled images. Instead of using labeled
  </p>
  <p>
   training shapes for supervised shape segmentation, an alternative
  </p>
  <p>
   source of training data can come in the form of segmented
  </p>
  <p>
   and labeled images, as demonstrated by Wang et al. [Wang et al.
  </p>
  <p>
   2013b]. Given an input 3D shape, this method first renders 2D binary
  </p>
  <p>
   images of it from different viewpoints. Each binary image
  </p>
  <p>
   is used to retrieve multiple segmented and labeled training images
  </p>
  <p>
   from an input database based on a bi-class Hausdorff distance measure.
  </p>
  <p>
   Each retrieved image is used to perform label transfer to
  </p>
  <p>
   the 2D shape projections. All labeled projections are then backprojected
  </p>
  <p>
   onto the input 3D model to compute a labeling probability
  </p>
  <p>
   map. The energy function for segmentation is formulated by
  </p>
  <p>
   using this probability map in the unary term expressed per face or
  </p>
  <p>
   point, while dihedral angles and Euclidean distances are used in the
  </p>
  <p>
   pairwise term.
  </p>
  <p>
   4.2 Semi-supervised shape segmentation
  </p>
  <p>
   Entropy regularization. The parameters . of Equation 2 can be
  </p>
  <p>
   learned not only from the training labeled shapes, but also from the
  </p>
  <p>
   unlabeled shapes [Lv et al. 2012]. The idea is that learning should
  </p>
  <p>
   maximize the likelihood function of the parameters over the labeled
  </p>
  <p>
   shapes, and also minimize the entropy (uncertainty) of the classifier
  </p>
  <p>
   over the unlabeled shapes (or correspondingly maximize the negative
  </p>
  <p>
   entropy). The idea is that minimizing the entropy over unlabeled
  </p>
  <p>
   shapes encourages the algorithm to find putative labelings for
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Segmentation Reported Dataset size for Reported
  </p>
  <p>
   method running times reported running times processor
  </p>
  <p>
   [Kalogerakis et al. 2010] 8h train. / 5 min test. 6 train. shapes / 1 test shape Intel Xeon E5355 2.66GHz
  </p>
  <p>
   [Benhabiles et al. 2011] 10 min train. / 1 min test. unknown for train. / 1 test shape Intel Core 2 Duo 2.99GHz
  </p>
  <p>
   [Huang et al. 2011] 32h 380 shapes unknown, 2.4 GHz
  </p>
  <p>
   [Sidi et al. 2011] 10 min 30 shapes AMD Opteron 2.4GHz
  </p>
  <p>
   [van Kaick et al. 2011a] 10h train. / few min test. 20-30 train. shapes / 1 test shape AMD Opteron 1GHz
  </p>
  <p>
   [Hu et al. 2012a] 8 min (excl. feat. extr.) 20 shapes Intel dual-core 2.93GHz
  </p>
  <p>
   [Lv et al. 2012] 7h train. / few min test. 20 shapes Intel I7 2600 3.4GHz
  </p>
  <p>
   [Wang et al. 2013b] 1.5 min (no train. step) 1 test shape unknown
  </p>
  <p>
   [Kim et al. 2013a] 11h 7442 shapes unknown
  </p>
  <p>
   [Huang et al. 2014b] 33h 8401 shapes unknown, 3.2GHZ
  </p>
  <p>
   [Xu et al. 2014b] 30 sec (no train. step) 1 test shape Intel I5 CPU
  </p>
  <p>
   [Xie et al. 2014] 15 sec train. (excl. feat. extr.) 6 train. shapes Intel Quad-Core 3.2 GHz
  </p>
  <p>
   Table 3: Running times reported for the data-driven segmentation methods of Table 2. We note that running times are reported in different
  </p>
  <p>
   dataset sizes and processors in the referenced papers, while it is frequently not specified whether the execution uses one or multiple threads
  </p>
  <p>
   or whether the running times include all the algorithm steps, such as super-face or feature extraction. Exact processor information is also
  </p>
  <p>
   frequently not provided. Thus, the reported running times of this table are only indicative and should not serve as a basis for a fair comparison.
  </p>
  <p>
   the unlabeled data [Jiao et al. 2006]. However, it is generally hard
  </p>
  <p>
   to strike a balance between the likelihood and entropy terms.
  </p>
  <p>
   Metric embedding and active learning. A more general formulation
  </p>
  <p>
   for semi-supervised segmentation was presented in [Wang
  </p>
  <p>
   et al. 2012]. Starting from a set of shapes that are co-segmented
  </p>
  <p>
   in an unsupervised manner [Sidi et al. 2011], the user interactively
  </p>
  <p>
   adds two types of constraints: must-link constraints, which specify
  </p>
  <p>
   that two patches (super-faces) should belong to the same cluster,
  </p>
  <p>
   and cannot-link constraints which specify that two patches
  </p>
  <p>
   must be in different clusters. These constraints are used to perform
  </p>
  <p>
   constrained clustering in an embedded feature space of super-faces
  </p>
  <p>
   coming from all the shapes of the input dataset. The key idea is
  </p>
  <p>
   to transform the original feature space, such that super-faces with
  </p>
  <p>
   must-link constraints come closer together to form a cluster in
  </p>
  <p>
   the embedded feature space, while super-faces with cannot-link
  </p>
  <p>
   constraints move away from each other. To minimize the effort required
  </p>
  <p>
   from the user, the method suggests to the user pairs of points
  </p>
  <p>
   in feature space that when constrained are likely to improve the
  </p>
  <p>
   co-segmentation. The suggestions involve points that are far from
  </p>
  <p>
   their cluster centers, and have a low confidence of belonging to their
  </p>
  <p>
   clusters. Yi et al. [Yi et al. 2016] propose a method for achieving
  </p>
  <p>
   accurate segmentation of a dataset. Their method balances between
  </p>
  <p>
   manual mesh labeling, automatic propagation of these annotations,
  </p>
  <p>
   and verification of manual and automatic annotations. Their optimization
  </p>
  <p>
   explicitly minimizes the expected human work time.
  </p>
  <p>
   Template fitting. A different form of partial supervision can
  </p>
  <p>
   come in the form of part-based templates. Kim et al.s method
  </p>
  <p>
   [Kim et al. 2013a] allows users to specify or refine a few templates
  </p>
  <p>
   made out of boxes representing expected parts in an input database.
  </p>
  <p>
   The boxes iteratively fit to the shapes of a collection through simultaneous
  </p>
  <p>
   alignment, surface segmentation and point-to-point correspondences
  </p>
  <p>
   estimated between each template and each input shape.
  </p>
  <p>
   Alternatively, the templates can be inferred automatically from the
  </p>
  <p>
   shapes of the input collection without human supervision based on
  </p>
  <p>
   single shape segmentation heuristics. Optionally, the user can refine
  </p>
  <p>
   and improve these estimated templates. From this aspect, Kim
  </p>
  <p>
   et al.s method can run in either a semi-supervised or unsupervised
  </p>
  <p>
   method. It was also the first method to handle segmentation and
  </p>
  <p>
   correspondences in collections with size on the order of thousands
  </p>
  <p>
   of shapes.
  </p>
  <p>
   4.3 Unsupervised segmentation
  </p>
  <p>
   Unsupervised data-driven shape segmentation techniques fall into
  </p>
  <p>
   two categories: clustering based techniques and matching based
  </p>
  <p>
   techniques. In the following, we highlight the key idea of each
  </p>
  <p>
   type of approach.
  </p>
  <p>
   Clustering based techniques are adapted from supervised techniques.
  </p>
  <p>
   They compute feature descriptors on points or faces. Clustering
  </p>
  <p>
   is performed over all points/faces over all shapes. Each
  </p>
  <p>
   resulting cluster indicates a consistent segment across the input
  </p>
  <p>
   shapes. The promise of the clustering based approach is that when
  </p>
  <p>
   the number of shapes becomes large, the sampling density in the
  </p>
  <p>
   clustering space becomes dense enough, so that certain statistical
  </p>
  <p>
   assumptions are satisfied, e.g., diffusion distances between points
  </p>
  <p>
   from different clusters is significantly larger than those between
  </p>
  <p>
   points within each cluster. When these assumptions are satisfied,
  </p>
  <p>
   clustering based approaches may produce results that are comparable
  </p>
  <p>
   to supervised techniques (c.f. [Hu et al. 2012a]) . In [Sidi et al.
  </p>
  <p>
   2011], the authors utilize spectral clustering to perform clustering.
  </p>
  <p>
   In [Hu et al. 2012a], the authors employ subspace clustering, a more
  </p>
  <p>
   advanced clustering method, to obtain improved results.
  </p>
  <p>
   Clustering methods can also be applied to shape parts. In [Xu et al.
  </p>
  <p>
   2010], the authors perform co-analysis over a set of shapes via factoring
  </p>
  <p>
   out the part scale variation by grouping the shapes into different
  </p>
  <p>
   styles, where style is defined by the anisotropic part scales of
  </p>
  <p>
   the shapes. In [van Kaick et al. 2013], the authors introduce unsupervised
  </p>
  <p>
   co-hierarchical analysis of a set of shapes. They propose
  </p>
  <p>
   a novel cluster-and-select scheme for selecting representative part
  </p>
  <p>
   hierarchies for all shapes and grouping the shapes according to the
  </p>
  <p>
   hierarchies. The method can be used to compute consistent hierarchical
  </p>
  <p>
   segmentations for the input set.
  </p>
  <p>
   Matching based methods [Golovinskiy and Funkhouser 2009b;
  </p>
  <p>
   Huang et al. 2011; Wang et al. 2013a; Huang et al. 2014b] build
  </p>
  <p>
   maps across shapes and utilize these maps to achieve consistency
  </p>
  <p>
   of segmentations. As shown in Figure 7, this strategy allows us to
  </p>
  <p>
   identify meaningful parts despite the lack of strong geometric cues
  </p>
  <p>
   on a particular shape. Likewise, the approach is able to identify coherent
  </p>
  <p>
   single parts even when the geometry of the individual shape
  </p>
  <p>
   suggests the presence of multiple segments. A challenge here is
  </p>
  <p>
   to find a suitable shape representation so that maps across diverse
  </p>
  <p>
   shapes are well-defined. In [Huang et al. 2011], Huang et al. introduce
  </p>
  <p>
   an optimization strategy that jointly optimizes shape segmentations
  </p>
  <p>
   and maps between optimized segmentations. Since the maps
  </p>
  <p>
   are defined at the part-level, this technique is suitable for hetero
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Figure 7: Comparison of single-shape segmentation (left) and
  </p>
  <p>
   joint shape segmentation (right) on models from the PSB benchmark
  </p>
  <p>
   [Chen et al. 2009]. Each segmentation on the left was produced
  </p>
  <p>
   by the top-performing algorithm in the benchmark for that
  </p>
  <p>
   shape. The segmentations on the right were produced by [Huang
  </p>
  <p>
   et al. 2011], which jointly optimized segmentations and correspondences
  </p>
  <p>
   across the entire dataset.
  </p>
  <p>
   geneous shape collections. Experimentally, it generates comparable
  </p>
  <p>
   results with supervised method [Kalogerakis et al. 2010] on the
  </p>
  <p>
   Princeton segmentation benchmark. Recently, Huang et al.[Huang
  </p>
  <p>
   et al. 2014b] formulated the same idea under the framework of functional
  </p>
  <p>
   maps [Ovsjanikov et al. 2012a] and gain improved segmentation
  </p>
  <p>
   quality and computational efficiency.
  </p>
  <p>
   5 Joint shape matching
  </p>
  <p>
   Another fundamental problem in shape analysis is shape matching,
  </p>
  <p>
   which finds relations or maps between shapes. These maps allow
  </p>
  <p>
   us to transfer information across shapes and aggregate information
  </p>
  <p>
   from a collection of shapes for a better understanding of individual
  </p>
  <p>
   shapes (e.g., detecting shared structures such as skeletons or shape
  </p>
  <p>
   parts). They also provide a powerful platform for comparing shapes
  </p>
  <p>
   (i.e., with respect to different measures and at different places). As
  </p>
  <p>
   we can see from other sections, shape maps are widely applied in
  </p>
  <p>
   shape classification and shape exploration as well.
  </p>
  <p>
   So far, most existing research in shape matching has focused on
  </p>
  <p>
   matching pairs of shapes in isolation. We refer to [van Kaick et al.
  </p>
  <p>
   2011c] for a survey and to [Leordeanu and Hebert 2005; Lipman
  </p>
  <p>
   and Funkhouser 2009; van Kaick et al. 2011c; Ovsjanikov et al.
  </p>
  <p>
   2010; Kim et al. 2011; Ovsjanikov et al. 2012a] for recent advances.
  </p>
  <p>
   Although significant progress has been made, state-of-the-art techniques
  </p>
  <p>
   are limited to shapes that are similar to each other. On the
  </p>
  <p>
   other hand, these techniques tend to be insufficient among shape
  </p>
  <p>
   collections that possess large geometric and topological variations.
  </p>
  <p>
   The availability of large shape collections offers opportunities to
  </p>
  <p>
   address this issue. Intuitively, when matching two dissimilar
  </p>
  <p>
   shapes, we may utilize intermediate shapes to transfer maps. In
  </p>
  <p>
   other words, we can build maps between similar shapes, and use
  </p>
  <p>
   the composite maps to obtain maps between less similar shapes. As
  </p>
  <p>
   we will see shortly, this intuition can be generalized to enforcing a
  </p>
  <p>
   cycle-consistency constraint; namely composite maps along cycles
  </p>
  <p>
   should be the identity map and the composite map between any two
  </p>
  <p>
   shapes is path-independent. In this section, we discuss joint shape
  </p>
  <p>
   matching techniques that take a shape collection and noisy maps as
  </p>
  <p>
   Figure 8: Joint shape matching takes as input maps computed between
  </p>
  <p>
   pairs of shapes in isolation and utilizes the cycle-consistency
  </p>
  <p>
   constraint to improve shape maps. This figure shows the result of
  </p>
  <p>
   Huang et al. [Huang et al. 2014b], which performs joint shape
  </p>
  <p>
   matching under the functional map setting.
  </p>
  <p>
   input, and output improved maps across the shape collection.
  </p>
  <p>
   5.1 Model graph and cycle-consistency
  </p>
  <p>
   To formulate the joint matching problem, we consider a model
  </p>
  <p>
   graph G = (S, E) (c.f. [Huber 2002]). The vertex set S =
  </p>
  <p>
   {S1,    , Sn)} consists of the input shapes. The edge set E characterizes
  </p>
  <p>
   the pairs of shapes that are selected for performing pair-wise
  </p>
  <p>
   matching. For small-scale datasets, we typically match all pairs
  </p>
  <p>
   of shapes. For large-scale datasets, the edge set usually connects
  </p>
  <p>
   shapes that are similar according to a pre-defined shape descriptor
  </p>
  <p>
   [Kim et al. 2012a; Huang et al. 2013a], thus generating a sparse
  </p>
  <p>
   shape graph.
  </p>
  <p>
   The key component of a joint matching algorithm is to utilize the
  </p>
  <p>
   so-called cycle-consistency constraint. In particular, if all the maps
  </p>
  <p>
   in G are correct, then composite maps along any loops should be
  </p>
  <p>
   the identity map. This is true for maps that are represented as transformations
  </p>
  <p>
   (e.g., rotations and rigid/affine transformations), or full
  </p>
  <p>
   point-wise maps that can be described as permutation matrices). We
  </p>
  <p>
   can easily modify the constraint to handle partial maps; namely,
  </p>
  <p>
   each point, when transformed along a loop, either disappears or
  </p>
  <p>
   goes back to the original point (See [Huang et al. 2014b] for details).
  </p>
  <p>
   The cycle-consistency constraint is useful because the initial maps,
  </p>
  <p>
   which are computed between pairs of shapes in isolation, are not
  </p>
  <p>
   expected to satisfy the cycle consistency constraint. On the other
  </p>
  <p>
   hand, although we do not know which maps or correspondences
  </p>
  <p>
   are incorrect, we can detect inconsistent cycles. These inconsistent
  </p>
  <p>
   cycles provide useful information for us to detect incorrect correspondences
  </p>
  <p>
   or maps, i.e., an inconsistent cycle indicates that at least
  </p>
  <p>
   one of the participating maps or correspondences is incorrect. To
  </p>
  <p>
   incorporate this observation into algorithms, one has to formulate
  </p>
  <p>
   the cycle-consistency constraint properly. Existing works in datadriven
  </p>
  <p>
   shape matching fall into two categories: combinatorial techniques
  </p>
  <p>
   and matrix recovery based techniques. The reminder of this
  </p>
  <p>
   section provides the details.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   5.2 Combinatorial techniques
  </p>
  <p>
   Spanning tree optimization. Earlier works in joint matching aim
  </p>
  <p>
   at finding a spanning tree in the model graph. In [Goldberg et al.
  </p>
  <p>
   2004; Huang et al. 2006], the authors propose to use the maximum
  </p>
  <p>
   spanning tree (MST) of the model graph. However, this strategy can
  </p>
  <p>
   easily fail since a single incorrect edge in the MST may break the
  </p>
  <p>
   entire matching result. In the seminal work [Huber 2002], Huber
  </p>
  <p>
   showed that finding the best spanning tree maximizing the number
  </p>
  <p>
   of consistent edges is NP-hard. Although finding the best spanning
  </p>
  <p>
   tree is not tractable, Huber introduced several local operations for
  </p>
  <p>
   improving the score of spanning trees. However, these approaches
  </p>
  <p>
   are generally limited to small-scale problems so that the search
  </p>
  <p>
   space can be sufficiently explored.
  </p>
  <p>
   Inconsistent cycle detection. Another line of approaches [Zach
  </p>
  <p>
   et al. 2010; Roberts et al. 2011; Nguyen et al. 2011] applies global
  </p>
  <p>
   optimization to select cycle-consistent maps. These approaches are
  </p>
  <p>
   typically formulated as solving constrained optimization problems,
  </p>
  <p>
   where objective functions encode the scores of selected maps, and
  </p>
  <p>
   constraints enforce the consistency of selected maps along cycles.
  </p>
  <p>
   The major advantage of these approaches is that the correct maps
  </p>
  <p>
   are determined globally. However, as the cycle consistency constraint
  </p>
  <p>
   needs to apportion blame along many edges on a cycle, the
  </p>
  <p>
   success of these approaches relies on the assumption that correct
  </p>
  <p>
   maps are dominant in the model graph so that the small number of
  </p>
  <p>
   bad maps can be identified through their participation in many bad
  </p>
  <p>
   cycles.
  </p>
  <p>
   MRF formulation. Joint matching may also be formulated as solving
  </p>
  <p>
   a second order Markov Random Field (or MRF) [Cho et al.
  </p>
  <p>
   2010b; Cho et al. 2010a; Crandall et al. 2011; Huang et al. 2012b].
  </p>
  <p>
   The basic idea is to sample the transformation/deformation space of
  </p>
  <p>
   each shape to obtain a candidate set of transformation/deformation
  </p>
  <p>
   samples per shape. Joint matching is then formulated as optimizing
  </p>
  <p>
   the best sample for each shape. The objective function considers
  </p>
  <p>
   initial maps. Specifically, each pair of samples from two different
  </p>
  <p>
   shapes would generate a candidate map between them. The
  </p>
  <p>
   objective function then formulates second-order potentials, where
  </p>
  <p>
   each term characterize the alignment score between these candidate
  </p>
  <p>
   maps and the initial maps [Huang et al. 2013a; Huang et al.
  </p>
  <p>
   2012b].
  </p>
  <p>
   The key challenge in the MRF formulation is generating the candidate
  </p>
  <p>
   samples for each shape. The most popular strategy is to
  </p>
  <p>
   perform uniform sampling [Crandall et al. 2011; Huang et al.
  </p>
  <p>
   2013a], which works well when the transformation space is lowdimensional.
  </p>
  <p>
   To apply the MRF formulation on high-dimensional
  </p>
  <p>
   problems, Huang et al. [Huang et al. 2012b] introduce a diffusionand-
  </p>
  <p>
   sharpening strategy. The idea is to diffuse the maps among the
  </p>
  <p>
   model graph to obtain rich samples of candidate transformations or
  </p>
  <p>
   correspondences and then perform clustering to reduce the number
  </p>
  <p>
   of candidate samples.
  </p>
  <p>
   5.3 Matrix based techniques
  </p>
  <p>
   A recent trend in map computation is to formulate joint map computation
  </p>
  <p>
   as inferring matrices [Singer and Wu 2011; Huang et al.
  </p>
  <p>
   2012b; Kim et al. 2012a; Huang and Guibas 2013a; Wang and
  </p>
  <p>
   Singer 2013; Chen et al. 2014c; Huang et al. 2014b]. The basic
  </p>
  <p>
   idea is to consider a big map collection matrix
  </p>
  <p>
   X =
  </p>
  <p>
   2
  </p>
  <p>
   664
  </p>
  <p>
   X11 X12    X1n
  </p>
  <p>
   X21 X22    X2n
  </p>
  <p>
   ...
  </p>
  <p>
   . . .
  </p>
  <p>
   ...
  </p>
  <p>
   X21       Xnn
  </p>
  <p>
   3
  </p>
  <p>
   775
  </p>
  <p>
   ,
  </p>
  <p>
   Figure 9: Comparison among various data-driven shape matching
  </p>
  <p>
   methods: optimized composite maps [Nguyen et al. 2011],
  </p>
  <p>
   fuzzy correspondences [Kim et al. 2012a], hub-and-spoke network
  </p>
  <p>
   [Huang et al. 2012b] and semidefinite programming relaxation
  </p>
  <p>
   [Huang and Guibas 2013a]. The input maps are given by
  </p>
  <p>
   blended intrinsic maps [Kim et al. 2011].
  </p>
  <p>
   where each block Xij encodes the map from shape Si to shape Sj .
  </p>
  <p>
   In this matrix representation, the cycle-consistency constraint can
  </p>
  <p>
   be equivalently described by simple properties ofX, i.e., depending
  </p>
  <p>
   on the types of maps, X is either positive semidefinite or low-rank
  </p>
  <p>
   (c.f. [Huang and Guibas 2013a; Huang et al. 2014b]). In addition,
  </p>
  <p>
   we may view the initial pair-wise maps as noisy measurements of
  </p>
  <p>
   the entries of X. Based on this perspective, we can formulate joint
  </p>
  <p>
   matching as matrix recovery from noisy measurements of its entries.
  </p>
  <p>
   Spectral techniques. The initial attempts in matrix recovery are
  </p>
  <p>
   spectral techniques and their variants [Singer and Wu 2011; Kim
  </p>
  <p>
   et al. 2012a; Wang et al. 2013a]. The basic idea is to consider the
  </p>
  <p>
   map collection Xinput that encodes initial maps in its blocks. Then,
  </p>
  <p>
   the recovered matrix is given byX = U.VT , whereU,.,V represent
  </p>
  <p>
   the singular value decomposition (or SVD) ofXinput. Various
  </p>
  <p>
   methods have added heuristics on top of this basic procedure. For
  </p>
  <p>
   example, Kim et al. [Kim et al. 2012a] use the optimized maps to
  </p>
  <p>
   recompute initial maps.
  </p>
  <p>
   This SVD strategy can be viewed as matrix recovery because X
  </p>
  <p>
   is equivalent to the optimal low-rank approximation of Xinput (with
  </p>
  <p>
   given rank) under the matrix Frobenius norm. However, as the input
  </p>
  <p>
   maps may contain outliers, employing the Frobenius norm for matrix
  </p>
  <p>
   recovery is sub-optimal. Moreover, it is hard to analyze these
  </p>
  <p>
   techniques, even in the very basic setting where maps are given by
  </p>
  <p>
   permutation matrices [Pachauri et al. 2013].
  </p>
  <p>
   Point-based maps. In a series of works, Huang and coworkers
  </p>
  <p>
   [Huang and Guibas 2013a; Chen et al. 2014c; Huang et al.
  </p>
  <p>
   2014a] consider the case of point-based maps and develop joint
  </p>
  <p>
   matching algorithms that admit theoretical guarantees. The work
  </p>
  <p>
   of [Huang and Guibas 2013a] considers the basic setting of permutation
  </p>
  <p>
   matrix maps and proves the equivalence between cycleconsistent
  </p>
  <p>
   maps and the low-rank or positive semi-definiteness of
  </p>
  <p>
   the map collection matrix. This leads to a semidefinite programming
  </p>
  <p>
   formulation for joint matching. In particular, the L1 norm
  </p>
  <p>
   is used to measure the distance between the recovered maps and
  </p>
  <p>
   the initial maps. The authors provide exact recovery conditions,
  </p>
  <p>
   which state that the ground-truth maps can be recovered if the percentage
  </p>
  <p>
   of incorrect correspondences in the input maps is below
  </p>
  <p>
   a constant. In a followup work, Chen et al. [Chen et al. 2014c]
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   extends this to partial maps and provide a better analysis in the
  </p>
  <p>
   case where incorrect correspondences in the input maps are random.
  </p>
  <p>
   The computational issue is addressed in [Huang et al. 2014a],
  </p>
  <p>
   which employs the alternating direction of multiplier methods for
  </p>
  <p>
   optimization. Figure 9 compares the SDP formulation of [Huang
  </p>
  <p>
   and Guibas 2013a] with several other data-driven shape matching
  </p>
  <p>
   methods. Note that all data-driven shape matching methods improve
  </p>
  <p>
   upon pair-wise matching, yet the SDP formulation leads to
  </p>
  <p>
   the largest improvement.
  </p>
  <p>
   Rotations and functional maps. Maps that are represented by general
  </p>
  <p>
   matrices (e.g., rotations or functional maps) can also be handled
  </p>
  <p>
   in a similar fashion. In [Wang and Singer 2013], Wang and Singer
  </p>
  <p>
   consider the case of rotations between objects. Their formulation
  </p>
  <p>
   is similar to [Huang and Guibas 2013a] but utilize an L1 Frobenius
  </p>
  <p>
   norm for measuring the distance between initial rotations and
  </p>
  <p>
   recovered rotations. Recently, Huang et al. [Huang et al. 2014b]
  </p>
  <p>
   extend this idea to functional maps. The major difference between
  </p>
  <p>
   functional maps and point-based maps or rotations is that the map
  </p>
  <p>
   collection matrix is no-longer symmetric. Thus, their method is
  </p>
  <p>
   formulated to recover low-rank matrices.
  </p>
  <p>
   5.4 Discussion and future directions
  </p>
  <p>
   The key to joint shape matching is to have a proper formulation
  </p>
  <p>
   of the cycle-consistency constraint. We have witnessed the evolution
  </p>
  <p>
   of this constraint from earlier works on combinatorial search
  </p>
  <p>
   and inconsistent cycle detection to more recent works which use
  </p>
  <p>
   spectral techniques, MRF based methods and matrix recovery techniques.
  </p>
  <p>
   In particular, matrix recovery techniques admit theoretical
  </p>
  <p>
   guarantees, providing a fundamental understanding of why joint
  </p>
  <p>
   shape matching can improve upon isolated pair-wise matching.
  </p>
  <p>
   One future direction is to integrate pair-wise matching and joint
  </p>
  <p>
   matching into one optimization problem. Since the major role of
  </p>
  <p>
   joint matching is to remove the noise presented in pair-wise matching,
  </p>
  <p>
   it makes sense to perform them together. Such unified approaches
  </p>
  <p>
   have the potential to further improve upon decomposed
  </p>
  <p>
   approaches (i.e., from pair-wise to joint). The technical challenge
  </p>
  <p>
   is to find map representations so that pair-wise matching and map
  </p>
  <p>
   consistency can be formulated in the same framework.
  </p>
  <p>
   6 Shape reconstruction
  </p>
  <p>
   Reconstructing geometric shapes from physical objects is a fundamental
  </p>
  <p>
   problem in geometry processing. The input to this problem
  </p>
  <p>
   is usually a point cloud produced by aligned range scans, which
  </p>
  <p>
   provides an observation of an object. The goal of a shape reconstruction
  </p>
  <p>
   algorithm is to convert this point cloud into a high-quality
  </p>
  <p>
   geometric model. In practice, the input point cloud data is noisy and
  </p>
  <p>
   incomplete. Thus, the key to a successful shape reconstruction algorithm
  </p>
  <p>
   is formulating appropriate shape priors. Traditional shape
  </p>
  <p>
   reconstruction algorithms usually utilize generic priors, such as surface
  </p>
  <p>
   smoothness [Diebel et al. 2006], and typically assume that the
  </p>
  <p>
   input data captures most of the objects surface. To handle higher
  </p>
  <p>
   degrees of noise and partiality of the input data, it is important to
  </p>
  <p>
   build structural shape priors.
  </p>
  <p>
   Data-driven techniques tackle this challenge by leveraging shape
  </p>
  <p>
   collections to learn strong structural priors from similar objects,
  </p>
  <p>
   and use them to reconstruct high-quality 3D models. Existing approaches
  </p>
  <p>
   fall into two categories, based on how they represent the
  </p>
  <p>
   shape priors: parametric and non-parametric. The former usually
  </p>
  <p>
   builds a low-dimensional parametric representation of the underlying
  </p>
  <p>
   shape space, learning the representation from exemplars
  </p>
  <p>
   and enforcing the parameterization when reconstructing new models.
  </p>
  <p>
   Parametric methods typically require building correspondences
  </p>
  <p>
   Figure 10: Derived from a dataset of prototypical 3D scans of
  </p>
  <p>
   faces, the morphable face model contributes to two main steps in
  </p>
  <p>
   face manipulation: (1) deriving a 3D face model from a novel image,
  </p>
  <p>
   and (2) modifying shape and texture in a natural way [Blanz
  </p>
  <p>
   and Vetter 1999].
  </p>
  <p>
   across the exemplar shapes. In contrast, non-parametric methods
  </p>
  <p>
   directly operate on the input shapes by copying and deforming existing
  </p>
  <p>
   shapes or shape parts.
  </p>
  <p>
   6.1 Parametric methods
  </p>
  <p>
   Morphable face. The morphable face model [Blanz and Vetter
  </p>
  <p>
   1999] is a representative work of parametric data-driven shape
  </p>
  <p>
   reconstruction, a technique which reconstructs 3D textured faces
  </p>
  <p>
   from photos and scans. The model is learned from a dataset of prototypical
  </p>
  <p>
   3D shapes of faces, and can then be used to derive a new
  </p>
  <p>
   3D face from a novel image. (See Figure10).
  </p>
  <p>
   In particular, the morphable face model represents the geometry of
  </p>
  <p>
   a face with a shape-vector S = (pT1
  </p>
  <p>
   ,    , pT
  </p>
  <p>
   n )T ) 2 R3n), that contains
  </p>
  <p>
   the 3D coordinates of its n vertices. Similarly, it encodes the
  </p>
  <p>
   texture of a face by a texture-vector T = (cT1
  </p>
  <p>
   , cT2
  </p>
  <p>
   ,    , cT
  </p>
  <p>
   n ) 2 R3n,
  </p>
  <p>
   that contains the RGB color values of the corresponding vertices.
  </p>
  <p>
   A morphable face model is then constructed using a database of
  </p>
  <p>
   m exemplar faces, each represented by its shape-vector Si and Ti.
  </p>
  <p>
   In [Blanz and Vetter 1999] the exemplar faces are constructed by
  </p>
  <p>
   matching a template to scanned human faces.
  </p>
  <p>
   The morphable face model uses Principal Component Analysis
  </p>
  <p>
   (PCA) to characterize the shape space. A new shape and its associated
  </p>
  <p>
   texture are given by
  </p>
  <p>
   Smod = S +
  </p>
  <p>
   mX.1
  </p>
  <p>
   i=1
  </p>
  <p>
   .isi, Tmod = T +
  </p>
  <p>
   mX.1
  </p>
  <p>
   i=1
  </p>
  <p>
   #iti,
  </p>
  <p>
   where S and T are the mean-shape and mean-texture, respectively,
  </p>
  <p>
   and si and ti are eigenvectors of the covariance matrices. .i and
  </p>
  <p>
   #i are coefficients. PCA also gives probability distributions over
  </p>
  <p>
   coefficients. The probability for coefficient .i is given by
  </p>
  <p>
   p({.i}) . exp
  </p>
  <p>
   .
  </p>
  <p>
   1
  </p>
  <p>
   2
  </p>
  <p>
   mX.1
  </p>
  <p>
   i=1
  </p>
  <p>
   (.i/$i)2
  </p>
  <p>
   !
  </p>
  <p>
   ,
  </p>
  <p>
   with $2
  </p>
  <p>
   i being the corresponding eigenvalue of the shape covariant
  </p>
  <p>
   matrix CS (the probability p({#i}) is computed in a similar way).
  </p>
  <p>
   With this morphable face model, reconstruction of textured models
  </p>
  <p>
   can be posed as a small-scale non-linear optimization problem.
  </p>
  <p>
   For example, given a 2D image of a human face Iinput, one can
  </p>
  <p>
   reconstruct the underlying textured 3D model by searching for a
  </p>
  <p>
   similar rendered face I({.i}, {#i}, p), parameterized by the shape
  </p>
  <p>
   and texture coefficients .i and #i, and the rendering parameters p
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Figure 11: Parameterizing the variation in human shapes can be
  </p>
  <p>
   used to synthesize new individuals or edit existing ones [Allen et al.
  </p>
  <p>
   2003].
  </p>
  <p>
   (e.g., camera configuration, lighting parameters). The optimization
  </p>
  <p>
   problem is formulated as minimizing a data term, which measures
  </p>
  <p>
   the distance between the input image and the rendered image, and
  </p>
  <p>
   regularization terms that are learned from exemplar faces. The success
  </p>
  <p>
   of the morphable model relies on the low-dimensionality of
  </p>
  <p>
   the solution space, and so is also applicable to several other data
  </p>
  <p>
   sets where this assumption holds, including the domain of human
  </p>
  <p>
   bodies and poses.
  </p>
  <p>
   Morphable human bodies. Allen et al. [Allen et al. 2003] generalize
  </p>
  <p>
   the morphable model to characterize human bodies (Figure
  </p>
  <p>
   11). Given a set of 250 scanned human bodies, the method first performs
  </p>
  <p>
   non-rigid registration to fit a hole-free, artist-generated mesh
  </p>
  <p>
   (template) to each of these scans. The result is a set of mutually
  </p>
  <p>
   consistent parameterized shapes based on the corresponding vertex
  </p>
  <p>
   positions originating from the template. Similar to [Blanz and
  </p>
  <p>
   Vetter 1999], the method employs PCA to characterize the shape
  </p>
  <p>
   space, which enables applications in shape exploration, synthesis
  </p>
  <p>
   and reconstruction.
  </p>
  <p>
   In addition to variations in body shape, human models also exhibit
  </p>
  <p>
   variations in poses. The SCAPE model (Shape Completion and
  </p>
  <p>
   Animation for People) [Anguelov et al. 2005a] addresses this challenge
  </p>
  <p>
   by learning two separate models of body deformation  one
  </p>
  <p>
   accounting for variations in poses and one for differences in body
  </p>
  <p>
   shapes. The pose deformation component is acquired from a set of
  </p>
  <p>
   dense 3D scans of a single person in multiple poses. A key aspect
  </p>
  <p>
   of the pose model is that it decomposes deformation into a rigid
  </p>
  <p>
   and a non-rigid component. The rigid component is modeled using
  </p>
  <p>
   a standard skeleton system. The non-rigid component, which captures
  </p>
  <p>
   remaining deformations such as flexing of the muscles, associates
  </p>
  <p>
   each triangle with a local affine transformation matrix. These
  </p>
  <p>
   transformation matrices are learned from exemplars using a joint regression
  </p>
  <p>
   model. In [Hasler et al. 2009b], Hasler et al. introduce a
  </p>
  <p>
   unified model for parameterizing both shapes and poses. The basic
  </p>
  <p>
   idea is to consider the relative transformations between all pairs of
  </p>
  <p>
   neighboring triangles. These transformation matrices allow us to
  </p>
  <p>
   reconstruct the original shape by solving a least squares problem.
  </p>
  <p>
   In this regard, each shape is encoded as a set of edge-wise transformation
  </p>
  <p>
   matrices, which are fit into the PCA framework to obtain a
  </p>
  <p>
   statistical model of human shapes. The model is further extended to
  </p>
  <p>
   estimate shapes of dressed humans from range scans [Hasler et al.
  </p>
  <p>
   2009a].
  </p>
  <p>
   Recent works on statistical human shape analysis focus on combining
  </p>
  <p>
   learned shape priors with sparse observations and special
  </p>
  <p>
   effects. In [Loper et al. 2014], the authors introduce an approach
  </p>
  <p>
   that reconstructs high-quality shapes and poses from a sparse set of
  </p>
  <p>
   markers. The success of this approach relies on learning meaningful
  </p>
  <p>
   shape priors from a database consisting of thousands of shapes.
  </p>
  <p>
   In [Tsoli et al. 2014], the authors study human breathing from acquired
  </p>
  <p>
   data.
  </p>
  <p>
   Figure 12: The data-driven shape reconstruction pipeline proposed
  </p>
  <p>
   in [Pauly et al. 2005a].
  </p>
  <p>
   Data-driven tracking. Object tracking aims to analyze dynamic
  </p>
  <p>
   shapes and/or poses of physical objects. Successful tracking techniques
  </p>
  <p>
   (e.g., [Weise et al. 2009; Weise et al. 2011; Li et al. 2013;
  </p>
  <p>
   Cao et al. 2013; Cao et al. 2014]) typically utilize parametric shape
  </p>
  <p>
   spaces. These reduced shape spaces provide shape priors that improve
  </p>
  <p>
   both the efficiency and robustness of the tracking process.
  </p>
  <p>
   The way to utilize and construct shape spaces vary in different
  </p>
  <p>
   settings, and are typically tailored to the specific problem setting.
  </p>
  <p>
   Weise et al. [Weise et al. 2009] utilize a linear PCA subspace
  </p>
  <p>
   trained with a very large set of pre-processed facial expressions.
  </p>
  <p>
   This method requires an extended training session with a careful
  </p>
  <p>
   choice of facial action units. In addition, the learned face model
  </p>
  <p>
   is actor-specific. These restrictions are partially resolved in [Li
  </p>
  <p>
   et al. 2010], which introduces an example-based blendshape optimization
  </p>
  <p>
   technique, involving only a limited number of random
  </p>
  <p>
   facial expressions. In [Weise et al. 2011], the authors combine both
  </p>
  <p>
   blendshapes and data-driven animation priors to improve the tracking
  </p>
  <p>
   performance. In a recent work, Li et al. [Li et al. 2013] employ
  </p>
  <p>
   adaptive PCA to further improve tracking performance on nuanced
  </p>
  <p>
   emotions and micro-expression. The key idea is to combine a general
  </p>
  <p>
   blendshape PCA model and a corrective PCA model that is updated
  </p>
  <p>
   on-the-fly. This corrective PCA model captures the details of
  </p>
  <p>
   the specific actor and missing deformations from the initial blendshape
  </p>
  <p>
   model. Wei et al. [Wei et al. 2016] train a neural network
  </p>
  <p>
   to predict correspondences between depth scans of humans. They
  </p>
  <p>
   demonstrate that one can train a network on synthetic data using the
  </p>
  <p>
   existing morphable human bodies.
  </p>
  <p>
   6.2 Non-parametric methods
  </p>
  <p>
   Parametric methods require canonical domains to characterize the
  </p>
  <p>
   shape space, which have so far been demonstrated within domains
  </p>
  <p>
   of organic shapes, such as body shapes or faces. In this section, we
  </p>
  <p>
   discuss another category of methods that have shown the potential
  </p>
  <p>
   to handle more diverse shape collections.
  </p>
  <p>
   Generally speaking, a non-parametric data-driven shape reconstruction
  </p>
  <p>
   method utilizes a collection of relevant shapes and combines
  </p>
  <p>
   three phases, i.e., a query phase, a transformation phase and an
  </p>
  <p>
   assembly phase. Existing methods differ in how the input shape
  </p>
  <p>
   collection is preprocessed and how these phases are performed.
  </p>
  <p>
   Example-based scan completion. Pauly et al. [Pauly et al.
  </p>
  <p>
   2005a] introduce one of the first non-parametric systems. As shown
  </p>
  <p>
   in [Pauly et al. 2005a], the method takes a point cloud and a collection
  </p>
  <p>
   of complete objects as input. The reconstruction procedure
  </p>
  <p>
   incorporates all three phases described above. The first phase determines
  </p>
  <p>
   a set of similar objects. The retrieval phase combines both
  </p>
  <p>
   text-based search and PCA signatures, which are then refined by
  </p>
  <p>
   rigid alignment. The second step performs non-rigid alignment between
  </p>
  <p>
   the retrieved shapes and the input point cloud. This step
  </p>
  <p>
   partitions the input point cloud into a set of patches, where each
  </p>
  <p>
   patch is associated with one retrieved shape (via the corresponding
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   region). The final phase merges the corresponding regions into a
  </p>
  <p>
   unified shape.
  </p>
  <p>
   Nan et al. [Nan et al. 2012] introduce a similar system for indoor
  </p>
  <p>
   scene reconstruction. Given an input point cloud of an indoor scene
  </p>
  <p>
   that consists of a set of objects with known categories, the method
  </p>
  <p>
   searches in a database of 3D models to find matched objects and
  </p>
  <p>
   then deforms them in a non-rigid manner to fit the input point cloud.
  </p>
  <p>
   Note that this method treats complete 3D objects as building blocks,
  </p>
  <p>
   so the final reconstruction does not necessarily reflect the original
  </p>
  <p>
   scene. Shao et al. [Shao et al. 2012] adopt an interactive approach
  </p>
  <p>
   to labeled segmentations of RGBD images, followed by 3D model
  </p>
  <p>
   retrieval for scene modeling. Chen et al. [Chen et al. 2014b] learn
  </p>
  <p>
   contextual relationships from a 3D scene database to further constrain
  </p>
  <p>
   the reconstruction for semantic compatibility between both
  </p>
  <p>
   objects and parts.
  </p>
  <p>
   In contrast to considering entire 3D shapes, Gal et al. [Gal et al.
  </p>
  <p>
   2007a] utilize a dictionary of local shape priors (defined as patches)
  </p>
  <p>
   for shape reconstruction. The method is mainly designed for enhancing
  </p>
  <p>
   shape features, where each region of an input point cloud
  </p>
  <p>
   is matched to a shape patch in the database. The matched shape
  </p>
  <p>
   patch is then used to enhance and rectify the local region. Recently,
  </p>
  <p>
   Mattausch et al. [Mattausch et al. 2014] introduce a patch-based
  </p>
  <p>
   reconstruction system for indoor scenes. Their method considers
  </p>
  <p>
   recognizing and fitting planar patches from point cloud data.
  </p>
  <p>
   Shen et al. [Shen et al. 2012] extends this idea for single object
  </p>
  <p>
   reconstruction by assembling object parts. Their method utilizes a
  </p>
  <p>
   collection consistently segmented 3D shapes. Given a scan of an
  </p>
  <p>
   object, the method recursively searches for parts in the collection to
  </p>
  <p>
   assemble the original object. The retrieval phase considers both the
  </p>
  <p>
   geometric similarity between the input and retrieved parts as well as
  </p>
  <p>
   part compatibility which is learned from the input shapes. Sung et
  </p>
  <p>
   al. [Sung et al. 2015] describe a framework for reliably estimating
  </p>
  <p>
   the part structure of partial scans by treating each part as a local
  </p>
  <p>
   coordinate system. Their method also utilizes symmetric properties
  </p>
  <p>
   of the target object and shape collection, providing more accurate
  </p>
  <p>
   reconstructions on their shape completion benchmark.
  </p>
  <p>
   Data-driven SLAM. Non-parametric methods have also found
  </p>
  <p>
   applications in reconstructing temporal geometric data (e.g., the
  </p>
  <p>
   output of the Kinect scanner). The Simultaneous localization and
  </p>
  <p>
   mapping (or SLAM) method is a notable technique which jointly
  </p>
  <p>
   estimates the trajectory of the scanning device along side the geometry
  </p>
  <p>
   of the environment. In this case, shape collections serve as
  </p>
  <p>
   priors for the objects in the environment, which can be used to train
  </p>
  <p>
   object detectors. For example, the SLAM++ system proposed by
  </p>
  <p>
   Salas-Moreno et al. [Salas-Moreno et al. 2013] trained domain specific
  </p>
  <p>
   object detectors from shape collections. The learned detectors
  </p>
  <p>
   are integrated inside the SLAM framework to recognize and track
  </p>
  <p>
   those objects. Similarly, Kim et al. [Kim et al. 2012b] use learned
  </p>
  <p>
   object models to reconstruct dense 3D models from a single scan
  </p>
  <p>
   of an indoor scene. More recently, Sun et al. [Song and Xiao 2014]
  </p>
  <p>
   introduced a 3D sliding window object detector with improved performance
  </p>
  <p>
   and capability extending to a broader range of objects. Li
  </p>
  <p>
   et al. [Li et al. 2015a] propose a data-assisted online reconstruction
  </p>
  <p>
   technique which allows object retrieval from a 3D shape database
  </p>
  <p>
   while simultaneously scanning an environment in real-time.
  </p>
  <p>
   Shape-driven reconstruction from images. Recently, there is
  </p>
  <p>
   a growing interest in reconstructing 3D objects directly from images
  </p>
  <p>
   (e.g., [Xu et al. 2011; Kholgade et al. 2014; Aubry et al. 2014;
  </p>
  <p>
   Su et al. 2014]). This problem introduces fundamental challenges
  </p>
  <p>
   in both querying similar objects and deforming objects/parts to fit
  </p>
  <p>
   the input object. In terms of searching similar objects, successful
  </p>
  <p>
   methods typically render objects in the database from a dense
  </p>
  <p>
   set of viewpoints and pick objects where one view is similar to
  </p>
  <p>
   the input image object. Since depth information is missing from
  </p>
  <p>
   the image object, it is important to properly regularize 3D object
  </p>
  <p>
   transformations; otherwise, a 3D object may be deformed arbitrarily
  </p>
  <p>
   even though its projection on the image domain matches the
  </p>
  <p>
   image object. Most existing techniques consider rigid transformations
  </p>
  <p>
   or user-specified deformations [Xu et al. 2011]. In a recent
  </p>
  <p>
   work, Su et al. [Su et al. 2014] propose to learn meaningful deformations
  </p>
  <p>
   of each shape from its optimal deformation to similar
  </p>
  <p>
   shapes. Huang et al. [Huang et al. 2015b] jointly analyze a large
  </p>
  <p>
   collection of images in object categories and a smaller collection of
  </p>
  <p>
   3D models to achieve simultaneous analysis and reconstruction of
  </p>
  <p>
   2D images.
  </p>
  <p>
   7 Shape modeling and synthesis
  </p>
  <p>
   So far, the creation of detailed three-dimensional content remains
  </p>
  <p>
   a tedious task which can mainly be performed by skilled artists.
  </p>
  <p>
   3D content creation has been a major bottleneck hindering the development
  </p>
  <p>
   of ubiquitous 3D graphics. Thus, providing easy-to-use
  </p>
  <p>
   tools for casual and novice users to design and create 3D models
  </p>
  <p>
   has been a key challenge in computer graphics. To address this
  </p>
  <p>
   challenge, current literature has been focused on two main directions,
  </p>
  <p>
   i.e., intelligent interfaces for interactive shape modeling and
  </p>
  <p>
   smart models for automated model synthesis. The former strives
  </p>
  <p>
   to endow modeling interfaces with a higher-level understanding of
  </p>
  <p>
   the structure and semantics of 3D shapes, allowing the interface to
  </p>
  <p>
   reason around the incomplete shapes being modeled. The latter direction
  </p>
  <p>
   focuses on developing data-driven models to synthesize new
  </p>
  <p>
   shapes automatically. The core problem is to learn generative shape
  </p>
  <p>
   models from a set of exemplars (e.g., probability distributions, fitness
  </p>
  <p>
   functions, functional constraints, etc) so that the synthesized
  </p>
  <p>
   shapes are plausible and novel. It can be seen that both of the two
  </p>
  <p>
   paradigms depend on data-driven modeling of shape structures and
  </p>
  <p>
   semantics. With the availability of large 3D shape collections, the
  </p>
  <p>
   data-driven approach becomes a promising answer to the content
  </p>
  <p>
   creation bottleneck.
  </p>
  <p>
   7.1 Interactive shape modeling and editing
  </p>
  <p>
   Interactive 3D modeling software (3DS Max, Maya, etc.) provide
  </p>
  <p>
   artists with a large set of tools for creating and editing detailed 3D
  </p>
  <p>
   models. Unfortunately, this same software is often onerous to harness
  </p>
  <p>
   for non-professional users. For casual users, more intuitive
  </p>
  <p>
   modeling interfaces with a certain machine intelligence are to be
  </p>
  <p>
   preferred. Below, we discuss such methods for assembly-based
  </p>
  <p>
   modeling and guided shape editing.
  </p>
  <p>
   Assembly-based modeling. Early works on 3D modeling based
  </p>
  <p>
   on shape sets are primarily driven by the purpose of content reuse
  </p>
  <p>
   in part-assembly based modeling approaches. The seminal work
  </p>
  <p>
   of modeling by example [Funkhouser et al. 2004] presents a pioneering
  </p>
  <p>
   system of shape modeling by searching a shape database
  </p>
  <p>
   for parts to reuse in the construction of new shapes. Kraevoy et
  </p>
  <p>
   al. [2007] describe a system for shape creation via interchanging
  </p>
  <p>
   parts between a small set of compatible shapes. Guo et al. [Guo
  </p>
  <p>
   et al. 2014] propose assembly-based creature modeling guided by a
  </p>
  <p>
   shape grammar.
  </p>
  <p>
   Beyond content reuse through database queries or hand-crafted
  </p>
  <p>
   rules, Chaudhuri and Koltun [2010] propose a data-driven technique
  </p>
  <p>
   for suggesting the modeler with shape parts that can potentially
  </p>
  <p>
   augment the current shape being built. Such part suggestions
  </p>
  <p>
   are generated through retrieving a shape from a database based on
  </p>
  <p>
   partial shape matching. Although this is a purely geometric method
  </p>
  <p>
   without accounting for the semantics of shape parts, it represents
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Figure 13: Given a library of models, a Bayesian network encoding semantic and geometric relationships among shape parts is
  </p>
  <p>
   learned [Chaudhuri et al. 2011a] (top). The modeling process (bottom) performs probabilistic inference in the learned Bayesian network to
  </p>
  <p>
   generate ranked lists of category labels and components within each category, customized for the currently assembled model.
  </p>
  <p>
   the first attempt at utilizing a shape database to augment the modeling
  </p>
  <p>
   interface. Later, Chaudhuri et al. [2011a] show that the incorporation
  </p>
  <p>
   of semantic relationships increases the relevance of presented
  </p>
  <p>
   parts. Given a repository of 3D shapes, the method learns a probabilistic
  </p>
  <p>
   graphical model encoding semantic and geometric relationships
  </p>
  <p>
   among shape parts. During modeling, inference in the learned
  </p>
  <p>
   Bayesian network is performed to produce a relevance ranking of
  </p>
  <p>
   the parts.
  </p>
  <p>
   A common limitation of the above techniques is that they do not
  </p>
  <p>
   provide a way to directly express a high-level design goal (e.g. create
  </p>
  <p>
   a cute toy). Chaudhuri et al. [2013] proposed a method that
  </p>
  <p>
   learns semantic attributes for shape parts that reflect the high-level
  </p>
  <p>
   intent people may have for creating content in a domain (e.g. adjectives
  </p>
  <p>
   such as dangerous, scary or strong) and ranks them according
  </p>
  <p>
   to the strength of each learned attribute (Figure 5). During
  </p>
  <p>
   an interactive session, the user explores and modifies the strengths
  </p>
  <p>
   of semantic attributes to generate new part assemblies.
  </p>
  <p>
   3D shape collections can supply other useful information as well,
  </p>
  <p>
   such as contextual and spatial relationships between shape parts, to
  </p>
  <p>
   enhance a variety of modeling interfaces. Xie et al. [Xie et al. 2013]
  </p>
  <p>
   propose a data-driven sketch-based 3D modeling system. In the offline
  </p>
  <p>
   learning stage, a shape database is pre-analyzed to extract the
  </p>
  <p>
   contextual information among parts. During the online stage, the
  </p>
  <p>
   user designs a 3D model by progressively sketching its parts and
  </p>
  <p>
   retrieving and assembling shape parts from the database. Both the
  </p>
  <p>
   retrieval and assembly are assisted by precomputed contextual information
  </p>
  <p>
   so that more relevant parts can be returned and selected
  </p>
  <p>
   parts can be automatically placed. Inspired by the ShadowDraw
  </p>
  <p>
   system [Lee et al. 2011], Fan et al. [Fan et al. 2013] propose 3D
  </p>
  <p>
   modeling by drawing with data-driven shadow guidance. The users
  </p>
  <p>
   strokes are used to query a 3D shape database for generating the
  </p>
  <p>
   shadow image, which in turn can guide the users drawing. Along
  </p>
  <p>
   the drawing, 3D candidate parts are retrieved for assembly-based
  </p>
  <p>
   modeling. Starting from a collection of expertly-created, fabricable
  </p>
  <p>
   3D models, Schulz et al. [Schulz et al. 2014] extract parameterized
  </p>
  <p>
   design templates encoding all information necessary for fabrication.
  </p>
  <p>
   The templates can then be used to generate new fabricable
  </p>
  <p>
   models in an interactive design system.
  </p>
  <p>
   Shape editing. The general idea of data-driven shape editing is
  </p>
  <p>
   to learn a model from a collection of closely related shapes that
  </p>
  <p>
   characterizes the plausible variations or deformations of the shapes
  </p>
  <p>
   in this collection. In this way, the learned model can be used to
  </p>
  <p>
   constrain a users edit to maintain plausibility. For organic shapes,
  </p>
  <p>
   such as human faces [Blanz and Vetter 1999; Chen et al. 2014a] or
  </p>
  <p>
   bodies [Allen et al. 2003], parametric models can be learned from
  </p>
  <p>
   a shape set characterizing its shape space. Such parametric models
  </p>
  <p>
   can be used to edit the shapes through exploring the shape space
  </p>
  <p>
   with the set of parameters.
  </p>
  <p>
   Figure 14: Given a hundred training airplanes (in green), the probabilistic
  </p>
  <p>
   model from [Kalogerakis et al. 2012a] synthesizes several
  </p>
  <p>
   hundreds of new airplanes (in blue).
  </p>
  <p>
   An alternative widely-adopted approach is the analyze-and-edit
  </p>
  <p>
   paradigm. This technique first extracts the structure of the input
  </p>
  <p>
   shape, and then uses this structure to constrain the editing phase
  </p>
  <p>
   to be more tenable [Gal et al. 2009]. Instead of learning structure
  </p>
  <p>
   from a single shape, Fish et al. [Fish et al. 2014] learn it from a set
  </p>
  <p>
   of shapes that belong to the same family, resulting in a set of probability
  </p>
  <p>
   distributions characterizing the part arrangements. These distributions
  </p>
  <p>
   can be used to guide structure-preserving editing, where
  </p>
  <p>
   models can be edited while maintaining their familial traits. Yumer
  </p>
  <p>
   et al. [Yumer and Kara 2014] extract co-constrained handles from
  </p>
  <p>
   a set of shapes for shape deformation. The handles are generated
  </p>
  <p>
   based on co-abstraction [Yumer and Kara 2012a] of the set of
  </p>
  <p>
   shapes and the deformation co-constraints are learned statistically
  </p>
  <p>
   from the set. The deformation handles can also be controlled with
  </p>
  <p>
   continuous semantic attributes [Yumer et al. 2015]. This approach
  </p>
  <p>
   also benefit from deep neural network trained to predict appropriate
  </p>
  <p>
   deformations for 3D models [Yumer and Mitra 2016].
  </p>
  <p>
   Based on learned structures from a database of 3D models, Xu et
  </p>
  <p>
   al. [Xu et al. 2011] propose photo-inspired 3D object modeling.
  </p>
  <p>
   Guided by the object in a photograph, the method creates a 3D
  </p>
  <p>
   model as a geometric variation of a candidate model retrieved from
  </p>
  <p>
   the database. Due to the pre-analyzed structural information, the
  </p>
  <p>
   method addresses the ill-posed problem of 3D modeling from a single
  </p>
  <p>
   2D image via structure-preserving 3D warping. The final result
  </p>
  <p>
   is structurally plausible and is readily usable for subsequent editing.
  </p>
  <p>
   Moreover, the resulting 3D model, although built from a single
  </p>
  <p>
   view, is structurally coherent from all views.
  </p>
  <p>
   7.2 Automated synthesis of shapes
  </p>
  <p>
   Many applications such as 3D games and films require large collections
  </p>
  <p>
   of 3D shapes for populating their environments. Modeling
  </p>
  <p>
   each shape individually can be tedious even with the best interactive
  </p>
  <p>
   tools. The goal of data-driven shape synthesis algorithms is to
  </p>
  <p>
   generate several shapes automatically with no or very little user su
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   pervision: users may only provide some preferences or high-level
  </p>
  <p>
   specifications to control the shape synthesis process. Existing methods
  </p>
  <p>
   achieve this task by using probabilistic generative models of
  </p>
  <p>
   shapes, evolutionary methods, or learned probabilistic grammars.
  </p>
  <p>
   Statistical models of shapes. The basic idea of these methods
  </p>
  <p>
   is to define a parametric shape space and then fit a probability distribution
  </p>
  <p>
   to the data points that represent the input exemplar shapes.
  </p>
  <p>
   Since the input shapes are assumed to be plausible and desired representatives
  </p>
  <p>
   of the shape space, high-probability areas of the shape
  </p>
  <p>
   space which tend to become associated with new, plausible shape
  </p>
  <p>
   variants. This idea was first explored in the context of parametric
  </p>
  <p>
   models [Blanz and Vetter 1999; Allen et al. 2003], discussed in
  </p>
  <p>
   Section 6. By associating each principal component of the shape
  </p>
  <p>
   space defined by these methods with a Gaussian distribution, this
  </p>
  <p>
   distribution can be sampled to generate new human faces or bodies
  </p>
  <p>
   (Figure 11). Since the probability distribution of plausible shapes
  </p>
  <p>
   tends to be highly non-uniform in several shape classes, Talton et
  </p>
  <p>
   al. [Talton et al. 2009a] use kernel density estimation with Gaussian
  </p>
  <p>
   kernels to represent plausible shape variability. The method is able
  </p>
  <p>
   to generate new shapes for tree and human body parametric spaces.
  </p>
  <p>
   Shapes have structure i.e., shapes vary in terms of their type and
  </p>
  <p>
   style, different shape styles have different number and type of parts,
  </p>
  <p>
   parts have various sub-parts that can be made of patches, and so
  </p>
  <p>
   on. Thus, to generate shapes in complex domains, it is important
  </p>
  <p>
   to define shape spaces over structural and geometric parameters,
  </p>
  <p>
   and capture hierarchical relationships between these parameters at
  </p>
  <p>
   different levels. Kalogerakis et al. [Kalogerakis et al. 2012a] (Figure
  </p>
  <p>
   14) proposed a probabilistic model that represents variation and
  </p>
  <p>
   relationships of geometric descriptors and adjacency features for
  </p>
  <p>
   different part styles, as well as variation and relationships of part
  </p>
  <p>
   styles and repetitions for different shape styles. The method learns
  </p>
  <p>
   the model from a set of consistently segmented shapes. Part and
  </p>
  <p>
   shape styles are discovered based on latent variables that capture the
  </p>
  <p>
   underlying modes of shape variability. The method uses a search
  </p>
  <p>
   procedure to assemble new shapes from parts of the input shapes
  </p>
  <p>
   according to the learned probability distribution. Users can also
  </p>
  <p>
   set preferences for generating shapes from a particular shape style,
  </p>
  <p>
   with given part styles or specific parts. Instead of relying on presegmented
  </p>
  <p>
   shapes and high-level part descriptors to encode shape
  </p>
  <p>
   variability, Huang et al. [Huang et al. 2015a] propose a probabilistic
  </p>
  <p>
   model that jointly estimates shape segmentation, surface correspondences,
  </p>
  <p>
   and surface descriptors from an input shape dataset. A
  </p>
  <p>
   deep learning procedure was used to capture hierarchical relationships
  </p>
  <p>
   of corresponding surface point positions and parts as well as
  </p>
  <p>
   their existence in the input shapes. Their probabilistic model can be
  </p>
  <p>
   sampled to directly generate point-sampled surface geometry and
  </p>
  <p>
   shape structure.
  </p>
  <p>
   Set evolution. Xu et al. [Xu et al. 2012] developed a method for
  </p>
  <p>
   generating shapes inspired by the theory of evolution in biology.
  </p>
  <p>
   The basic idea of set evolution is to define cross-over and mutation
  </p>
  <p>
   operators on shapes to perform part warping and part replacement.
  </p>
  <p>
   Starting from an initial generation of shapes with part correspondences
  </p>
  <p>
   and built-in structural information such as inter-part symmetries,
  </p>
  <p>
   these operators are applied to create a new generation of
  </p>
  <p>
   shapes. A selected subset from the generation is presented via a
  </p>
  <p>
   gallery to the user who provides feedback to the system by rating
  </p>
  <p>
   them. The ratings are used to define the fitness function for the evolution.
  </p>
  <p>
   Through the evolution, the set is personalized and populated
  </p>
  <p>
   with shapes that better fit to the user. At the same time, the system
  </p>
  <p>
   explicitly maintains the diversity of the population so as to prevent
  </p>
  <p>
   it from converging into an elite set.
  </p>
  <p>
   Figure 15: Scene comparisons may yield different similarity distances
  </p>
  <p>
   (left) depending on the focal points [Xu et al. 2014a].
  </p>
  <p>
   Learned shape grammars. Talton et al. [Talton et al. 2012]
  </p>
  <p>
   leverage techniques from natural language processing to learn probabilistic
  </p>
  <p>
   generative grammars of shapes. The method takes as input
  </p>
  <p>
   a set of exemplar shapes represented with a scene graph specifying
  </p>
  <p>
   parent/child relationships and relative transformations between labeled
  </p>
  <p>
   shape components. They use Bayesian inference to learn a
  </p>
  <p>
   probabilistic formal grammar that can be used to synthesize novel
  </p>
  <p>
   shapes.
  </p>
  <p>
   8 Scene analysis and synthesis
  </p>
  <p>
   Analyzing and modeling indoor and outdoor environments has important
  </p>
  <p>
   applications in various domains. For example, in robotics
  </p>
  <p>
   it is desirable for an autonomous agent to understand the semantics
  </p>
  <p>
   of 3D environments to be able to interact with them. In urban
  </p>
  <p>
   planning and architecture, professionals build digital models of
  </p>
  <p>
   cities and buildings to validate and improve their designs. In computer
  </p>
  <p>
   graphics, artists create novel 3D scenes for movies and video
  </p>
  <p>
   games.
  </p>
  <p>
   The fast growing number of 3D scenes in digital repositories provide
  </p>
  <p>
   new opportunities for data-driven scene analysis, editing, and
  </p>
  <p>
   synthesis. Emerging collections of 3D scenes pose novel research
  </p>
  <p>
   challenges that cannot be easily addressed with existing tools. In
  </p>
  <p>
   particular, representations created for analyzing collections of single
  </p>
  <p>
   models mostly focus on arrangement and relations between
  </p>
  <p>
   shape parts [Mitra et al. 2014], which usually exhibit less variations
  </p>
  <p>
   than objects in scenes. Capturing scene structure poses a greater
  </p>
  <p>
   challenge due to looser spatial relations and a more diverse mixture
  </p>
  <p>
   of functional substructures.
  </p>
  <p>
   Inferring scene semantics is a long-standing problem in image
  </p>
  <p>
   understanding, with many methods developed for object recognition
  </p>
  <p>
   [Quattoni and Torralba 2009], classification [Swadzba and
  </p>
  <p>
   Wachsmuth 2010], layout and structure reasoning [Choi et al. 2013;
  </p>
  <p>
   Fouhey et al. 2013] with a single image. Previous work demonstrates
  </p>
  <p>
   that one can leverage collections of 3D models to facilitate
  </p>
  <p>
   scene understanding in images [Satkin et al. 2012]. In addition, the
  </p>
  <p>
   depth information in RGBD scans can be used to establish the link
  </p>
  <p>
   between 2D and 3D for model-driven scene understanding [Silberman
  </p>
  <p>
   et al. 2012]. The semantic annotations of images are not immediately
  </p>
  <p>
   useful for modeling and synthesizing 3D scenes, for which
  </p>
  <p>
   the geometric and structural priors have to be learned from 3D data.
  </p>
  <p>
   In this section, we cover the data-driven techniques that leverage
  </p>
  <p>
   collections of 3D scenes for modeling, editing, and synthesizing
  </p>
  <p>
   novel scenes.
  </p>
  <p>
   Context-based retrieval. To address the large variation in the geometry
  </p>
  <p>
   and arrangement of objects in scenes, Fisher et al. [Fisher
  </p>
  <p>
   and Hanrahan 2010; Fisher et al. 2011] propose to take advantage
  </p>
  <p>
   of local context. One of the key insights of their work is that collec
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Figure 16: The algorithm processes raw scene graphs with possible
  </p>
  <p>
   over-segmentation (a) into consistent hierarchies capturing semantic
  </p>
  <p>
   and functional groups (b,c) [Liu et al. 2014].
  </p>
  <p>
   tions of 3D scenes provide rich information about context in which
  </p>
  <p>
   objects appear. They show that capturing these contextual priors
  </p>
  <p>
   can help in scene retrieval and editing.
  </p>
  <p>
   Their system takes an annotated collection of 3D scenes as input,
  </p>
  <p>
   where each object in a scene is classified. They represent each scene
  </p>
  <p>
   as a graph, where nodes represent objects and edges represent relations
  </p>
  <p>
   between objects, such as support and surface contact. In order
  </p>
  <p>
   to compare scenes, they define kernel functions for pairs of nodes
  </p>
  <p>
   measuring similarity in object geometry, and for pairs of edges,
  </p>
  <p>
   measuring similarity in relations of two pairs of objects. They further
  </p>
  <p>
   define a graph kernel to compare pairs of scenes. In particular,
  </p>
  <p>
   they compare all walks of fixed length originating at all pairs of objects
  </p>
  <p>
   in both scene graphs, which loosely captures similarities of all
  </p>
  <p>
   contexts in which objects appear [Fisher et al. 2011]. They show
  </p>
  <p>
   that this similarity metric can be used to retrieve scenes. By comparing
  </p>
  <p>
   only paths originated at a particular object, they can retrieve
  </p>
  <p>
   objects for interactive scene editing.
  </p>
  <p>
   Focal points. Measuring the similarity of complex hybrid scenes
  </p>
  <p>
   such as studios composed of a bedroom, living room, and dining
  </p>
  <p>
   room poses a challenge to graph kernel techniques since they
  </p>
  <p>
   only measure global scene similarity. Thus, Xu et al. [2014a] advocate
  </p>
  <p>
   analyzing salient sub-scenes, which they call focal points,
  </p>
  <p>
   to compare hybrid scenes, i.e., scenes containing multiple salient
  </p>
  <p>
   sub-scenes. Figure 15 shows an example of comparing complex
  </p>
  <p>
   scenes, where the middle scene is a hybrid one encompassing two
  </p>
  <p>
   semantically salient sub-scenes, i.e., bed-nightstands and TV-tablesofa.
  </p>
  <p>
   The middle scene is closer to the left one when the bed and
  </p>
  <p>
   nightstands are focused on, and otherwise when the TV-table-sofa
  </p>
  <p>
   combo is the focal point. Therefore, scene comparison may yield
  </p>
  <p>
   different similarity distances depending on the focal points.
  </p>
  <p>
   Formally, a focal point is defined as a representative substructure
  </p>
  <p>
   of a scene which can characterize a semantic scene category. That
  </p>
  <p>
   means the substructure should re-occur frequently only within that
  </p>
  <p>
   category. Therefore, focal point detection is naturally coupled with
  </p>
  <p>
   the identification of scene categories via scene clustering. This
  </p>
  <p>
   poses coupled problems of detecting focal points based on scene
  </p>
  <p>
   groups and grouping scenes based on focal points. These two problems
  </p>
  <p>
   are solved via interleaved optimization which alternates between
  </p>
  <p>
   focal point detection and focal-based scene clustering. The
  </p>
  <p>
   former is achieved by mining frequent substructures and the latter
  </p>
  <p>
   uses subspace clustering, where scene distances are defined in a
  </p>
  <p>
   focal-centric manner. Inspired by the work of Fisher et al. [Fisher
  </p>
  <p>
   et al. 2011], scene distances are computed using focal-centric graph
  </p>
  <p>
   kernels which are estimated from walks originating from representative
  </p>
  <p>
   focal points.
  </p>
  <p>
   The detected focal points can be used to organize the scene collection
  </p>
  <p>
   and to support efficient exploration of the collection (see
  </p>
  <p>
   Section 9). Focal-based scene similarity can be used for novel applications
  </p>
  <p>
   such as multi-query scene retrieval, where one may issue
  </p>
  <p>
   queries consisting of multiple semantically related scenes and wish
  </p>
  <p>
   Figure 17: The interaction bisector surface (in blue) of several
  </p>
  <p>
   two-object scenes [Zhao et al. 2014].
  </p>
  <p>
   to retrieve more scenes of the same kind.
  </p>
  <p>
   Synthesis. Given an annotated scene collection, one can also
  </p>
  <p>
   synthesize new scenes that have a similar distribution of objects.
  </p>
  <p>
   The scene synthesis technique of Fisher et al. [2012] learns two
  </p>
  <p>
   probabilistic models from the training dataset: (1) object occurrence,
  </p>
  <p>
   indicating which objects should be placed in the scene, and
  </p>
  <p>
   (2) layout optimization, indicating where to place the objects. Next,
  </p>
  <p>
   it takes an example scene, and then synthesizes similar scenes using
  </p>
  <p>
   the learned priors. It replaces or adds new objects using contextbased
  </p>
  <p>
   retrieval techniques, and then optimizes for object placement
  </p>
  <p>
   based on learned object-to-object spatial relations. Synthesizing
  </p>
  <p>
   example scenes might be a challenging task, thus Xu et al. [2013a]
  </p>
  <p>
   propose modeling 3D indoor scenes from 2D sketches, by leveraging
  </p>
  <p>
   a database of 3D scenes. Their system jointly optimizes for
  </p>
  <p>
   sketch-guided co-retrieval and co-placement of all objects.
  </p>
  <p>
   Hierarchical scene annotation. All aforementioned applications
  </p>
  <p>
   take an annotated collection of 3D scenes as an input. Unfortunately,
  </p>
  <p>
   most scenes in public repositories are not annotated
  </p>
  <p>
   and thus require additional manual labeling [Fisher et al. 2012].
  </p>
  <p>
   Liu et al. [2014] address the challenge of annotating novel scenes.
  </p>
  <p>
   The key observation of their work is that understanding hierarchical
  </p>
  <p>
   structure of a scene enables efficient encoding of functional scene
  </p>
  <p>
   substructures, which significantly simplifies detecting objects and
  </p>
  <p>
   representing their relationships. Thus, they propose a supervised
  </p>
  <p>
   learning approach to estimate a hierarchical structure for novel
  </p>
  <p>
   scenes. Given a collection of scene graphs with consistent hierarchies
  </p>
  <p>
   and labels, they train a probabilistic hierarchical grammar
  </p>
  <p>
   encoding the distributions of shapes, cardinalities, and spatial relationships
  </p>
  <p>
   between objects. Such a grammar can then be used to
  </p>
  <p>
   parse new scenes: find segmentations, object labels, and hierarchical
  </p>
  <p>
   organization of objects consistent with the annotated collection
  </p>
  <p>
   (see Figure 16).
  </p>
  <p>
   Challenges and opportunities. The topic of 3D scene analysis
  </p>
  <p>
   is quite new and there are many open problems and research
  </p>
  <p>
   opportunities. The first problem is to efficiently characterize spatial
  </p>
  <p>
   relationships between objects and object groups. Most existing
  </p>
  <p>
   methods work with bounding box representation which are efficient
  </p>
  <p>
   to process, but not sufficiently informative to characterize objectto-
  </p>
  <p>
   object relationships. For example, one cannot reliably determine
  </p>
  <p>
   the object enclosure relationship based on a bounding box. Recently,
  </p>
  <p>
   He et al. [2014] propose to use biologically-inspired bisector
  </p>
  <p>
   surface to characterize the geometric interaction between adjacent
  </p>
  <p>
   objects and to index 3D scenes (Figure 17). The bisector surface
  </p>
  <p>
   can be extended into a geometric descriptor for contextual modeling
  </p>
  <p>
   of the functionality of a 3D object in a given scene [Hu et al.
  </p>
  <p>
   2015]. Second, most existing techniques heavily rely on expert user
  </p>
  <p>
   supervision for scene understanding. Unfortunately, online repositories
  </p>
  <p>
   rarely have models with reliable object tags. Therefore there
  </p>
  <p>
   is a need for methods that can leverage scenes containing only partial
  </p>
  <p>
   and/or noisy annotations. Finally, the popularity of commodity
  </p>
  <p>
   RGBD cameras has significantly simplified the acquisition of in
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   door scenes. This emerging scanning technique opens space for
  </p>
  <p>
   new applications such as online scene analysis [Zhang et al. 2014;
  </p>
  <p>
   Xu et al. 2015].
  </p>
  <p>
   9 Exploration and organization
  </p>
  <p>
   The rapidly growing quantity and variety of digital 3D models in
  </p>
  <p>
   large online collections have caused an emerging need to develop
  </p>
  <p>
   algorithms and techniques that effectively organize these large collections
  </p>
  <p>
   and allow users to interactively explore them. For example,
  </p>
  <p>
   an architect might furnish a digital building by searching through
  </p>
  <p>
   databases organized according to furniture types, regions of interest
  </p>
  <p>
   and design styles. Likewise, an industrial designer can explore
  </p>
  <p>
   shape variations among existing products when creating a new object.
  </p>
  <p>
   Most existing repositories only support text-based search, relying
  </p>
  <p>
   on user-entered tags and titles. This approach suffers from inaccurate
  </p>
  <p>
   and ambiguous tags, often entered in different languages.
  </p>
  <p>
   While it is possible to try using shape analysis to infer consistent
  </p>
  <p>
   tags as discussed in Section 3, it is difficult to convey stylistic and
  </p>
  <p>
   geometric variations using only text. An alternative approach can
  </p>
  <p>
   be to perform shape, sketch, or image based queries. However, to
  </p>
  <p>
   formulate such search queries the user needs to have a clear mental
  </p>
  <p>
   model of the shape that should be retrieved. Thus, some researchers
  </p>
  <p>
   focus on providing tools for exploring shape collections. Unlike
  </p>
  <p>
   search, exploration techniques do not assume a-priori knowledge
  </p>
  <p>
   of the repository content, and help the user to understand geometric,
  </p>
  <p>
   topological, and semantic variations within the collection.
  </p>
  <p>
   Problem statement and method categorization. Data exploration
  </p>
  <p>
   and organization is a classical problem in data analysis and
  </p>
  <p>
   visualization [Paulovich et al. 2011]. Given a data collection, the
  </p>
  <p>
   research focuses on grouping and relating data points, learning
  </p>
  <p>
   the data variations in the collection, and organizing the collection
  </p>
  <p>
   into a structured form, to facilitate retrieval, browsing, summarization,
  </p>
  <p>
   and visualization of the data, based on efficient interfaces or
  </p>
  <p>
   metaphors.
  </p>
  <p>
   The first step to organizing model collections is to devise appropriate
  </p>
  <p>
   metrics to relate different data points. Various similarity metrics
  </p>
  <p>
   have been proposed in the past to relate entire shapes as well
  </p>
  <p>
   as local regions on shapes. In particular, previous sections of this
  </p>
  <p>
   document cover algorithms for computing global shape similarities
  </p>
  <p>
   (Section 3), part-wise correspondences (Section 4), and point-wise
  </p>
  <p>
   correspondences (Section 5). In this section, we will focus on techniques
  </p>
  <p>
   that take advantage of these correlations to provide different
  </p>
  <p>
   interfaces for exploring and understanding geometric variability in
  </p>
  <p>
   collections of 3D shapes. We categorize the existing exploration
  </p>
  <p>
   approaches based on four aspects:
  </p>
  <p>
   Metaphor: a user interface for exploring shape variations.
  </p>
  <p>
   We will discuss five basic exploration interfaces, ones that
  </p>
  <p>
   use proxy shapes (templates), regions of interest, probability
  </p>
  <p>
   plots, query shapes, or continuous attributes.
  </p>
  <p>
   Shape comparison: techniques used to relate different
  </p>
  <p>
   shapes. We will discuss techniques that use global shape similarities,
  </p>
  <p>
   as well as part or point correspondences.
  </p>
  <p>
   Variability: shape variations captured by the system. Most
  </p>
  <p>
   methods we will discuss rely on geometric variability of
  </p>
  <p>
   shapes or parts. Some techniques also take advantage of topological
  </p>
  <p>
   variability; that is, variance in number of parts or how
  </p>
  <p>
   they are connected (or variance in numbers of objects and
  </p>
  <p>
   their arrangements in scenes).
  </p>
  <p>
   Organizational form: a method to group shapes. We will
  </p>
  <p>
   discuss methods that group similar shapes to facilitate explor-
  </p>
  <p>
   Method Meta. Comp. Var. Org.
  </p>
  <p>
   [Ovsjanikov et al. 2011] temp. simi. geom. n/a
  </p>
  <p>
   [Kim et al. 2013a] temp. part both cluster
  </p>
  <p>
   [Averkiou et al. 2014] plot part both cluster
  </p>
  <p>
   [Kim et al. 2012a] ROI point both n/a
  </p>
  <p>
   [Rustamov et al. 2013] ROI point geom. n/a
  </p>
  <p>
   [Huang et al. 2014b] ROI point both cluster
  </p>
  <p>
   [Xu et al. 2014a] ROI simi. topo. cluster
  </p>
  <p>
   [Fish et al. 2014] plot part geom. cluster
  </p>
  <p>
   [Huang et al. 2013b] query simi. both hierarchy
  </p>
  <p>
   Table 4: A summary of several recent works over four aspects.
  </p>
  <p>
   Metaphor: templates, surface painted ROIs, probability distribution
  </p>
  <p>
   plots, or query shapes. Shape Comparison: shape similarity,
  </p>
  <p>
   part or point correspondence. Variability: geometry, topology or
  </p>
  <p>
   both. Organization Form: cluster or hierarchy.
  </p>
  <p>
   ing intra-group similarities and inter-group variations, typically
  </p>
  <p>
   including clustering and hierarchical clustering.
  </p>
  <p>
   Table 4 summarizes several representative works in terms of these
  </p>
  <p>
   aspects. In the remaining part of this section we list several recent
  </p>
  <p>
   techniques which are grouped based on the exploration metaphor.
  </p>
  <p>
   Template-based exploration. Component-wise variability in
  </p>
  <p>
   position and scale of parts reveals useful information about a model
  </p>
  <p>
   collection. Several techniques use box-like templates to show variations
  </p>
  <p>
   among models of the same class. Ovsjanikov et al. [Ovsjanikov
  </p>
  <p>
   et al. 2011] describe a technique for learning these partwise
  </p>
  <p>
   variations without solving the challenging problem of consistent
  </p>
  <p>
   segmentation. First, they use the segmentation of a single shape
  </p>
  <p>
   to construct the initial template. This is the only step that needs to
  </p>
  <p>
   be verified and potentially fixed by the user. The next goal is to
  </p>
  <p>
   automatically infer deformations of the template that would capture
  </p>
  <p>
   the most important geometric variations of other models in the
  </p>
  <p>
   collection. They hypothesize that all shapes can be projected on a
  </p>
  <p>
   low-dimensional manifold based on their global shape descriptors.
  </p>
  <p>
   Finally, they reveal the manifold structure by deforming a template
  </p>
  <p>
   to fit to the sample points. Directions for interesting variations are
  </p>
  <p>
   depicted by arrows on the template and the shapes that correspond
  </p>
  <p>
   to the current template configuration are presented to the user.
  </p>
  <p>
   The descriptor-based approach described above assumes that all
  </p>
  <p>
   intra-class shapes share the same parts and that there exists a lowdimensional
  </p>
  <p>
   manifold that can be captured by deforming a single
  </p>
  <p>
   template. These assumptions do not hold for large and diverse
  </p>
  <p>
   collections of 3D models. To tackle this challenge, Kim et
  </p>
  <p>
   al. [Kim et al. 2013a] proposed an algorithm for learning several
  </p>
  <p>
   part-based templates capturing multi-modal variability in collections
  </p>
  <p>
   of shapes. They start with an initial template that includes a
  </p>
  <p>
   super-set of all parts that might occur in a dataset, and jointly learn
  </p>
  <p>
   part segmentations, point-to-point surface correspondence as well
  </p>
  <p>
   as a compact deformation model. The output is a set of templates
  </p>
  <p>
   that groups the input models into clusters, capturing their styles and
  </p>
  <p>
   variations.
  </p>
  <p>
   ROI-based exploration. Not all interesting variations occur at
  </p>
  <p>
   the scale of parts: they can occur at sub-part scale, or span multiple
  </p>
  <p>
   sub-regions from multiple parts. In these cases the user may prefer
  </p>
  <p>
   to select an arbitrary region on a 3D model and look for more
  </p>
  <p>
   models sharing similar regions of interest. Such detailed and flexible
  </p>
  <p>
   queries require a finer understanding of correspondences between
  </p>
  <p>
   different shapes. Kim et al. [Kim et al. 2012a] propose fuzzy
  </p>
  <p>
   point correspondences to encode the inherent ambiguity in relating
  </p>
  <p>
   diverse shapes. Fuzzy point correspondences are represented
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Figure 18: Shape exploration based on fuzzy correspondence. The
  </p>
  <p>
   user paints a region of interest (ROI) on a query shape (left column),
  </p>
  <p>
   and the method sorts models based on their similarity within the
  </p>
  <p>
   region (right).
  </p>
  <p>
   by real values specified for all pairs of points, indicating how well
  </p>
  <p>
   the points correspond. They leverage transitivity in correspondence
  </p>
  <p>
   relationships to compute this representation from a sparse set of
  </p>
  <p>
   pairwise point correspondences. The interface proposed by Kim et
  </p>
  <p>
   al. allows users to paint regions of interest directly on a surface
  </p>
  <p>
   and then retrieve similar regions among other shapes, or even show
  </p>
  <p>
   geometric variations found in the selected region (see Figure 18).
  </p>
  <p>
   One limitation of correspondence-based techniques is that they typically
  </p>
  <p>
   do not consider the entire collection when estimating shape
  </p>
  <p>
   differences. Rustamov et al. [Rustamov et al. 2013] focus on a
  </p>
  <p>
   fundamental intrinsic representation for shape differences. Starting
  </p>
  <p>
   with a functional map between two shapes, that is, a map that describes
  </p>
  <p>
   a change of functional basis, they derive a shape difference
  </p>
  <p>
   operator revealing detailed information about the location, type,
  </p>
  <p>
   and magnitude of distortions induced by a map. This makes shape
  </p>
  <p>
   difference a quantifiable object that can be co-analyzed within a
  </p>
  <p>
   context of the entire collection. They show that this deeper understanding
  </p>
  <p>
   of shape differences can help in exploration. For example,
  </p>
  <p>
   one can embed shapes in a low-dimensional space based
  </p>
  <p>
   on shape differences, or use shape difference to interpolate variations
  </p>
  <p>
   by showing intermediate shapes between two regions of
  </p>
  <p>
   interest. To extend these technique to man-made objects, Huang et
  </p>
  <p>
   al. [Huang et al. 2014b] construct a consistent functional basis for
  </p>
  <p>
   shape collections exhibiting large geometric and topological variability.
  </p>
  <p>
   They show that the resulting consistent maps are capable
  </p>
  <p>
   of capturing discrete topological variability, such as variance in the
  </p>
  <p>
   number of bars of the back of a chair.
  </p>
  <p>
   ROI-based scene exploration. Recent works on organizing and
  </p>
  <p>
   exploring 3D visual data mostly focus on object collections. Exploring
  </p>
  <p>
   3D scenes poses additional challenges since scenes typically
  </p>
  <p>
   exhibit more structural variations. Unlike man-made objects
  </p>
  <p>
   that usually contain a handful of object parts, scenes can contain
  </p>
  <p>
   anywhere from ten to hundreds of objects. Not only this, but the
  </p>
  <p>
   objects themselves do not typically have a prescribed rigid arrangement
  </p>
  <p>
   with respect to each other. Thus, global scene similarity metrics,
  </p>
  <p>
   such as the graph kernel based one used in [Fisher et al. 2012]
  </p>
  <p>
   are limited to organizing datasets based on very high-level features,
  </p>
  <p>
   such as scene type. Xu et al. [Xu et al. 2014a] advocate that 3D
  </p>
  <p>
   scenes should be compared from the perspective of a particular focal
  </p>
  <p>
   point which is a representative substructure of a specific scene
  </p>
  <p>
   category. Focal points are detected through contextual analysis of
  </p>
  <p>
   a collection of scenes, resulting in a clustering of the scene collection
  </p>
  <p>
   where each cluster is characterized by its representative focal
  </p>
  <p>
   points (see Section 8). Consequently, the focal points extracted
  </p>
  <p>
   from a scene collection can be used to organize collection into an
  </p>
  <p>
   Figure 19: Focal-based scene clustering produces overlapping
  </p>
  <p>
   clusters, which is due to hybrid scenes possessing multiple focal
  </p>
  <p>
   points. An exploratory path, from (a) to (e), through the overlap,
  </p>
  <p>
   smoothly transit between the two scene clusters, representing bedroom
  </p>
  <p>
   and offices, respectively.
  </p>
  <p>
   Figure 20: Given a set of heterogeneous shapes, a reliable qualitative
  </p>
  <p>
   similarity is derived from quartets composed of two pairs of
  </p>
  <p>
   objects (left). Aggregating such qualitative information from many
  </p>
  <p>
   quartets computed across the whole set leads to a categorization
  </p>
  <p>
   tree as a hierarchical organization of the input shape collection
  </p>
  <p>
   (right).
  </p>
  <p>
   interlinked and well-connected cluster formation, which facilitates
  </p>
  <p>
   scene exploration. Figure 19 shows an illustration of such clusterbased
  </p>
  <p>
   organization and an exploratory path transiting between two
  </p>
  <p>
   scene clusters/categories.
  </p>
  <p>
   Plot-based exploration. All aforementioned exploration techniques
  </p>
  <p>
   typically do not visualize the probabilistic nature of shape
  </p>
  <p>
   variations. Fish et al. [Fish et al. 2014] study the configurations
  </p>
  <p>
   of shape parts from a probabilistic perspective, trying to indicate
  </p>
  <p>
   which shape variations are more likely to occur. To learn the distributions
  </p>
  <p>
   of part arrangements, all shapes in the family are presegmented
  </p>
  <p>
   consistently. The resulting set of probability density
  </p>
  <p>
   functions (PDFs) characterizes the variability of relations and arrangements
  </p>
  <p>
   across different parts. A peak in a PDF curve represents
  </p>
  <p>
   that particular a configuration of the related parts frequently appeared
  </p>
  <p>
   among several shapes in the family. The multiple PDFs can
  </p>
  <p>
   be used as interfaces to interactively explore the shape family from
  </p>
  <p>
   various perspectives. Averkiou et al. [Averkiou et al. 2014] use
  </p>
  <p>
   part structure inferred by this method to produce a low-dimensional
  </p>
  <p>
   part-aware embedding of all models. The user can explore interesting
  </p>
  <p>
   variations in part arrangements simply by moving the mouse
  </p>
  <p>
   over the 2D embedding. In addition, their technique allows the
  </p>
  <p>
   synthesis of novel shapes by clicking on empty spaces in the embedded
  </p>
  <p>
   space. Upon clicking, the system would deform parts from
  </p>
  <p>
   neighboring shapes to synthesize a novel part arrangement.
  </p>
  <p>
   Query-based exploration. For a heterogeneous shape collection
  </p>
  <p>
   encompassing diverse object classes, it is typically not possible to
  </p>
  <p>
   characterize part-structure and correspondences between all pairs of
  </p>
  <p>
   shapes. Even global shape similarity is not a very reliable feature in
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   this setting, which makes organizing and exploring heterogeneous
  </p>
  <p>
   collections especially difficult. To address this challenge, Huang et
  </p>
  <p>
   al. [Huang et al. 2013b] introduce qualitative analysis techniques
  </p>
  <p>
   from the field of bioinformatics. Instead of relying on quantitative
  </p>
  <p>
   distances, which may be ill-applied between dissimilar shapes,
  </p>
  <p>
   the method considers a more reliable qualitative similarity derived
  </p>
  <p>
   from quartets composed of two pairs of objects. The shapes that
  </p>
  <p>
   are paired in the quartet are close to each other and far from the
  </p>
  <p>
   shapes in the other pair, where distances are estimated from multiple
  </p>
  <p>
   shape descriptors. They aggregate this topological information
  </p>
  <p>
   from many quartets computed across the entire shape collection,
  </p>
  <p>
   and construct a hierarchical categorization tree (see Figure 20).
  </p>
  <p>
   Analogous to the phylogenetic trees of species, this categorization
  </p>
  <p>
   tree of a shape collection provides an overview of the shapes as
  </p>
  <p>
   well as their mutual distance and hierarchical relations. Based on
  </p>
  <p>
   such an organization, they also define the degree of separation chart
  </p>
  <p>
   for every shape in the collection and apply it for interactive shape
  </p>
  <p>
   exploration.
  </p>
  <p>
   Attribute-based exploration. Yet another approach seeks to allow
  </p>
  <p>
   users to interactively explore shapes with continuously valued
  </p>
  <p>
   semantic attributes. Blanz and Vetter [Blanz and Vetter 1999] provide
  </p>
  <p>
   an interface to explore faces based on continuous facial attributes,
  </p>
  <p>
   such as smile or frown, built upon the face parametric
  </p>
  <p>
   model (Section 6). Similarly, Allen et al. [Allen et al. 2003] allow
  </p>
  <p>
   users to explore the range of human bodies with features such as
  </p>
  <p>
   height, weight, and age. Chaudhuri et al.s [Chaudhuri et al. 2013]
  </p>
  <p>
   interface enables exploration of shape parts according to learned
  </p>
  <p>
   strengths of semantic attributes (Figure 5).
  </p>
  <p>
   10 Discussion
  </p>
  <p>
   There is no magic recipe for developing new data-driven and machine
  </p>
  <p>
   learning applications in geometry processing and computer
  </p>
  <p>
   graphics. Yet, there are some important considerations one needs
  </p>
  <p>
   to make in devising a data-driven method, including computational
  </p>
  <p>
   complexity, scalability, applicability issues, proper evaluation procedures
  </p>
  <p>
   and limitations. In this section, we briefly discuss these
  </p>
  <p>
   issues.
  </p>
  <p>
   Computational complexity. As explained in Section 2, datadriven
  </p>
  <p>
   shape analysis and processing algorithms generally contain
  </p>
  <p>
   several stages (Figure 2). The complexity of each stage varies and
  </p>
  <p>
   largely depends on the number of input shapes, resolution of the
  </p>
  <p>
   input shape representation (number of faces, surface points, pixels
  </p>
  <p>
   or voxels), as well as the number and type of the used geometric
  </p>
  <p>
   features. The feature extraction stage is usually executed per
  </p>
  <p>
   each shape, thus, its time complexity often tends to be linear in the
  </p>
  <p>
   number of input shapes. Local geometric features, such as surface
  </p>
  <p>
   curvature or PCA-based descriptors, are usually computed within a
  </p>
  <p>
   small neighborhood around each vertex, face, or surface point, thus
  </p>
  <p>
   their extraction depends linearly on the number of these primitives
  </p>
  <p>
   in the input shape representation. Extracting geometric features that
  </p>
  <p>
   capture less local or global information about the shape, such as
  </p>
  <p>
   shape diameter, geodesic distance-based features or heat-kernel descriptors,
  </p>
  <p>
   is often computationally more intensive i.e., super-linear
  </p>
  <p>
   in the number of primitives.
  </p>
  <p>
   During the learning and inference steps, data-driven methods usually
  </p>
  <p>
   solve an optimization problem, which involves minimizing or
  </p>
  <p>
   maximizing a function e.g., a data likelihood function. In general,
  </p>
  <p>
   optimization techniques inhabit a wide range of computational
  </p>
  <p>
   complexities. For example, if the optimization involves the leastsquares
  </p>
  <p>
   solution of a linear system, as in the case of linear regression,
  </p>
  <p>
   the complexity is O(N  F2), where N is the number of input
  </p>
  <p>
   training examples and F is the dimensionality of the input feature
  </p>
  <p>
   vector. If optimization is performed through a steepest descent algorithm,
  </p>
  <p>
   the complexity is O(N  F) per parameter update step.
  </p>
  <p>
   However, the performance of iterative optimization algorithms depends
  </p>
  <p>
   on the number of steps, which in turn varies according to
  </p>
  <p>
   their convergence properties and the function they optimize. We
  </p>
  <p>
   refer the reader to [Nocedal and Wright 2006; Koller and Friedman
  </p>
  <p>
   2009b; Solomon 2015] for an in-depth discussion on the computational
  </p>
  <p>
   complexity and convergence properties of various optimization
  </p>
  <p>
   and inference algorithms.
  </p>
  <p>
   Scalability. Data-driven methods are inherently bound up with
  </p>
  <p>
   the input data. Rapid developments in capturing and modeling techniques
  </p>
  <p>
   have engendered the growth of 3D shape and scene repositories
  </p>
  <p>
   over recent years, which have in turn influenced the advancement
  </p>
  <p>
   of data-driven geometry processing. This is evidenced by the
  </p>
  <p>
   fact that the number of training shapes employed in data-driven geometry
  </p>
  <p>
   processing techniques has grown from a few tens [Kalogerakis
  </p>
  <p>
   et al. 2010; Sidi et al. 2011] to several thousands [Kim et al.
  </p>
  <p>
   2013a; Huang et al. 2014b]. On the one hand, the increasing availability
  </p>
  <p>
   of 3D data can improve the accuracy and generalization of
  </p>
  <p>
   data-driven methods. On the other hand, issues of scalability arise.
  </p>
  <p>
   More data causes longer processing times, which in turn makes
  </p>
  <p>
   the debugging of such methods harder for developers. The scalability
  </p>
  <p>
   issues are further exacerbated by the complexity (i.e., highdimensionality)
  </p>
  <p>
   of the 3D geometric feature representations. Potential
  </p>
  <p>
   workarounds include debugging the pipeline of these methods
  </p>
  <p>
   on smaller datasets before turning to larger ones, trying simpler
  </p>
  <p>
   learning techniques before switching to more complex ones, or
  </p>
  <p>
   making use of computing clusters for executing offline steps.
  </p>
  <p>
   Scope of application. Not every problem in shape analysis and
  </p>
  <p>
   processing is well suited to be solved by a data-driven method.
  </p>
  <p>
   When the underlying rules, principles, and parameters can be manually
  </p>
  <p>
   and unambiguously specified in a problem, then non-datadriven
  </p>
  <p>
   methods should be considered for it. For example, deforming
  </p>
  <p>
   a shape with an elastic material and known physical parameters
  </p>
  <p>
   and forces can be addressed by a physics-based method rather
  </p>
  <p>
   than a data-driven one. In contrast, there are several problems in
  </p>
  <p>
   shape analysis and processing for which it is hard, or even impossible,
  </p>
  <p>
   to hand-design a set of rules and principles, or quantify them
  </p>
  <p>
   through manually specified parameters. This is often the case for
  </p>
  <p>
   problems that involve shape and scene recognition, high-level processing,
  </p>
  <p>
   structure parsing, co-analysis, reconstruction from noisy
  </p>
  <p>
   missing data, and modeling with high-level user input. Shape coanalysis
  </p>
  <p>
   (e.g., co-segmentation), in particular, requires estimating
  </p>
  <p>
   several possible geometric and semantic correlations among input
  </p>
  <p>
   shapes, which would be practically impossible to capture through
  </p>
  <p>
   hand-designed rules. Data-driven methods that automatically compute
  </p>
  <p>
   geometric, semantic and structural relationships in the input
  </p>
  <p>
   shapes are more appropriate for such co-analysis problems [Xu
  </p>
  <p>
   et al. 2010; Huang et al. 2011; Huang and Guibas 2013a]. Another
  </p>
  <p>
   example can be found in the problem of shape style analysis.
  </p>
  <p>
   Although humans have an innate sense of style similarity, it is hard
  </p>
  <p>
   to manually quantify geometric criteria for modeling the stylistic
  </p>
  <p>
   similarity of shapes. A style analysis algorithm whose parameters
  </p>
  <p>
   are learned through a data-driven method is much more well-suited
  </p>
  <p>
   to perform this quantification [Liu et al. 2015; Lun et al. 2015].
  </p>
  <p>
   Evaluation. Correctly evaluating the predictive performance of
  </p>
  <p>
   data-driven methods should be another important consideration for
  </p>
  <p>
   researchers of such methods. A common pitfall is to evaluate
  </p>
  <p>
   the predictive performance of a data-driven method with the same
  </p>
  <p>
   dataset on which it was trained on (e.g., through supervised learning),
  </p>
  <p>
   or a dataset for which any parameters of the method were
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   manually tuned. The risk here is that the method might not be able
  </p>
  <p>
   to generalize to any other datasets beyond the ones used in training
  </p>
  <p>
   or hand-tuning. A method that simply memorizes the training
  </p>
  <p>
   dataset, or overfits a model to a particular dataset, will obviously
  </p>
  <p>
   perform well there. However, if its performance on other datasets
  </p>
  <p>
   is poor, the method is effectively useless. The best practice is to
  </p>
  <p>
   introduce training and test splits of the input datasets. The models
  </p>
  <p>
   and parameters should then be learned or tuned exclusively on the
  </p>
  <p>
   training portion, and evaluated exclusively on the testing portion.
  </p>
  <p>
   To insure fair-play, it is also necessary that different data-driven
  </p>
  <p>
   methods be compared using the same training and test splits.
  </p>
  <p>
   Limitations. The data-driven approach to shape analysis and processing
  </p>
  <p>
   is bound by a few limitations that we summarize below. We
  </p>
  <p>
   also discuss potential workarounds to overcome some of these.
  </p>
  <p>
   Generalization guarantees. It is generally hard to provide
  </p>
  <p>
   any guarantees about the generalization performance of datadriven
  </p>
  <p>
   algorithms. In other words, when a data-driven algorithm
  </p>
  <p>
   makes use of a particular dataset for training, it is often
  </p>
  <p>
   impossible to predict how well it will generalize to other
  </p>
  <p>
   datasets beforehand. Although statistical error bounds can
  </p>
  <p>
   be provided under particular assumptions on data distributions,
  </p>
  <p>
   in particular within the context of the Probably Approximately
  </p>
  <p>
   Correct learning theory [Valiant 1984] or the Bayes
  </p>
  <p>
   decision theory [Fukunaga 1990], these assumptions often
  </p>
  <p>
   cannot be validated in practice.
  </p>
  <p>
   Complexity and scalability. As discussed above, data-driven
  </p>
  <p>
   methods are computationally intensive in general. The complexity
  </p>
  <p>
   of data-driven methods depends on the number of input
  </p>
  <p>
   training shapes. As a general rule of thumb, the accuracy
  </p>
  <p>
   of data-driven methods improves with more training data. On
  </p>
  <p>
   the other hand, this comes at a higher computational cost during
  </p>
  <p>
   training time.
  </p>
  <p>
   Size of 3D shape datasets. Despite their recent growth, the
  </p>
  <p>
   size of available 3D shape datasets remains much smaller than
  </p>
  <p>
   those used in computer vision and natural language processing
  </p>
  <p>
   (e.g., image and text datasets). As a result, overfitting remains
  </p>
  <p>
   a common issue with data-driven methods for 3D shape
  </p>
  <p>
   processing. Overfitting occurs when a learned model, or function,
  </p>
  <p>
   captures random error, noise, or patterns specific only to
  </p>
  <p>
   the input training dataset instead of the underlying, correct
  </p>
  <p>
   relationships in the data. It usually occurs when the learned
  </p>
  <p>
   model, or function, is excessively complex, e.g., having an extremely
  </p>
  <p>
   large number of parameters relative to the size of the
  </p>
  <p>
   training dataset. To mitigate this issue, regularization techniques
  </p>
  <p>
   can be used to favor simpler models and functions [Ng
  </p>
  <p>
   2004; Domingos 2012]. Another promising approach is to
  </p>
  <p>
   use both 3D shapes and 2D images as input to data-driven
  </p>
  <p>
   methods, or in other words, to perform co-analysis of image
  </p>
  <p>
   and shape data. We discuss this issue as one important future
  </p>
  <p>
   research direction for further development in data-driven
  </p>
  <p>
   methods in the next section.
  </p>
  <p>
   Data collection. Data-driven techniques rely on the availability
  </p>
  <p>
   of data for the particular problem they attempt to solve.
  </p>
  <p>
   Gathering training data for several geometry processing tasks
  </p>
  <p>
   is often not an easy task, especially when human labor is
  </p>
  <p>
   involved to process or annotate geometric data. Although
  </p>
  <p>
   crowdsourcing Internet marketplaces, such as Amazon Mechanical
  </p>
  <p>
   Turk [Amazon 2009], can help gather training data
  </p>
  <p>
   efficiently, online questionnaires and user studies still require
  </p>
  <p>
   careful design, monetary compensation, and participant consistency
  </p>
  <p>
   checks.
  </p>
  <p>
   From data to knowledge. Data-driven methods put particular
  </p>
  <p>
   emphasis on discovering patterns and models that explain
  </p>
  <p>
   the input data and provide useful insights to the problem being
  </p>
  <p>
   solved. However, these learned patterns and models might not
  </p>
  <p>
   always be readily interpretable i.e., might not correspond to
  </p>
  <p>
   easy-to-understand rules. This is a common situation when
  </p>
  <p>
   one treats the internals of the data-driven method (e.g., the
  </p>
  <p>
   learning process) as a black box without first trying to understand
  </p>
  <p>
   their exact functionality in detail. In general, interpreting
  </p>
  <p>
   such patterns and models requires significant time and
  </p>
  <p>
   effort.
  </p>
  <p>
   11 Conclusion and open problems
  </p>
  <p>
   In this survey, we have so far discussed state-of-the-art on datadriven
  </p>
  <p>
   methods for 3D shape analysis and processing. We also
  </p>
  <p>
   presented the main concepts and methodologies used to develop
  </p>
  <p>
   such methods. We hope that this survey will act as a tutorial that
  </p>
  <p>
   will help researchers develop new data-driven algorithms related to
  </p>
  <p>
   shape analysis and processing. There are several exciting research
  </p>
  <p>
   directions that have not been sufficiently explored so far in our community
  </p>
  <p>
   that we discuss below:
  </p>
  <p>
   Joint analysis of 2D and 3D data. Generating 3D content from
  </p>
  <p>
   images requires building mappings from 2D to 3D space. Unfortunately,
  </p>
  <p>
   the problem remains largely ill-posed. However, with the
  </p>
  <p>
   help vast quantities of 2D images available on the web, effective
  </p>
  <p>
   priors can be developed to map 2D visual elements or features to
  </p>
  <p>
   3D shape and scene representations. Indeed, we have in fact seen
  </p>
  <p>
   recent attempts made in this very vein of thought with some success
  </p>
  <p>
   in [Su et al. 2014; Aubry et al. 2014; Li et al. 2015b; Hueting et al.
  </p>
  <p>
   2015; Su et al. 2015b], which attempts depth estimation through
  </p>
  <p>
   joint analysis over 2D image collections and 3D model databases.
  </p>
  <p>
   We have also seen success of the joint analysis framework in the
  </p>
  <p>
   setting of texture-data with [Yumer et al. 2014], which attempts
  </p>
  <p>
   cosegementation of textured 3D shapes.
  </p>
  <p>
   Following this line, it would be interesting to jointly analyze and
  </p>
  <p>
   process multi-modal visual data, including depth scans and videos.
  </p>
  <p>
   The key challenge lies in the integration of heterogeneous information
  </p>
  <p>
   in a unified learning framework.
  </p>
  <p>
   Better and scalable shape analysis techniques. Many datadriven
  </p>
  <p>
   applications rely on high-quality shape analysis results, particularly
  </p>
  <p>
   shape segmentations and correspondences. We believe it
  </p>
  <p>
   is important to further advance research in both these directions.
  </p>
  <p>
   This includes designing shape analysis techniques for specific data
  </p>
  <p>
   and/or making them scalable to very large datasets, especially recently
  </p>
  <p>
   emerging large-scale richly-annotated repositories [Su et al.
  </p>
  <p>
   2015c].
  </p>
  <p>
   From geometry to semantics and vice versa. Several datadriven
  </p>
  <p>
   methods have tried to map 2D and 3D geometric data to
  </p>
  <p>
   high-level concepts, such as shape categories, semantic attributes,
  </p>
  <p>
   or part labels. Gathering relevant training data is a key component
  </p>
  <p>
   in achieving this aim, a task which remains a non-trivial endeavor.
  </p>
  <p>
   Several recent promising works employ crowdsourcing to address
  </p>
  <p>
   this issue [Chen et al. 2009; Chaudhuri et al. 2013; Lun et al. 2015;
  </p>
  <p>
   Liu et al. 2015; Yumer et al. 2015]. Existing methods deal with
  </p>
  <p>
   cases where only a handful of different entities are predicted for input
  </p>
  <p>
   shapes or scenes. Scaling these methods to handle thousands
  </p>
  <p>
   of categories, part labels and other such entities, as well as attaining
  </p>
  <p>
   human-level performance, is an open problem. The opposite
  </p>
  <p>
   direction is also interesting and insufficiently explored: generating
  </p>
  <p>
   shapes and scenes based on high-level specifications such as shape
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   styles, attributes, or even natural language. Such approaches may
  </p>
  <p>
   even potentially be combined with further diverse inputs, such as
  </p>
  <p>
   sketches and interactive handles, in the shape-generating pipeline.
  </p>
  <p>
   WordsEye [Coyne and Sproat 2001] was an early attempt to bridge
  </p>
  <p>
   this gap, yet requires extensive manual mappings.
  </p>
  <p>
   Understanding function from geometry. The geometry of a
  </p>
  <p>
   shape is strongly related to its functionality, including the shapes
  </p>
  <p>
   relationship to human activity. Thus, analyzing shapes and scenes
  </p>
  <p>
   requires some understanding of their function. The recent works
  </p>
  <p>
   by Laga et al. [Laga et al. 2013], Kim et al. [Kim et al. 2014] and
  </p>
  <p>
   Hu et al. [Hu et al. 2015; ?] are important examples of data-driven
  </p>
  <p>
   approaches that take into account functional aspects of shapes in
  </p>
  <p>
   the process of their analysis. In addition, data-driven methods
  </p>
  <p>
   can guide the synthesis of shapes that can be manufactured or 3D
  </p>
  <p>
   printed based on given functional specifications; an example of
  </p>
  <p>
   such an attempt is reflected in the work of Schulz et al [Schulz et al.
  </p>
  <p>
   2014].
  </p>
  <p>
   Data-driven shape abstractions. It is relatively easy for humans
  </p>
  <p>
   to communicate the essence of shapes with a few lines,
  </p>
  <p>
   sketches, and abstract forms. Developing methods that can build
  </p>
  <p>
   such abstractions automatically has significant applications in shape
  </p>
  <p>
   and scene visualization, artistic rendering, and shape analysis.
  </p>
  <p>
   There are a few data-driven approaches to line drawing [Cole et al.
  </p>
  <p>
   2008; Kalogerakis et al. 2009; Kalogerakis et al. 2012b], saliency
  </p>
  <p>
   analysis [Chen et al. 2012], surface abstraction [Yumer and Kara
  </p>
  <p>
   2012a], and viewpoint preferences [Secord et al. 2011] related to
  </p>
  <p>
   this goal. Matching human performance in these tasks is still a
  </p>
  <p>
   largely open problem, while synthesizing and editing shapes using
  </p>
  <p>
   shape abstractions as input remains a significant challenge.
  </p>
  <p>
   Feature learning. Several shape and scene processing tasks depend
  </p>
  <p>
   on engineering geometric features for points and shapes, as
  </p>
  <p>
   discussed in Section 3. In general, it seems that some features work
  </p>
  <p>
   well in particular settings, but can fail in others. A prevailing issue
  </p>
  <p>
   is that universal geometric descriptors - features that can serve
  </p>
  <p>
   as reliable mid or high level representations ubiquitously across all
  </p>
  <p>
   variety of shapes - do not yet exist.
  </p>
  <p>
   Recent work in machine learning has demonstrated that powerful
  </p>
  <p>
   feature representations can be learned directly from raw input
  </p>
  <p>
   text and image data with deep architectures [Hinton et al. 2006;
  </p>
  <p>
   Krizhevsky et al. 2012; Zeiler and Fergus 2014]. These architectures
  </p>
  <p>
   are composed of multiple processing layers which learn
  </p>
  <p>
   representations of the input data at multiple levels of abstraction.
  </p>
  <p>
   These data-driven representations are optimized for processingperformance
  </p>
  <p>
   in complex interpretation tasks. Such feature learning
  </p>
  <p>
   for 3D shapes with deep architectures has recently been demonstrated
  </p>
  <p>
   in the context of shape classification [Wu et al. 2015; Su
  </p>
  <p>
   et al. 2015a; Xie et al. 2015; Huang et al. 2015a]. Learning features
  </p>
  <p>
   for performing other complex high-level shape analysis and
  </p>
  <p>
   processing tasks remains an open problem.
  </p>
  <p>
   Biographical sketches
  </p>
  <p>
   Kai Xu received his PhD in Computer Science at the National
  </p>
  <p>
   University of Defense Technology (NUDT). He is now a faculty
  </p>
  <p>
   member at NUDT and a visiting associate professor at SIAT. Between
  </p>
  <p>
   2009 and 2010, he visited Simon Fraser University, supported
  </p>
  <p>
   by the Chinese government. His research interests include
  </p>
  <p>
   geometry processing and geometric modeling, especially methods
  </p>
  <p>
   that utilize large collections of 3D shapes. He is currently serving
  </p>
  <p>
   on the editorial board of Computer Graphics Forum, Computer and
  </p>
  <p>
   Graphics, and The Visual Computer. He has served on the program
  </p>
  <p>
   committees of SGP, PG, etc.
  </p>
  <p>
   Vladimir G. Kim is a Research Scientist at Adobe Research. His
  </p>
  <p>
   interests include geometry processing and analysis of shapes and
  </p>
  <p>
   collections of 3D models. He received his PhD in the Computer
  </p>
  <p>
   Science Department at Princeton University in 2013, and was a
  </p>
  <p>
   postdoctoral scholar at Stanford University 20132015. Vladimir is
  </p>
  <p>
   a recipient of the Siebel Scholarship and the NSERC Postgraduate
  </p>
  <p>
   Scholarship. He was also on the International Program Committee
  </p>
  <p>
   for SIGGRAPH 2015, SGP 20132015, and PG.
  </p>
  <p>
   Qixing Huang is a research assistant professor at TTI Chicago.
  </p>
  <p>
   He earned his PhD from the Department of Computer Science at
  </p>
  <p>
   Stanford University in 2012. He obtained both his MS and BS
  </p>
  <p>
   degrees in Computer Science from Tsinghua University in 2005
  </p>
  <p>
   and 2002, respectively. His research interests include data-driven
  </p>
  <p>
   geometry processing and co-analysis of shapes and collections of
  </p>
  <p>
   3D models using convex optimization techniques. He received a
  </p>
  <p>
   Best Paper Award from SGP 2013 and the Most Cited Paper Award
  </p>
  <p>
   from the journal of Computer-Aided Geometric Design in 2011 and
  </p>
  <p>
   2012. He has served on the program committees of SGP, PG and
  </p>
  <p>
   GMP.
  </p>
  <p>
   Niloy Mitra is a Professor of Computer Science at the University
  </p>
  <p>
   College London (UCL). His research focuses on algorithmic
  </p>
  <p>
   issues in shape analysis and geometry processing. He received the
  </p>
  <p>
   ACM SIGGRAPH Significant New Researcher award in 2013 and
  </p>
  <p>
   the BCS Roger Needham award in 2015.
  </p>
  <p>
   Evangelos Kalogerakis is an assistant professor in computer
  </p>
  <p>
   science at the University of Massachusetts Amherst. His research
  </p>
  <p>
   deals with automated analysis and synthesis of 3D visual content,
  </p>
  <p>
   with particular emphasis on machine learning techniques that learn
  </p>
  <p>
   to perform these tasks by combining data, probabilistic models,
  </p>
  <p>
   and prior knowledge. He obtained his PhD from the University of
  </p>
  <p>
   Toronto in 2010 and BEng from the Technical University of Crete in
  </p>
  <p>
   2005. He was a postdoctoral researcher at Stanford University from
  </p>
  <p>
   2010 to 2012. He served on the program committees of EG 2014
  </p>
  <p>
   and 2015, SGP 2012, 2014 and 2015. His research is supported by
  </p>
  <p>
   NSF and NVidia.
  </p>
  <p>
   Acknowledgements
  </p>
  <p>
   We thank Zimo Li for proofreading this survey and the anonymous
  </p>
  <p>
   reviewers for helpful suggestions. Kalogerakis gratefully acknowledges
  </p>
  <p>
   support from NSF (CHS-1422441). Kai Xu is supported by
  </p>
  <p>
   NSFC (61572507, 61202333 and 61532003).
  </p>
  <p>
   References
  </p>
  <p>
   Trimble 3d warehouse.
  </p>
  <p>
   AIGER, D., MITRA, N. J., AND COHEN-OR, D. 2008. 4-points
  </p>
  <p>
   congruent sets for robust pairwise surface registration. In ACM
  </p>
  <p>
   SIGGRAPH 2008 papers, ACM, SIGGRAPH 08, 85:185:10.
  </p>
  <p>
   ALEXA, M., COHEN-OR, D., AND LEVIN, D. 2000. As-rigid-aspossible
  </p>
  <p>
   shape interpolation. In Proceedings of the 27th annual
  </p>
  <p>
   conference on Computer graphics and interactive techniques,
  </p>
  <p>
   ACM Press/Addison-Wesley Publishing Co., New York, NY,
  </p>
  <p>
   USA, SIGGRAPH 00, 157164.
  </p>
  <p>
   ALLEN, B., CURLESS, B., AND POPOVIC
  </p>
  <p>
   , Z. 2003. The space of
  </p>
  <p>
   human body shapes: Reconstruction and parameterization from
  </p>
  <p>
   range scans. ACM Trans. Graph. 22, 3.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   Work Training data Feature Rep. Preproc. Scale Type Sel. Learning model/approach Learning type Learning outcome [Funkhouser et al. 2005] Point No Thousands Local No SVM classifier Supervised Object classifier [Bronstein et al. 2011] Mesh No Thousands Local No Similarity Sensitive Hashing Supervised Distance metric [Huang et al. 2013a] Mesh Pre-align. Thousands Local No Max-marginal distance learning Semi-supervised Distance metric [Kalogerakis et al. 2010] Mesh No Tens Local Yes Jointboost classifier Supervised Face classifier [van Kaick et al. 2011a] Mesh Yes Tens Local Yes Gentleboost classifier Supervised Face classifier [Benhabiles et al. 2011] Mesh No Tens L.&amp;G. Yes Adaboost classifier Supervised Boundary classifier [Xie et al. 2014] Mesh No Hundreds Local Yes Feedforward neural networks Supervised Face/patch classifier [Xu et al. 2014b] Mesh Pre-seg. Tens Local No Sparse model selection Supervised Segment similarity [Lv et al. 2012] Mesh No Tens Local Yes Entropy regularization Semi-supervised Face classifier [Wang et al. 2012] Mesh Pre-seg. Hundreds Local No Active learning Semi-supervised Segment classifier [Wang et al. 2013b] Image Labeled parts Hundreds Local No 2D shape matching Supervised 2D shape similarity [Hu et al. 2012a] Mesh Over-seg. Tens Local Yes Subspace clustering Unsupervised Patch similarity [Sidi et al. 2011] Mesh Pre-seg. Tens Local No Spectral clustering Unsupervised Seg. simi./classifier [Xu et al. 2010] Mesh Part Tens Struct. No Spectral clustering Unsupervised Part proportion simi. [van Kaick et al. 2013] Mesh Part Tens Struct. No Multi-instance clustering Unsupervised Seg. hier. simi. [Golovinskiy and Funkhouser 2009b] Mesh No Tens Global No Global shape alignment Unsupervised Face similarity [Huang et al. 2011] Mesh Pre-seg. Tens Local No Joint part matching Unsupervised Segment similarity [Huang et al. 2014b] Mesh Init. corr. Tens Global No Consistent func. map networks Unsupervised Segment similarity [Kim et al. 2013a] Mesh Template Thousands Local No Shape alignment Semi-supervised Templates [Mattausch et al. 2014] Mesh Over-seg. Hundreds Local No Density-based clustering Unsupervised Patch similarity [Nguyen et al. 2011] Mesh Init. corr. Tens L.&amp;G. No Inconsistent map detection Unsupervised Point similarity [Huang et al. 2012b] Mesh Init. corr. Tens L.&amp;G. No MRF joint matching Unsupervised Point similarity [Kim et al. 2012a] Mesh Pre-align. Tens Global No Spectral matrix recovery Unsupervised Point similarity [Huang and Guibas 2013a] Mesh Init. corr. Tens Global No Low-rank matrix recovery Unsupervised Point similarity [Ovsjanikov et al. 2011] Mesh Part Hundreds Global No Manifold learning Unsupervised Parametric model [Rustamov et al. 2013] Mesh Map Tens None N/A Functional map analysis Unsupervised Difference operator [Fish et al. 2014] Mesh Labeled parts Hundreds Struct. No Kernel Density Estimation Supervised Prob. distributions [Averkiou et al. 2014] Mesh [Kim et al. 2013a] Thousands Struct. No Manifold learning Unsupervised Parametric models [Huang et al. 2013b] Mesh No Hundreds Global No Quartet analysis and clustering Unsupervised Distance measure [Blanz and Vetter 1999] Mesh Pre-align. Hundreds Local No Principal Component Analysis Unsupervised Parametric model [Allen et al. 2003] Point Pre-align. Hundreds Local No Principal Component Analysis Unsupervised Parametric model [Hasler et al. 2009b] Point Pre-align. Hundreds Local No PCA &amp; linear regression Unsupervised Parametric model [Pauly et al. 2005a] Mesh Pre-align. Hundreds Global No Global shape alignment Unsupervised Shape similarity [Nan et al. 2012] Point Labeled parts Hundreds Struct. No Random Forest Classifier Supervised Object classifier [Shen et al. 2012] Mesh Labeled parts Tens Global No Part matching Unsupervised Part detector [Kim et al. 2012b] Point Labeled parts Tens Local No Joint part fitting and matching Unsupervised Object detector [Salas-Moreno et al. 2013] Mesh No Tens L.&amp;G. No Shape matching Unsupervised Object detector [Xu et al. 2011] Mesh Labeled parts Tens Struct. No Structural shape matching Unsupervised Part detector [Aubry et al. 2014] Mesh Projected Thousands Visual No Linear Discriminant Analysis Supervised Object detector [Su et al. 2014] Mesh Projected Tens Visual No Shape matching Unsupervised 2D-3D correlation [Chaudhuri and Koltun 2010] Mesh No Thousands Global No Shape matching Unsupervised Part detector [Chaudhuri et al. 2011a] Mesh [Kalogerakis et al. 2010] Hundreds Local No Bayesian Network Unsupervised Part reasoning model [Xie et al. 2013] Mesh Labeled parts Tens Struct. No Contextual part matching Unsupervised Part detector [Kalogerakis et al. 2012a] Mesh [Kalogerakis et al. 2010] Hundreds L.&amp;G. No Bayesian Network Unsupervised Shape reasoning model [Xu et al. 2012] Mesh Part Tens Struct. No Part matching Unsupervised Part similarity [Talton et al. 2012] Mesh Labeled parts Tens Struct. No Structured concept learning Unsupervised Probabilistic grammar [Yumer and Kara 2012a] Mesh No Tens Global No Shape matching Unsupervised Shape abs. similarity [Yumer and Kara 2014] Mesh Pre-seg. Tens Local No Segment matching Unsupervised Segment abs. simi. [Chaudhuri et al. 2013] Mesh [Kalogerakis et al. 2010] Hundreds L.&amp;G. No SVM ranking Supervised Ranking metric [Fisher et al. 2011] Scene Labeled obj. Tens Struct. No Relevance feedback Supervised Contextual obj. simi. [Fisher et al. 2012] Scene Labeled obj. Hundreds Struct. No Bayesian Network Supervised Mixture models [Xu et al. 2013a] Scene Labeled obj. Hundreds Struct. No Frequent subgraph mining Unsupervised Frequent obj. groups [Xu et al. 2014a] Scene Labeled obj. Hundreds Struct. No Weighted subgraph mining Unsupervised Distinct obj. groups [Liu et al. 2014] Scene Labeled hier. Tens Struct. No Probabilistic learning Supervised Probabilistic grammar Table 5: Comparison of various works on data-driven shape analysis and processing. For each work, we summarize over the criterion
  </p>
  <p>
   set defined for data-driven methods: training data (encompassing data representation, preprocessing and scale), feature (including feature
  </p>
  <p>
   type and whether feature selection is involved), learning model or approach, learning type (supervised, semi-supervised, and unsupervised),
  </p>
  <p>
   learning outcome (e.g., a classifier or a distance metric), as well as its typical application scenario. See the text for detailed explanation of
  </p>
  <p>
   the criteria. Some works employ another work as a pre-processing stage (e.g., [Chaudhuri et al. 2013] requires the labeled segmentation
  </p>
  <p>
   produced by [Kalogerakis et al. 2010]). There are four types of features including local geometric features (Local), global shape descriptors
  </p>
  <p>
   (Global), both local and global shape features (L.&amp;G.), structural features (Struct.) as well as 2D visual features (Visual).
  </p>
  <p>
   ALLIEZ, P., COHEN-STEINER, D., DEVILLERS, O., LEVY, B.,
  </p>
  <p>
   AND DESBRUN, M. 2003. Anisotropic polygonal remeshing. In
  </p>
  <p>
   ACM SIGGRAPH 2003 Papers, SIGGRAPH 03, 485493.
  </p>
  <p>
   ALON, N.,MOSHKOVITZ, D., AND SAFRA, S. 2006. Algorithmic
  </p>
  <p>
   construction of sets for k-restrictions. ACM Trans. Algorithms 2,
  </p>
  <p>
   153177.
  </p>
  <p>
   AMAZON, 2009. Mechanical Turk. http://www.mturk.com.
  </p>
  <p>
   AMENTA, N., BERN, M., AND KAMVYSSELIS, M. 1998. A new
  </p>
  <p>
   voronoi-based surface reconstruction algorithm. In Proceedings
  </p>
  <p>
   of the 25th annual conference on Computer graphics and interactive
  </p>
  <p>
   techniques, ACM, SIGGRAPH 98, 415421.
  </p>
  <p>
   AMIT, Y., FINK, M., SREBRO, N., AND ULLMAN, S. 2007. Uncovering
  </p>
  <p>
   shared structures in multiclass classification. 1724.
  </p>
  <p>
   ANGUELOV, D., SRINIVASAN, P., PANG, H.-C., KOLLER, D.,
  </p>
  <p>
   THRUN, S., AND DAVIS, J. 2004. The correlated correspon
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   dence algorithm for unsupervised registration of nonrigid surfaces.
  </p>
  <p>
   NIPS 17, 3340.
  </p>
  <p>
   ANGUELOV, D., SRINIVASAN, P., KOLLER, D., THRUN, S.,
  </p>
  <p>
   RODGERS, J., AND DAVIS, J. 2005. Scape: shape completion
  </p>
  <p>
   and animation of people. In Proc. of SIGGRAPH, 408416.
  </p>
  <p>
   ANGUELOV, D., SRINIVASAN, P., KOLLER, D., THRUN, S.,
  </p>
  <p>
   RODGERS, J., AND DAVIS, J. 2005. Scape: shape completion
  </p>
  <p>
   and animation of people. In ACM SIGGRAPH 2005 Papers,
  </p>
  <p>
   SIGGRAPH 05, 408416.
  </p>
  <p>
   ANGUELOV, D., TASKAR, B., CHATALBASHEV, V., KOLLER, D.,
  </p>
  <p>
   GUPTA, D., HEITZ, G., AND NG, A. 2005. Discriminative
  </p>
  <p>
   learning of markov random fields for segmentation of 3D scan
  </p>
  <p>
   data. In Proc. CVPR.
  </p>
  <p>
   ANKERST, M., KASTENMULLER, G., KRIEGEL, H.-P., AND
  </p>
  <p>
   SEIDL, T. 1999. 3D shape histograms for similarity search and
  </p>
  <p>
   classification in spatial databases. In SSD99, Springer, 207226.
  </p>
  <p>
   ATTENE, M., KATZ, S., MORTARA, M., PATANE, G., SPAGNUOLO,
  </p>
  <p>
   M., AND TAL, A. 2006. Mesh segmentation - a comparative
  </p>
  <p>
   study. In Proceedings of the IEEE International Conference
  </p>
  <p>
   on Shape Modeling and Applications 2006, IEEE Computer Society,
  </p>
  <p>
   Washington, DC, USA, 7.
  </p>
  <p>
   ATTENE, M., FALCIDIENO, B., AND SPAGNUOLO, M. 2006. Hierarchical
  </p>
  <p>
   mesh segmentation based on fitting primitives. The
  </p>
  <p>
   Visual Computer 22, 3, 181193.
  </p>
  <p>
   AUBRY, M., MATURANA, D., EFROS, A. A., RUSSELL, B. C.,
  </p>
  <p>
   AND SIVIC, J. 2014. Seeing 3d chairs: Exemplar part-based
  </p>
  <p>
   2d-3d alignment using a large dataset of cad models. In Proc.
  </p>
  <p>
   CVPR.
  </p>
  <p>
   AVERKIOU, M., KIM, V. G., ZHENG, Y., AND MITRA, N. J.
  </p>
  <p>
   2014. ShapeSynth: Parameterizing Model Collections for Coupled
  </p>
  <p>
   Shape Exploration and Synthesis. Computer Graphics Forum
  </p>
  <p>
   33, 2.
  </p>
  <p>
   AVERKIOU, M., KIM, V. G., AND MITRA, N. J. 2015. Autocorrelation
  </p>
  <p>
   descriptor for efficient co-alignment of 3d shape collections.
  </p>
  <p>
   Computer Graphics Forum.
  </p>
  <p>
   BAAK, A., MULLER, M., BHARAJ, G., SEIDEL, H.-P., AND
  </p>
  <p>
   THEOBALT, C. 2011. A data-driven approach for real-time full
  </p>
  <p>
   body pose reconstruction from a depth camera. In Proc. ICCV,
  </p>
  <p>
   10921099.
  </p>
  <p>
   BAGHSHAH, M. S., AND SHOURAKI, S. B. 2009. Semisupervised
  </p>
  <p>
   metric learning using pairwise constraints. IJCAI09,
  </p>
  <p>
   12171222.
  </p>
  <p>
   BANKO, M., AND BRILL, E. 2001. Mitigating the paucity-of-data
  </p>
  <p>
   problem: exploring the effect of training corpus size on classifier
  </p>
  <p>
   performance for natural language processing. In Proc. Int. Conf.
  </p>
  <p>
   on Human Lang. Tech. Research, 15.
  </p>
  <p>
   BARRETT, W.W., JOHNSON, C. R., AND LOEWY, R. 1996. The
  </p>
  <p>
   Real Positive Definite Completion Problem: Cycle Completability.
  </p>
  <p>
   Amer Mathematical Society.
  </p>
  <p>
   BARUTCUOGLU, Z., AND DECORO, C. 2006. Hierarchical shape
  </p>
  <p>
   classification using bayesian aggregation. Shape Modeling International.
  </p>
  <p>
   BELONGIE, S., MALIK, J., AND PUZICHA, J. 2002. Shape Matching
  </p>
  <p>
   and Object Recognition Using Shape Contexts. IEEE Trans.
  </p>
  <p>
   Pattern Anal. Mach. Intell. 24, 4.
  </p>
  <p>
   BEN-CHEN, M., BUTSCHER, A., SOLOMON, J., AND GUIBAS,
  </p>
  <p>
   L. J. 2010. On discrete killing vector fields and patterns on
  </p>
  <p>
   surfaces. Comput. Graph. Forum 29, 5, 17011711.
  </p>
  <p>
   BENGIO, Y. 2009. Learning deep architectures for ai. Foundations
  </p>
  <p>
   and Trends in Machine Learning 2, 1, 1127.
  </p>
  <p>
   BENHABILES, H., LAVOUE
  </p>
  <p>
   , G., VANDEBORRE, J.-P., AND
  </p>
  <p>
   DAOUDI, M. 2011. Learning boundary edges for 3d-mesh segmentation.
  </p>
  <p>
   Computer Graphics Forum 30, 8.
  </p>
  <p>
   BERNER, A., BOKELOH, M., WAND, M., SCHILLING, A., AND
  </p>
  <p>
   SEIDEL, H.-P. 2008. A graph-based approach to symmetry
  </p>
  <p>
   detection. In Proceedings of the Eurographics / IEEE VGTC
  </p>
  <p>
   Workshop on Volume Graphics 2008, Los Angeles, California,
  </p>
  <p>
   USA, 2008, 18.
  </p>
  <p>
   BESL, P. J., AND MCKAY, N. D. 1992. A method for registration
  </p>
  <p>
   of 3-d shapes. IEEE Trans. Pattern Anal. Mach. Intell. 14, 2.
  </p>
  <p>
   BESL, P. J., AND MCKAY, N. D. 1992. A method for registration
  </p>
  <p>
   of 3-d shapes. IEEE Trans. Pat. Ana. &amp; Mach. Int. 14, 2, 239
  </p>
  <p>
   256.
  </p>
  <p>
   BEZDEK, J. C., AND HATHAWAY, R. J. 2003. Convergence of
  </p>
  <p>
   alternating optimization. Neural, Parallel Sci. Comput. 11, 351
  </p>
  <p>
   368.
  </p>
  <p>
   BIEDERMAN, I. 1987. Recognition-by-components: A theory of
  </p>
  <p>
   human image understanding. Psychological Review 94, 115
  </p>
  <p>
   147.
  </p>
  <p>
   BISHOP, C. M. 2006. Pattern Recognition and Machine Learning.
  </p>
  <p>
   Springer.
  </p>
  <p>
   BLANZ, V., AND VETTER, T. 1999. A morphable model for the
  </p>
  <p>
   synthesis of 3D faces. In Proc. of SIGGRAPH, 187194.
  </p>
  <p>
   BLUM, M., SPRINGENBERG, J. T., WULFING, J., AND RIEDMILLER,
  </p>
  <p>
   M. 2012. A learned feature descriptor for object
  </p>
  <p>
   recognition in RGB-D data. In Proc. IEEE Int. Conf. on Rob.
  </p>
  <p>
   and Auto., 12981303.
  </p>
  <p>
   BO, L., REN, X., AND FOX, D. 2014. Learning hierarchical sparse
  </p>
  <p>
   features for RGB-(D) object recognition. International Journal
  </p>
  <p>
   of Robotics Research, to appear.
  </p>
  <p>
   BOGO, F., ROMERO, J., LOPER, M., AND BLACK, M. J. 2014.
  </p>
  <p>
   FAUST: Dataset and evaluation for 3D mesh registration. In Proceedings
  </p>
  <p>
   IEEE Conf. on Computer Vision and Pattern Recognition
  </p>
  <p>
   (CVPR), 3794 3801.
  </p>
  <p>
   BOKELOH, M., BERNER, A., WAND, M., SEIDEL, H.-P., AND
  </p>
  <p>
   SCHILLING, A. 2009. Symmetry detection using line features.
  </p>
  <p>
   Computer Graphics Forum 28, 2, 697706.
  </p>
  <p>
   BOKELOH, M., WAND, M., AND SEIDEL, H.-P. 2010. A connection
  </p>
  <p>
   between partial symmetry and inverse procedural modeling.
  </p>
  <p>
   In ACM SIGGRAPH 2010 papers, ACM, SIGGRAPH 10,
  </p>
  <p>
   104:1104:10.
  </p>
  <p>
   BOMMES, D., ZIMMER, H., AND KOBBELT, L. 2009. Mixedinteger
  </p>
  <p>
   quadrangulation. In ACM SIGGRAPH 2009 papers,
  </p>
  <p>
   ACM, SIGGRAPH 09, 77:177:10.
  </p>
  <p>
   BOSCAINI, D., MASCI, J., MELZI, S., BRONSTEIN, M. M.,
  </p>
  <p>
   CASTELLANI, U., AND VANDERGHEYNST, P. 2015. Learning
  </p>
  <p>
   class-specific descriptors for deformable shapes using localized
  </p>
  <p>
   spectral convolutional networks. Computer Graphics Forum 34,
  </p>
  <p>
   5.
  </p>
  <p>
   BOYD, S., AND VANDENBERGHE, L. 2004. Convex Optimization.
  </p>
  <p>
   Cambridge University Press.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   BOYD, S., PARIKH, N., CHU, E., PELEATO, B., AND ECKSTEIN,
  </p>
  <p>
   J. 2011. Distributed optimization and statistical learning via the
  </p>
  <p>
   alternating direction method of multipliers. Found. Trends Mach.
  </p>
  <p>
   Learn. 3, 1, 1122.
  </p>
  <p>
   BOYD, S., PARIKH, N., CHU, E., PELEATO, B., AND ECKSTEIN,
  </p>
  <p>
   J. 2011. Distributed optimization and statistical learning via the
  </p>
  <p>
   alternating direction method of multipliers. Found. Trends Mach.
  </p>
  <p>
   Learn. 3, 1, 1122.
  </p>
  <p>
   BRONSTEIN, M.M., AND BRONSTEIN, A. M. 2011. Shape recognition
  </p>
  <p>
   with spectral distances. IEEE Trans. Pattern Analysis and
  </p>
  <p>
   Machine Intelligence (PAMI) 33, 5, 10651071.
  </p>
  <p>
   BRONSTEIN, M. M., BRONSTEIN, A. M., AND KIMMEL, R.
  </p>
  <p>
   2006. Generalized multidimensional scaling: a framework for
  </p>
  <p>
   isometry-invariant partial surface matching. 11681172.
  </p>
  <p>
   BRONSTEIN, A., BRONSTEIN, M., AND KIMMEL, R. 2008. Numerical
  </p>
  <p>
   Geometry of Non-Rigid Shapes, 1 ed. Springer Publishing
  </p>
  <p>
   Company, Incorporated.
  </p>
  <p>
   BRONSTEIN, A. M., BRONSTEIN, M. M., OVSJANIKOV, M.,
  </p>
  <p>
   AND GUIBAS, L. J. 2011. Shape google: geometric words and
  </p>
  <p>
   expressions for invariant shape retrieval. ACM Trans. Graphics
  </p>
  <p>
   30, 1, 120.
  </p>
  <p>
   BRONSTEIN, M. M., GLASHOFF, K., AND LORING, T. A. 2014.
  </p>
  <p>
   Making Laplacians commute: multimodal spectral geometry using
  </p>
  <p>
   closest commuting operators. SIAM Journal on Imaging Sciences
  </p>
  <p>
   (SIIS), submitted.
  </p>
  <p>
   BROWN, B., AND RUSINKIEWICZ, S. 2007. Global non-rigid
  </p>
  <p>
   alignment of 3-D scans. ACM Trans. Graph. 26, 3.
  </p>
  <p>
   CAMPEN, M., ATTENE, M., AND KOBBELT, L. 2012. A Practical
  </p>
  <p>
   Guide to Polygon Mesh Repairing. In Eurographics tutorials.
  </p>
  <p>
   CAND`ES, E. J., AND PLAN, Y. 2010. Matrix completion with
  </p>
  <p>
   noise. Proceedings of the IEEE 98, 6, 925936.
  </p>
  <p>
   CANDES, E. J., AND PLAN, Y. 2011. Tight oracle inequalities
  </p>
  <p>
   for low-rank matrix recovery from a minimal number of noisy
  </p>
  <p>
   random measurements. IEEE Trans. Inf. Theor. 57, 4, 2342
  </p>
  <p>
   2359.
  </p>
  <p>
   CAND`ES, E. J., AND RECHT, B. 2009. Exact matrix completion
  </p>
  <p>
   via convex optimization. Found. Comput. Math. 9, 6, 717772.
  </p>
  <p>
   CANDES, E. J., AND TAO, T. 2005. Decoding by linear programming.
  </p>
  <p>
   Trans. Inf. Theor. 51, 12, 42034215.
  </p>
  <p>
   CAND`ES, E. J., ROMBERG, J., AND TAO, T. 2006. Robust uncertainty
  </p>
  <p>
   principles: exact signal reconstruction from highly incomplete
  </p>
  <p>
   frequency information. IEEE Transactions on Information
  </p>
  <p>
   Theory 52, 2, 489509.
  </p>
  <p>
   CAND`ES, E. J., LI, X., MA, Y., AND WRIGHT, J. 2011. Robust
  </p>
  <p>
   principal component analysis? J. ACM 58, 3, 11:111:37.
  </p>
  <p>
   CAO, C., WENG, Y., LIN, S., AND ZHOU, K. 2013. 3d shape
  </p>
  <p>
   regression for real-time facial animation. ACM Trans. Graph.
  </p>
  <p>
   32, 4, 41:141:10.
  </p>
  <p>
   CAO, C., HOU, Q., AND ZHOU, K. 2014. Displaced dynamic
  </p>
  <p>
   expression regression for real-time facial tracking and animation.
  </p>
  <p>
   ACM Trans. Graph. 33, 4, 43:143:10.
  </p>
  <p>
   CAZALS, F., AND POUGET, M. 2003. Estimating differential
  </p>
  <p>
   quantities using polynomial fitting of osculating jets. In Proc.
  </p>
  <p>
   Symp. on Geom. Proc., 177187.
  </p>
  <p>
   CEYLAN, D., MITRA, N. J., ZHENG, Y., AND PAULY, M. 2014.
  </p>
  <p>
   Coupled structure-from-motion and 3d symmetry detection for
  </p>
  <p>
   urban facades. ACM Trans. Graph. 33, 1, 2:12:15.
  </p>
  <p>
   CHANG, W., AND ZWICKER, M. 2008. Automatic registration for
  </p>
  <p>
   articulated shapes. Comput. Graph. Forum 27, 5, 14591468.
  </p>
  <p>
   CHANG, J. Y., RASKAR, R., AND AGRAWAL, A. K. 2009. In
  </p>
  <p>
   Proc. CVPR, 17061713.
  </p>
  <p>
   CHANG, W., LI, H., MITRA, N., PAULY, M., AND WAND, M.
  </p>
  <p>
   2012. Dynamic Geometry Processing. In Eurographics tutorials.
  </p>
  <p>
   CHANG, W., LI, H., MITRA, N. J., PAULY, M., AND WAND, M.
  </p>
  <p>
   2012. Dynamic geometry processing. In Eurographics 2012:
  </p>
  <p>
   Tutorial Notes.
  </p>
  <p>
   CHARPIAT, G. 2009. Learning shape metrics based on deformations
  </p>
  <p>
   and transport. In NORDIA 2009.
  </p>
  <p>
   CHAUDHURI, S., AND KOLTUN, V. 2010. Data-driven suggestions
  </p>
  <p>
   for creativity support in 3d modeling. ACM Trans. Graph. 29, 6,
  </p>
  <p>
   183:1183:10.
  </p>
  <p>
   CHAUDHURI, S., KALOGERAKIS, E., GUIBAS, L., AND
  </p>
  <p>
   KOLTUN, V. 2011. Probabilistic reasoning for assembly-based
  </p>
  <p>
   3d modeling. ACM Trans. Graph. 30, 4, 35:135:10.
  </p>
  <p>
   CHAUDHURI, S., KALOGERAKIS, E., GUIBAS, L., AND
  </p>
  <p>
   KOLTUN, V. 2011. Probabilistic reasoning for assembly-based
  </p>
  <p>
   3d modeling. ACM Trans. Graph. 30, 4, 35:135:10.
  </p>
  <p>
   CHAUDHURI, S., KALOGERAKIS, E., GIGUERE, S., , AND
  </p>
  <p>
   FUNKHOUSER, T. 2013. AttribIt: Content creation with semantic
  </p>
  <p>
   attributes. UIST.
  </p>
  <p>
   CHEKURI, C., KHANNA, S., NAOR, J. S., AND ZOSIN, L. 2001.
  </p>
  <p>
   Approximation algorithms for the metric labeling problem via
  </p>
  <p>
   a new linear programming formulation. In Proceedings of the
  </p>
  <p>
   twelfth annual ACM-SIAM symposium on Discrete algorithms,
  </p>
  <p>
   Society for Industrial and Applied Mathematics, Philadelphia,
  </p>
  <p>
   PA, USA, SODA 01, 109118.
  </p>
  <p>
   CHEN, Y., AND MEDIONI, G. 1992. Object modelling by registration
  </p>
  <p>
   of multiple range images. Image Vision Comput. 10,
  </p>
  <p>
   145155.
  </p>
  <p>
   CHEN, D.-Y., TIAN, X.-P., SHEN, Y.-T., AND OUHYOUNG, M.
  </p>
  <p>
   2003. On visual similarity based 3D model retrieval. Computer
  </p>
  <p>
   Graphics Forum 22, 3, 223232.
  </p>
  <p>
   CHEN, D.-Y., TIAN, X.-P., SHEN, Y.-T., AND OUHYOUNG, M.
  </p>
  <p>
   2003. On visual similarity based 3D model retrieval. Computer
  </p>
  <p>
   Graphics Forum 22, 3, 223232.
  </p>
  <p>
   CHEN, G., SONG, Y., WANG, F., AND ZHANG, C. 2008. Semisupervised
  </p>
  <p>
   multi-label learning by solving a sylvester equation.
  </p>
  <p>
   In SDM [DBL 2008], 410419.
  </p>
  <p>
   CHEN, X., KANG, S. B., XU, Y.-Q., DORSEY, J., AND
  </p>
  <p>
   SHUM, H.-Y. 2008. Sketching reality: Realistic interpretation
  </p>
  <p>
   of architectural. ACM Trans. Graph. 27, 2, 11:111:21.
  </p>
  <p>
   CHEN, X., GOLOVINSKIY, A., AND FUNKHOUSER, T. 2009. A
  </p>
  <p>
   benchmark for 3D mesh segmentation. ACM Trans. Graph. 28,
  </p>
  <p>
   3, 73:173:12.
  </p>
  <p>
   CHEN, X., SAPAROV, A., PANG, B., AND FUNKHOUSER, T.
  </p>
  <p>
   2012. Schelling points on 3D surface meshes. ACM Trans.
  </p>
  <p>
   Graph..
  </p>
  <p>
   CHEN, C., WENG, Y., ZHOU, S., TONG, Y., AND ZHOU, K.
  </p>
  <p>
   2014. Facewarehouse: a 3d facial expression database for visual
  </p>
  <p>
   computing. IEEE Trans. Vis. &amp; Comp. Graphics 20, 3.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   CHEN, K., LAI, Y.-K., WU, Y.-X., MARTIN, R., AND HU, S.-
  </p>
  <p>
   M. 2014. Automatic semantic modeling of indoor scenes from
  </p>
  <p>
   low-quality rgb-d data using contextual information. ACM Trans.
  </p>
  <p>
   Graph. 33, 6.
  </p>
  <p>
   CHEN, Y., GUIBAS, L. J., AND HUANG, Q.-X. 2014. Nearoptimal
  </p>
  <p>
   joint object matching via convex relaxation. CoRR
  </p>
  <p>
   abs/1402.1473.
  </p>
  <p>
   CHO, T. S., AVIDAN, S., AND FREEMAN, W. T. 2010. The patch
  </p>
  <p>
   transform. IEEE Trans. Pattern Anal. Mach. Intell. 32, 8, 1489
  </p>
  <p>
   1501.
  </p>
  <p>
   CHO, T. S., AVIDAN, S., AND FREEMAN, W. T. 2010. A probabilistic
  </p>
  <p>
   image jigsaw puzzle solver. In Proc. CVPR, 183190.
  </p>
  <p>
   CHOI, W., CHAO, Y.-W., PANTOFARU, C., AND SAVARESE, S.
  </p>
  <p>
   2013. Understanding indoor scenes using 3d geometric phrases.
  </p>
  <p>
   In Proc. CVPR.
  </p>
  <p>
   CHOUDHARY, S., TREVOR, A., CHRISTENSEN, H., AND DELLAERT,
  </p>
  <p>
   F. 2014. Slam with object discovery; modeling and
  </p>
  <p>
   mapping.
  </p>
  <p>
   CHUI, H., AND RANGARAJAN, A. 2003. A new point matching algorithm
  </p>
  <p>
   for non-rigid registration. Comput. Vis. Image Underst.
  </p>
  <p>
   89, 2-3, 114141.
  </p>
  <p>
   CHUNG, F. R. K., LU, L., AND VU, V. H. 2003. The spectra of
  </p>
  <p>
   random graphs with given expected degrees. Internet Mathematics
  </p>
  <p>
   1, 3, 257275.
  </p>
  <p>
   CLARENZ, U., RUMPF, M., AND TELEA, A. 2004. Robust feature
  </p>
  <p>
   detection and local classification for surfaces based on moment
  </p>
  <p>
   analysis. TVCG 10, 5, 516524.
  </p>
  <p>
   COHEN-STEINER, D., ALLIEZ, P., AND DESBRUN, M. 2004.
  </p>
  <p>
   Variational shape approximation. In ACM SIGGRAPH 2004 Papers,
  </p>
  <p>
   ACM, SIGGRAPH 04, 905914.
  </p>
  <p>
   COIFMAN, R. R., LAFON, S., LEE, A. B., MAGGIONI, M.,
  </p>
  <p>
   WARNER, F., AND ZUCKER, S. 2005. Geometric diffusions
  </p>
  <p>
   as a tool for harmonic analysis and structure definition of data:
  </p>
  <p>
   Diffusion maps. In PNAS, 74267431.
  </p>
  <p>
   COLE, F., GOLOVINSKIY, A., LIMPAECHER, A., BARROS, H. S.,
  </p>
  <p>
   FINKELSTEIN, A., FUNKHOUSER, T., AND RUSINKIEWICZ,
  </p>
  <p>
   S. 2008. Where do people draw lines? ACM Trans. Graph..
  </p>
  <p>
   CONSTALES, D. 1998. A closed formula for the moore-penrose
  </p>
  <p>
   generalized inverse of a complex matrix of given rank. Acta
  </p>
  <p>
   Math. Hungar 80, 8388.
  </p>
  <p>
   COOPER, M., FOOTE, J., GIRGENSOHN, A., AND WILCOX, L.
  </p>
  <p>
   2005. Temporal event clustering for digital photo collections.
  </p>
  <p>
   ACM Trans. on Multi. Comp., Comm. and App. 1, 3, 269288.
  </p>
  <p>
   COSTA, L. D. F. D., AND CESAR, JR., R. M. 2000. Shape Analysis
  </p>
  <p>
   and Classification: Theory and Practice, 1st ed. CRC Press,
  </p>
  <p>
   Inc., Boca Raton, FL, USA.
  </p>
  <p>
   COYNE, B., AND SPROAT, R. 2001. Wordseye: An automatic
  </p>
  <p>
   text-to-scene conversion system. In Proc. of SIGGRAPH.
  </p>
  <p>
   CRANDALL, D., OWENS, A., SNAVELY, N., AND HUTTENLOCHER,
  </p>
  <p>
   D. 2011. Discrete-continuous optimization for largescale
  </p>
  <p>
   structure from motion. CVPR 11, 30013008.
  </p>
  <p>
   CYR, C. M., AND KIMIA, B. B. 2004. A similarity-based aspectgraph
  </p>
  <p>
   approach to 3d object recognition. Int. J. Comput. Vision
  </p>
  <p>
   57, 1, 522.
  </p>
  <p>
   DA FONTOURA COSTA, L., AND CESAR JR., R. M. 2009. Shape
  </p>
  <p>
   Classification and Analysis: Theory and Practice, 2nd ed. CRC
  </p>
  <p>
   Press, Inc., Boca Raton, FL, USA.
  </p>
  <p>
   DANTZIG, G. B. 1951. Maximization of a linear function of variables
  </p>
  <p>
   subject to linear inequalities. In Activity Analysis of Production
  </p>
  <p>
   and Allocation, Cowles Commission Monograph No.
  </p>
  <p>
   13. John Wiley &amp; Sons Inc., 339347.
  </p>
  <p>
   DATTORRO, J. 2011. Convex Optimization &amp; Euclidean Distance
  </p>
  <p>
   Geometry. Meboo Publishing USA.
  </p>
  <p>
   2008. Proceedings of the SIAM International Conference on Data
  </p>
  <p>
   Mining, SDM 2008, April 24-26, 2008, Atlanta, Georgia, USA,
  </p>
  <p>
   SIAM.
  </p>
  <p>
   2009. 2009 IEEE Computer Society Conference on Computer Vision
  </p>
  <p>
   and Pattern Recognition (CVPR 2009), 20-25 June 2009,
  </p>
  <p>
   Miami, Florida, USA, IEEE.
  </p>
  <p>
   DE GOES, F., GOLDENSTEIN, S., AND VELHO, L. 2008. A hierarchical
  </p>
  <p>
   segmentation of articulated bodies. Comput. Graph.
  </p>
  <p>
   Forum 27, 5, 13491356.
  </p>
  <p>
   DE OLIVEIRA, M. C. F., AND LEVKOWITZ, H. 2003. From visual
  </p>
  <p>
   data exploration to visual data mining: a survey. IEEE Trans. Vis.
  </p>
  <p>
   &amp; Comp. Graphics 9, 3, 378394.
  </p>
  <p>
   DECARLO, D., FINKELSTEIN, A., RUSINKIEWICZ, S., AND
  </p>
  <p>
   SANTELLA, A. 2003. Suggestive contours for conveying shape.
  </p>
  <p>
   In ACM SIGGRAPH 2003 Papers, ACM, New York, NY, USA,
  </p>
  <p>
   SIGGRAPH 03, 848855.
  </p>
  <p>
   DENG, J., DONG, W., SOCHER, R., LI, L.-J., LI, K., AND LI,
  </p>
  <p>
   F.-F. 2009. In Proc. CVPR, 248255.
  </p>
  <p>
   DENG, J., KRAUSE, J., AND FEI-FEI, L. 2013. Fine-grained
  </p>
  <p>
   crowdsourcing for fine-grained recognition. In CVPR13.
  </p>
  <p>
   DEY, T. K. 2007. Curve and surface reconstruction: algorithms
  </p>
  <p>
   with mathematical analysis, vol. 23. Cambridge University
  </p>
  <p>
   Press.
  </p>
  <p>
   DIEBEL, J., THRUN, S., AND BRUENING, M. 2006. A bayesian
  </p>
  <p>
   method for probable surface reconstruction and decimation.
  </p>
  <p>
   ACM Trans. Graph. 25, 1.
  </p>
  <p>
   DING, X., AND JIANG, T. 2010. Spectral distributions of adjacency
  </p>
  <p>
   and laplacian matrices of random graphs. Annals of Applied
  </p>
  <p>
   Probability 20(6), 20862117.
  </p>
  <p>
   DO CARMO, M. 1976. Differential Geometry of Curves and Surfaces.
  </p>
  <p>
   Prentice Hall.
  </p>
  <p>
   DOMINGOS, P. 2012. A few useful things to know about machine
  </p>
  <p>
   learning. Communications of the ACM 55, 10, 7887.
  </p>
  <p>
   DONG, S., KIRCHER, S., AND GARLAND, M. 2005. Harmonic
  </p>
  <p>
   functions for quadrilateral remeshing of arbitrary manifolds.
  </p>
  <p>
   Comput. Aided Geom. Des. 22, 5, 392423.
  </p>
  <p>
   DONG, S., BREMER, P.-T., GARLAND, M., PASCUCCI, V., AND
  </p>
  <p>
   HART, J. C. 2006. Spectral surface quadrangulation. In ACM
  </p>
  <p>
   SIGGRAPH 2006 Papers, ACM, New York, NY, USA, SIGGRAPH
  </p>
  <p>
   06, 10571066.
  </p>
  <p>
   DUDA, R. O., HART, P. E., AND STORK, D. G. 2000. Pattern
  </p>
  <p>
   Classification (2Nd Edition). Wiley-Interscience.
  </p>
  <p>
   EITZ, M., RICHTER, R., BOUBEKEUR, T., HILDEBRAND, K.,
  </p>
  <p>
   AND ALEXA, M. 2012. Sketch-based shape retrieval. ACM
  </p>
  <p>
   Trans. Graph. 31, 4, 5:15:10.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   EITZ, M., RICHTER, R., BOUBEKEUR, T., HILDEBRAND, K.,
  </p>
  <p>
   AND ALEXA, M. 2012. Sketch-based shape retrieval. ACM
  </p>
  <p>
   Trans. Graph. 31, 4, 31:131:10.
  </p>
  <p>
   EYNARD, D., KOVNATSKY, A., BRONSTEIN, M. M.,
  </p>
  <p>
   GLASHOFF, K., AND BRONSTEIN, A. M. Multimodal
  </p>
  <p>
   manifold analysis using simultaneous diagonalization of
  </p>
  <p>
   laplacians. submitted to SIAM Journal on Imaging Sciences.
  </p>
  <p>
   FAN, L., WANG, R., XU, L., DENG, J., AND LIU, L. 2013. Modeling
  </p>
  <p>
   by drawing with shadow guidance. Computer Graphics
  </p>
  <p>
   Forum 32, 7, 157166.
  </p>
  <p>
   FEI-FEI, L., FERGUS, R., AND PERONA, P. 2006. One-shot learning
  </p>
  <p>
   of object categories. IEEE Trans. Pat. Ana. &amp; Mach. Int. 28,
  </p>
  <p>
   4.
  </p>
  <p>
   FERGUS, R., WEISS, Y., AND TORRALBA, A. 2009. Semisupervised
  </p>
  <p>
   learning in gigantic image collections. In NIPS, 522
  </p>
  <p>
   530.
  </p>
  <p>
   FIEDLER, M. 1973. Algebraic Connectivity of Graphs. Czechoslovak
  </p>
  <p>
   Mathematical Journal 23, 298305.
  </p>
  <p>
   FISCHLER, M. A., AND BOLLES, R. C. 1981. Random sample
  </p>
  <p>
   consensus: a paradigm for model fitting with applications to image
  </p>
  <p>
   analysis and automated cartography. Commun. ACM 24, 6,
  </p>
  <p>
   381395.
  </p>
  <p>
   FISH, N., AVERKIOU, M., VAN KAICK, O., SORKINEHORNUNG,
  </p>
  <p>
   O., COHEN-OR, D., AND MITRA, N. J. 2014.
  </p>
  <p>
   Meta-representation of shape families. ACM Trans. Graph. 33,
  </p>
  <p>
   4, 34:134:11.
  </p>
  <p>
   FISHER, M., AND HANRAHAN, P. 2010. Context-based search for
  </p>
  <p>
   3d models. ACM Trans. Graph. 29, 6, 182:1182:10.
  </p>
  <p>
   FISHER, M., SAVVA, M., AND HANRAHAN, P. 2011. Characterizing
  </p>
  <p>
   structural relationships in scenes using graph kernels. ACM
  </p>
  <p>
   Trans. Graph. 30, 4, 34:134:12.
  </p>
  <p>
   FISHER, M., RITCHIE, D., SAVVA, M., FUNKHOUSER, T., AND
  </p>
  <p>
   HANRAHAN, P. 2012. Example-based synthesis of 3d object
  </p>
  <p>
   arrangements. ACM Trans. Graph. 31, 6, 135:1135:12.
  </p>
  <p>
   FLORIANI, LEILA DE; SPAGNUOLO, M. E. 2007.
  </p>
  <p>
   Shape Analysis and Structuring. Springer.
  </p>
  <p>
   http://www.springer.com/west/home/math/analysis?SGWID=4-
  </p>
  <p>
   10044-22-173665611-detailsPage=ppmmediatoc.
  </p>
  <p>
   FOSSATI, A., GALL, J., GRABNER, H., REN, X., AND KONOLIGE,
  </p>
  <p>
   K. 2013. Consumer Depth Cameras for Computer Vision,
  </p>
  <p>
   Chapter 12. Springer.
  </p>
  <p>
   FOUHEY, D. F., GUPTA, A., AND HEBERT, M. 2013. Data-driven
  </p>
  <p>
   3d primitives for single image understanding. In Proc. ICCV.
  </p>
  <p>
   FREIFELD, O., AND BLACK, M. J. 2012. Lie bodies: A manifold
  </p>
  <p>
   representation of 3d human shape. In Proceedings of the
  </p>
  <p>
   12th European Conference on Computer Vision - Volume Part I,
  </p>
  <p>
   ECCV12, 114.
  </p>
  <p>
   FREUND, Y., AND SCHAPIRE, R. E. 1995. A decision-theoretic
  </p>
  <p>
   generalization of on-line learning and an application to boosting.
  </p>
  <p>
   Springer-Verlag, London, UK, UK, EuroCOLT 95, 2337.
  </p>
  <p>
   FROME, A., HUBER, D., KOLLURI, R., BULOW, T., AND MALIK,
  </p>
  <p>
   J. 2004. Recognizing objects in range data using regional
  </p>
  <p>
   point descriptors. In Proc. ECCV. 224237.
  </p>
  <p>
   FU, H., COHEN-OR, D., DROR, G., AND SHEFFER, A. Upright
  </p>
  <p>
   orientation of man-made objects. SIGGRAPH 08, 42:142:7.
  </p>
  <p>
   FUKUNAGA, K. 1990. Introduction to Statistical Pattern Recognition.
  </p>
  <p>
   Academic Press Professional, Inc.
  </p>
  <p>
   FUNKHOUSER, T., AND SHILANE, P. 2006. Partial matching of
  </p>
  <p>
   3d shapes with priority-driven search. In Proc. Symp. on Geom.
  </p>
  <p>
   Proc.
  </p>
  <p>
   FUNKHOUSER, T., MIN, P., KAZHDAN, M., CHEN, J., HALDERMAN,
  </p>
  <p>
   A., DOBKIN, D., AND JACOBS, D. 2003. A search engine
  </p>
  <p>
   for 3D models. ACM Trans. Graph. 22, 1, 83105.
  </p>
  <p>
   FUNKHOUSER, T., KAZHDAN, M., SHILANE, P., MIN, P.,
  </p>
  <p>
   KIEFER, W., TAL, A., RUSINKIEWICZ, S., AND DOBKIN, D.
  </p>
  <p>
   2004. Modeling by example. ACM Trans. Graph. 23, 3, 652
  </p>
  <p>
   663.
  </p>
  <p>
   FUNKHOUSER, T. A., KAZHDAN, M. M., MIN, P., AND SHILANE,
  </p>
  <p>
   P. 2005. Shape-based retrieval and analysis of 3d models.
  </p>
  <p>
   Commun. ACM 48, 6, 5864.
  </p>
  <p>
   GAL, R., SHAMIR, A., HASSNER, T., PAULY, M., AND COHENOR,
  </p>
  <p>
   D. 2007. Surface reconstruction using local shape priors.
  </p>
  <p>
   In Proc. Symp. on Geom. Proc., SGP 07, 253262.
  </p>
  <p>
   GAL, R., SHAMIR, A., HASSNER, T., PAULY, M., AND COHENOR,
  </p>
  <p>
   D. 2007. Surface reconstruction using local shape priors.
  </p>
  <p>
   In Proc. Symp. on Geom. Proc., 253262.
  </p>
  <p>
   GAL, R., SORKINE, O., MITRA, N. J., AND COHEN-OR, D.
  </p>
  <p>
   2009. iwires: an analyze-and-edit approach to shape manipulation.
  </p>
  <p>
   ACM Trans. Graph., 33:133:10.
  </p>
  <p>
   GAO, L., LAI, Y.-K., HUANG, Q., AND HU, S.-M. 2013. A datadriven
  </p>
  <p>
   approach to realistic shape morphing. Computer Graphics
  </p>
  <p>
   Forum 32, 2, 449457.
  </p>
  <p>
   GELFAND, N., AND GUIBAS, L. J. 2004. Shape segmentation
  </p>
  <p>
   using local slippage analysis. In Proceedings of the 2004 Eurographics/
  </p>
  <p>
   ACM SIGGRAPH symposium on Geometry processing,
  </p>
  <p>
   ACM, SGP 04, 214223.
  </p>
  <p>
   GELFAND, N., MITRA, N. J., GUIBAS, L. J., AND POTTMANN,
  </p>
  <p>
   H. 2005. Robust global registration. In Proc. Symp. on Geom.
  </p>
  <p>
   Proc.
  </p>
  <p>
   GIORGI, D., BIASDTTI, S., AND PARABOSCHI, L., 2007.
  </p>
  <p>
   Shrec: shape retreval contex: Watertight models track.
  </p>
  <p>
   GIORGI, D., BIASOTTI, S., AND PARABOSCHI, L., 2007. Shape
  </p>
  <p>
   retrieval contest 2007: Watertight models track.
  </p>
  <p>
   GOLDBERG, D., MALON, C., AND BERN, M. 2004. A global approach
  </p>
  <p>
   to automatic solution of jigsaw puzzles. Comput. Geom.
  </p>
  <p>
   Theory Appl. 28, 165174.
  </p>
  <p>
   GOLDMAN, D. B., CURLESS, B., HERTZMANN, A., AND SEITZ,
  </p>
  <p>
   S. M. 2005. Shape and spatially-varying brdfs from photometric
  </p>
  <p>
   stereo. In Proceedings of the Tenth IEEE International
  </p>
  <p>
   Conference on Computer Vision (ICCV05) Volume 1 - Volume
  </p>
  <p>
   01, ICCV 05, 341348.
  </p>
  <p>
   GOLOVINSKIY, A., AND FUNKHOUSER, T. 2008. Randomized
  </p>
  <p>
   cuts for 3d mesh analysis. In ACM SIGGRAPH Asia 2008 papers,
  </p>
  <p>
   ACM, SIGGRAPH Asia 08, 145:1145:12.
  </p>
  <p>
   GOLOVINSKIY, A., AND FUNKHOUSER, T. 2009. Consistent Segmentation
  </p>
  <p>
   of 3D Models. Proc. SMI 33, 3.
  </p>
  <p>
   GOLOVINSKIY, A., AND FUNKHOUSER, T. A. 2009. Consistent
  </p>
  <p>
   segmentation of 3d models. Computers &amp; Graphics 33, 3, 262
  </p>
  <p>
   269.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   GOLOVINSKIY, A., KIM, V. G., AND FUNKHOUSER, T. 2009.
  </p>
  <p>
   Shape-based Recognition of 3D Point Clouds in Urban Environments.
  </p>
  <p>
   In Proc. ICCV.
  </p>
  <p>
   GONEN, M., AND ALPAYDIN, E. 2011. Multiple kernel learning
  </p>
  <p>
   algorithms. J. of Mac. Learn. Res. 12, 22112268.
  </p>
  <p>
   GRANT, M., AND BOYD, S., 2011. CVX: Matlab software for disciplined
  </p>
  <p>
   convex programming. http://www.stanford.edu/.boyd/
  </p>
  <p>
   cvx/.
  </p>
  <p>
   GUO, X., LIN, J., XU, K., AND JIN, X. 2014. Creature grammar
  </p>
  <p>
   for creative modeling of 3d monsters. Graphical Models (Special
  </p>
  <p>
   Issue of GMP) 76, 5, 376389.
  </p>
  <p>
   HALLD ORSSON, M., AND RADHAKRISHNAN, J. 1994. Greed is
  </p>
  <p>
   good: approximating independent sets in sparse and boundeddegree
  </p>
  <p>
   graphs. STOC 94, 439448.
  </p>
  <p>
   HASLER, N., STOLL, C., ROSENHAHN, B., THORM AHLEN, T.,
  </p>
  <p>
   AND SEIDEL, H.-P. 2009. Technical section: Estimating body
  </p>
  <p>
   shape of dressed humans. Comput. Graph. 33, 3, 211216.
  </p>
  <p>
   HASLER, N., STOLL, C., SUNKEL, M., ROSENHAHN, B., AND
  </p>
  <p>
   SEIDEL, H.-P. 2009. A statistical model of human pose and
  </p>
  <p>
   body shape. Comput. Graph. Forum 28, 2, 337346.
  </p>
  <p>
   HASLER, N., THORMAHLEN, T., ROSENHAHN, B., AND SEIDEL,
  </p>
  <p>
   H.-P. 2010. Learning skeletons for shape and pose. In Proceedings
  </p>
  <p>
   of the 2010 ACM SIGGRAPH Symposium on Interactive 3D
  </p>
  <p>
   Graphics and Games, I3D 10, 2330.
  </p>
  <p>
   HAYS, J., LEORDEANU, M., EFROS, A. A., AND LIU, Y. 2006.
  </p>
  <p>
   Discovering texture regularity as a higher-order correspondence
  </p>
  <p>
   problem. In Computer Vision - ECCV 2006, 9th European Conference
  </p>
  <p>
   on Computer Vision, Graz, Austria, May 7-13, 2006,
  </p>
  <p>
   Proceedings, Part II, 522535.
  </p>
  <p>
   HELD, D., LEVINSON, J., THRUN, S., AND SAVARESE, S. 2014.
  </p>
  <p>
   Combining 3d shape, color, and motion for robust anytime tracking.
  </p>
  <p>
   In RSS.
  </p>
  <p>
   HILAGA, M., SHINAGAWA, Y., KOHMURA, T., AND KUNII, T. L.
  </p>
  <p>
   2001. Topology matching for fully automatic similarity estimation
  </p>
  <p>
   of 3d shapes. In Proc. of SIGGRAPH, 203212.
  </p>
  <p>
   HINTON, G. E., OSINDERO, S., AND TEH, Y.-W. 2006. A fast
  </p>
  <p>
   learning algorithm for deep belief nets. Neural Computation 18,
  </p>
  <p>
   7, 15271554.
  </p>
  <p>
   HOCHBAUM, D. S., AND SINGH, V. 2009. An efficient algorithm
  </p>
  <p>
   for co-segmentation. In IEEE 12th International Conference on
  </p>
  <p>
   Computer Vision, ICCV 2009, Kyoto, Japan, September 27 - October
  </p>
  <p>
   4, 2009, 269276.
  </p>
  <p>
   HOFER, M., ODEHNAL, B., POTTMANN, H., STEINER, T., AND
  </p>
  <p>
   WALLNER, J. 2005. 3d shape recognition and reconstruction
  </p>
  <p>
   based on line element geometry. ICCV 05, 15321538.
  </p>
  <p>
   HOFFMAN, D. D., AND SINGH, M. 1997. Salience of visual parts.
  </p>
  <p>
   Cognition 63, 1, 2978.
  </p>
  <p>
   HOFFMAN, D., RICHARDS, W., PENTL, A., RUBIN, J., AND
  </p>
  <p>
   SCHEUHAMMER, J. 1983. Parts of recognition. Cognition 18,
  </p>
  <p>
   6596.
  </p>
  <p>
   HOI, S. C., LIU, W., AND CHANG, S.-F. 2010. Semi-supervised
  </p>
  <p>
   distance metric learning for collaborative image retrieval and
  </p>
  <p>
   clustering. ACM Trans. Multimedia Comput. Commun. Appl. 6,
  </p>
  <p>
   3, 18:118:26.
  </p>
  <p>
   HOIEM, D., EFROS, A. A., AND HEBERT, M. 2005. Automatic
  </p>
  <p>
   photo pop-up. ACM Trans. Graph. 24, 3, 577584.
  </p>
  <p>
   HOLLAND, P. W., AND WELSCH, R. E. 1977. Robust regression
  </p>
  <p>
   using iteratively reweighted least-squares. Communications in
  </p>
  <p>
   Statistics: Theory and Methods A6, 813827.
  </p>
  <p>
   HORN, B. K. P., HILDEN, H., AND NEGAHDARIPOUR, S. 1988.
  </p>
  <p>
   Closed-form solutions of absolute orientation using orthonormal
  </p>
  <p>
   matrices. Journal of the Optical Society 5, 11271135.
  </p>
  <p>
   HORN, B. K. P. 1984. Extended Gaussian Images. Proceedings of
  </p>
  <p>
   the IEEE 72, 12, 16711686.
  </p>
  <p>
   HORN, B. K. P. 1987. Closed-form solution of absolute orientation
  </p>
  <p>
   using unit quaternions. Journal of the Optical Society of America
  </p>
  <p>
   A 4, 4, 629642.
  </p>
  <p>
   HU, R., FAN, L., AND LIU, L. 2012. Co-segmentation of 3d
  </p>
  <p>
   shapes via subspace clustering. Computer Graphics Forum 31,
  </p>
  <p>
   5.
  </p>
  <p>
   HU, R., FAN, L., AND LIU, L. 2012. Co-Segmentation of 3D
  </p>
  <p>
   shapes via subspace clustering. Computer Graphics Forum 31,
  </p>
  <p>
   5, 17031713.
  </p>
  <p>
   HU, R., FAN, L., AND LIU, L. 2012. Co-Segmentation of 3D
  </p>
  <p>
   shapes via subspace clustering. Comput. Graph. Forum 31, 5,
  </p>
  <p>
   17031713.
  </p>
  <p>
   HU, R., ZHU, C., VAN KAICK, O., LIU, L., SHAMIR, A., AND
  </p>
  <p>
   ZHANG, H. 2015. Interaction context (icon): Towards a geometric
  </p>
  <p>
   functionality descriptor. ACM Trans. Graph. 34, 4, 83:1
  </p>
  <p>
   83:12.
  </p>
  <p>
   HUANG, Q.-X., AND ANGUELOV, D. 2010. High quality pose estimation
  </p>
  <p>
   by aligning multiple scans to a latent map. In IEEE
  </p>
  <p>
   International Conference on Robotics and Automation, ICRA
  </p>
  <p>
   2010, Anchorage, Alaska, USA, 3-7 May 2010, 13531360.
  </p>
  <p>
   HUANG, Q., AND GUIBAS, L. 2013. Consistent shape maps via
  </p>
  <p>
   semidefinite programming. Computer Graphics Forum (SGP)
  </p>
  <p>
   32, 5, 177186.
  </p>
  <p>
   HUANG, Q., AND GUIBAS, L. 2013. Consistent shape maps via
  </p>
  <p>
   semidefinite programming. Computer Graphics Forum (SGP)
  </p>
  <p>
   32, 5, 177186.
  </p>
  <p>
   HUANG, Q., FL ORY, S., GELFAND, N., HOFER, M., AND
  </p>
  <p>
   POTTMANN, H. 2006. Reassembling fractured objects by geometric
  </p>
  <p>
   matching. ACM Trans. Graph. 25, 3, 569578.
  </p>
  <p>
   HUANG, Q.-X., ADAMS, B., AND WAND, M. 2007. Bayesian surface
  </p>
  <p>
   reconstruction via iterative scan alignment to an optimized
  </p>
  <p>
   prototype. In Proc. Symp. on Geom. Proc., SGP 07, 213223.
  </p>
  <p>
   HUANG, J., ZHANG, M., MA, J., LIU, X., KOBBELT, L., AND
  </p>
  <p>
   BAO, H. 2008. Spectral quadrangulation with orientation and
  </p>
  <p>
   alignment control. In ACM SIGGRAPH Asia 2008 papers, ACM,
  </p>
  <p>
   SIGGRAPH Asia 08, 147:1147:9.
  </p>
  <p>
   HUANG, Q., ADAMS, B.,WICKE, M., AND GUIBAS, L. J. 2008.
  </p>
  <p>
   Non-rigid registration under isometric deformations. In Proc.
  </p>
  <p>
   Symp. on Geom. Proc., 14491457.
  </p>
  <p>
   HUANG, Q.-X., WICKE, M., ADAMS, B., AND GUIBAS, L. J.
  </p>
  <p>
   2009. Shape decomposition using modal analysis. Comput.
  </p>
  <p>
   Graph. Forum 28, 2, 407416.
  </p>
  <p>
   HUANG, Q., KOLTUN, V., AND GUIBAS, L. 2011. Joint shape
  </p>
  <p>
   segmentation using linear programming. ACM Trans. Graph.
  </p>
  <p>
   30, 6, 125:1125:12.
  </p>
  <p>
   HUANG, Q., GUIBAS, L., AND MITRA, N. 2012. Near-regular
  </p>
  <p>
   structure extraction using linear programming. ACM Trans.
  </p>
  <p>
   Graph..
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   HUANG, Q., ZHANG, G.-X., GAO, L., HU, S.-M., BUTSCHER,
  </p>
  <p>
   A., AND GUIBAS, L. 2012. An optimization approach for
  </p>
  <p>
   extracting and encoding consistent maps in a shape collection.
  </p>
  <p>
   ACM Trans. Graph. 31, 6, 167:1167:11.
  </p>
  <p>
   HUANG, Q., SU, H., AND GUIBAS, L. 2013. Fine-grained
  </p>
  <p>
   semi-supervised labeling of large shape collections. ACM Trans.
  </p>
  <p>
   Graph. 32, 6, 190:1190:10.
  </p>
  <p>
   HUANG, S.-S., SHAMIR, A., SHEN, C.-H., ZHANG, H., SHEFFER,
  </p>
  <p>
   A., HU, S.-M., AND COHEN-OR, D. 2013. Qualitative
  </p>
  <p>
   organization of collections of shapes via quartet analysis. ACM
  </p>
  <p>
   Trans. Graph. 32, 4, 71:171:10.
  </p>
  <p>
   HUANG, Q., CHEN, Y., AND GUIBAS, L. J., 2014. Scalable
  </p>
  <p>
   semidefinite relaxation for maximum a posterior estimation.
  </p>
  <p>
   HUANG, Q., WANG, F., AND GUIBAS, L. 2014. Functional map
  </p>
  <p>
   networks for analyzing and exploring large shape collections.
  </p>
  <p>
   ACM Trans. Graph. 33, 4.
  </p>
  <p>
   HUANG, H., KALOGERAKIS, E., AND MARLIN, B. 2015. Analysis
  </p>
  <p>
   and synthesis of 3d shape families via deep-learned generative
  </p>
  <p>
   models of surfaces. Computer Graphics Forum 34, 5.
  </p>
  <p>
   HUANG, Q., WANG, H., AND KOLTUN, V. 2015. Single-view
  </p>
  <p>
   reconstruction via joint analysis of image and shape collections.
  </p>
  <p>
   ACM Trans. Graph. 34, 87:187:10.
  </p>
  <p>
   HUBER, D. 2002. Automatic Three-dimensional Modeling from
  </p>
  <p>
   Reality. PhD thesis, Robotics Institute, Carnegie Mellon University,
  </p>
  <p>
   Pittsburgh, PA.
  </p>
  <p>
   HUETING, M., OVSJANIKOV, M., AND MITRA, N. J. 2015.
  </p>
  <p>
   Crosslink: Joint understanding of image and 3d model collections
  </p>
  <p>
   through shape and camera pose variations. ACM Trans.
  </p>
  <p>
   Graph..
  </p>
  <p>
   IBM. 2011. Ibm ilog cplex optimization studio. http://www-
  </p>
  <p>
   01.ibm.com/software/integration/optimization/cplexoptimization-
  </p>
  <p>
   studio/.
  </p>
  <p>
   JAIN, A., THORM AHLEN, T., RITSCHEL, T., AND SEIDEL, H.-P.
  </p>
  <p>
   2012. Exploring shape variations by 3D-model decomposition
  </p>
  <p>
   and part-based recombination. Computer Graphics Forum 31, 2,
  </p>
  <p>
   631640.
  </p>
  <p>
   JAMES, D. L., AND TWIGG, C. D. 2005. Skinning mesh animations.
  </p>
  <p>
   ACM Trans. Graph. 24, 3, 399407.
  </p>
  <p>
   JIAO, F., WANG, S., LEE, C.-H., GREINER, R., AND SCHUURMANS,
  </p>
  <p>
   D. 2006. Semi-supervised conditional random fields for
  </p>
  <p>
   improved sequence segmentation and labeling. In 21st International
  </p>
  <p>
   Conference on Computational Linguistics.
  </p>
  <p>
   JOHNSON, A., AND HEBERT, M. 1999. Using Spin Images for Efficient
  </p>
  <p>
   Object Recognition in Cluttered 3D Scenes. IEEE Trans.
  </p>
  <p>
   Pat. Ana. &amp; Mach. Int. 21, 5, 433449.
  </p>
  <p>
   JOHNSON, A. E., AND HEBERT, M. 1999. Using spin images for
  </p>
  <p>
   efficient object recognition in cluttered 3d scenes. IEEE Trans.
  </p>
  <p>
   Pattern Anal. Mach. Intell. 21, 5, 433449.
  </p>
  <p>
   JOHNSON, A. 1997. Spin-Images: A Representation for 3-D Surface
  </p>
  <p>
   Matching. PhD thesis, Robotics Institute, Carnegie Mellon
  </p>
  <p>
   University, Pittsburgh, PA.
  </p>
  <p>
   JOULIN, A., BACH, F. R., AND PONCE, J. 2010. Discriminative
  </p>
  <p>
   clustering for image co-segmentation. In Proc. CVPR, 1943
  </p>
  <p>
   1950.
  </p>
  <p>
   KAHRAMAN, A., MORRIS, R. J., LASKOWSKI, R. A., AND
  </p>
  <p>
   THORNTON, J. M. 2007. Shape variation in protein binding
  </p>
  <p>
   pockets and their ligands. Journal of Molecular Biology 368, 1,
  </p>
  <p>
   283301.
  </p>
  <p>
   KALOGERAKIS, E., SIMARI, P., NOWROUZEZAHRAI, D., AND
  </p>
  <p>
   SINGH, K. 2007. Robust statistical estimation of curvature on
  </p>
  <p>
   discretized surfaces. In Proc. Symp. on Geom. Proc.
  </p>
  <p>
   KALOGERAKIS, E., NOWROUZEZAHRAI, D., SIMARI, P., MCCRAE,
  </p>
  <p>
   J., HERTZMANN, A., AND SINGH, K. 2009. Datadriven
  </p>
  <p>
   curvature for real-time line drawing of dynamic scenes.
  </p>
  <p>
   ACM Trans. Graph. 28, 1, 113.
  </p>
  <p>
   KALOGERAKIS, E., HERTZMANN, A., AND SINGH, K. 2010.
  </p>
  <p>
   Learning 3d mesh segmentation and labeling. ACM Trans.
  </p>
  <p>
   Graph. 29, 102:1102:12.
  </p>
  <p>
   KALOGERAKIS, E., CHAUDHURI, S., KOLLER, D., AND
  </p>
  <p>
   KOLTUN, V. 2012. A probabilistic model for component-based
  </p>
  <p>
   shape synthesis. ACM Trans. Graph. 31, 4.
  </p>
  <p>
   KALOGERAKIS, E., NOWROUZEZAHRAI, D., BRESLAV, S., AND
  </p>
  <p>
   HERTZMANN, A. 2012. Learning Hatching for Pen-and-Ink
  </p>
  <p>
   Illustration of Surfaces. ACM Trans. Graph. 31, 1.
  </p>
  <p>
   KARP, R. 1972. Reducibility among combinatorial problems.
  </p>
  <p>
   In Complexity of Computer Computations, R. Miller and
  </p>
  <p>
   J. Thatcher, Eds. Plenum Press, 85103.
  </p>
  <p>
   KASS, M., WITKIN, A., AND TERZOPOULOS, D. 1988. Snakes:
  </p>
  <p>
   Active contour models. INTERNATIONAL JOURNAL OF COMPUTER
  </p>
  <p>
   VISION 1, 4, 321331.
  </p>
  <p>
   KATZ, S., AND TAL, A. 2003. Hierarchical mesh decomposition
  </p>
  <p>
   using fuzzy clustering and cuts. SIGGRAPH 03, 954961.
  </p>
  <p>
   KATZ, S., LEIFMAN, G., AND TAL, A. 2005. Mesh segmentation
  </p>
  <p>
   using feature point and core extraction. The Visual Computer 21,
  </p>
  <p>
   8-10, 649658.
  </p>
  <p>
   KAZHDAN, M., CHAZELLE, B., DOBKIN, D., FUNKHOUSER, T.,
  </p>
  <p>
   AND RUSINKIEWICZ, S. 2003. A reflective symmetry descriptor
  </p>
  <p>
   for 3d models. Algorithmica 38, 1.
  </p>
  <p>
   KAZHDAN, M., FUNKHOUSER, T., AND RUSINKIEWICZ, S.
  </p>
  <p>
   2003. Rotation invariant spherical harmonic representation of
  </p>
  <p>
   3d shape descriptors. Proc. Symp. on Geom. Proc..
  </p>
  <p>
   KAZHDAN, M., FUNKHOUSER, T., AND RUSINKIEWICZ, S.
  </p>
  <p>
   2003. Rotation invariant spherical harmonic representation of
  </p>
  <p>
   3d shape descriptors. SGP 03, 156164.
  </p>
  <p>
   KAZHDAN, M., FUNKHOUSER, T., AND RUSINKIEWICZ, S.
  </p>
  <p>
   2004. Symmetry descriptors and 3d shape matching. Proc.
  </p>
  <p>
   Symp. on Geom. Proc..
  </p>
  <p>
   KAZHDAN, M., FUNKHOUSER, T., AND RUSINKIEWICZ, S.
  </p>
  <p>
   2004. Shape matching and anisotropy. In ACM SIGGRAPH
  </p>
  <p>
   2004 Papers, ACM, SIGGRAPH 04, 623629.
  </p>
  <p>
   KAZHDAN, M., FUNKHOUSER, T., AND RUSINKIEWICZ, S.
  </p>
  <p>
   2004. Symmetry descriptors and 3d shape matching. In Proc.
  </p>
  <p>
   Symp. on Geom. Proc., ACM, SGP 04, 115123.
  </p>
  <p>
   KAZHDAN, M., BOLITHO, M., AND HOPPE, H. 2006. Poisson
  </p>
  <p>
   surface reconstruction. In Proc. Symp. on Geom. Proc., SGP 06,
  </p>
  <p>
   6170.
  </p>
  <p>
   KHOLGADE, N., SIMON, T., EFROS, A., AND SHEIKH, Y. 2014.
  </p>
  <p>
   3d object manipulation in a single photograph using stock 3d
  </p>
  <p>
   models. ACM Trans. Graph. 33, 4, 127:1127:12.
  </p>
  <p>
   KILIAN, M.,MITRA, N. J., AND POTTMANN, H. 2007. Geometric
  </p>
  <p>
   modeling in shape space. In ACM SIGGRAPH 2007 papers,
  </p>
  <p>
   ACM, SIGGRAPH 07.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   KIM, V. G., LIPMAN, Y., CHEN, X., AND FUNKHOUSER, T. A.
  </p>
  <p>
   2010. Mobius transformations for global intrinsic symmetry
  </p>
  <p>
   analysis. Comput. Graph. Forum 29, 5, 16891700.
  </p>
  <p>
   KIM, V. G., LIPMAN, Y., AND FUNKHOUSER, T. 2011. Blended
  </p>
  <p>
   intrinsic maps. ACM Trans. Graph. 30, 4, 79:179:12.
  </p>
  <p>
   KIM, V. G., LI, W., MITRA, N. J., DIVERDI, S., AND
  </p>
  <p>
   FUNKHOUSER, T. 2012. Exploring collections of 3D models
  </p>
  <p>
   using fuzzy correspondences. ACM Trans. Graph. 31, 4, 54:1
  </p>
  <p>
   54:11.
  </p>
  <p>
   KIM, Y.M.,MITRA, N. J., YAN, D.-M., AND GUIBAS, L. 2012.
  </p>
  <p>
   Acquiring 3d indoor environments with variability and repetition.
  </p>
  <p>
   ACM Trans. Graph. 31, 6, 138:1138:11.
  </p>
  <p>
   KIM, V. G., LI, W., MITRA, N. J., CHAUDHURI, S., DIVERDI,
  </p>
  <p>
   S., AND FUNKHOUSER, T. 2013. Learning part-based templates
  </p>
  <p>
   from large collections of 3D shapes. ACM Trans. Graph. 32, 4,
  </p>
  <p>
   70:170:12.
  </p>
  <p>
   KIM, V. G., LI, W., MITRA, N. J., CHAUDHURI, S., DIVERDI,
  </p>
  <p>
   S., AND FUNKHOUSER, T. 2013. Learning Part-based Templates
  </p>
  <p>
   from Large Collections of 3D Shapes. Transactions on
  </p>
  <p>
   Graphics (Proc. of SIGGRAPH) 32, 4.
  </p>
  <p>
   KIM, Y. M., MITRA, N. J., HUANG, Q.-X., AND GUIBAS, L. J.
  </p>
  <p>
   2013. Guided real-time scanning of indoor objects. Comput.
  </p>
  <p>
   Graph. Forum 32, 7, 177186.
  </p>
  <p>
   KIM, V. G., CHAUDHURI, S., GUIBAS, L., AND FUNKHOUSER,
  </p>
  <p>
   T. 2014. Shape2Pose: Human-Centric Shape Analysis. ACM
  </p>
  <p>
   Trans. Graph. 33, 4.
  </p>
  <p>
   KIN-CHUNG AU, O., TAI, C.-L., COHEN-OR, D., ZHENG, Y.,
  </p>
  <p>
   AND FU, H. 2010. Electors voting for fast automatic shape
  </p>
  <p>
   correspondence. Computer Graphics Forum 29, 2, 645654.
  </p>
  <p>
   KLEINBERG, J., AND TARDOS, E. 2002. Approximation algorithms
  </p>
  <p>
   for classification problems with pairwise relationships:
  </p>
  <p>
   metric labeling and markov random fields. J. ACM 49, 616639.
  </p>
  <p>
   KOBBELT, L., CAMPAGNA, S., AND SEIDEL, H. P. 1998. A
  </p>
  <p>
   general framework for mesh decimation. Citeseer, 4350.
  </p>
  <p>
   KOKKINOS, I., BRONSTEIN, M., LITMAN, R., AND BRONSTEIN,
  </p>
  <p>
   A. 2012. Intrinsic shape context descriptors for deformable
  </p>
  <p>
   shapes. In Proc. CVPR.
  </p>
  <p>
   KOLLER, D., AND FRIEDMAN, N. 2009. Probabilistic Graphical
  </p>
  <p>
   Models: Principles and Techniques. MIT Press.
  </p>
  <p>
   KOLLER, D., AND FRIEDMAN, N. 2009. Probabilistic Graphical
  </p>
  <p>
   Models: Principles and Techniques. MIT Press.
  </p>
  <p>
   KOLMOGOROV, V. 2006. Convergent tree-reweighted message
  </p>
  <p>
   passing for energy minimization. IEEE Trans. Pattern Anal.
  </p>
  <p>
   Mach. Intell. 28, 15681583.
  </p>
  <p>
   KOMODAKIS, N., PARAGIOS, N., AND TZIRITAS, G. 2007. Mrf
  </p>
  <p>
   optimization via dual decomposition: Message-passing revisited.
  </p>
  <p>
   Computer Vision, IEEE International Conference on 0, 18.
  </p>
  <p>
   KOVNATSKY, A., BRONSTEIN, M. M., BRONSTEIN, A. M.,
  </p>
  <p>
   GLASHOFF, K., AND KIMMEL, R. 2013. Coupled quasiharmonic
  </p>
  <p>
   bases. In Eurographics13, 439448.
  </p>
  <p>
   KRAEVOY, V., AND SHEFFER, A. 2004. Cross-parameterization
  </p>
  <p>
   and compatible remeshing of 3d models. In ACM SIGGRAPH
  </p>
  <p>
   2004 Papers, ACM, SIGGRAPH 04, 861869.
  </p>
  <p>
   KRAEVOY, V., SHEFFER, A., SHAMIR, A., AND COHEN-OR,
  </p>
  <p>
   D. 2008. Non-homogeneous resizing of complex models. In
  </p>
  <p>
   ACM SIGGRAPH Asia 2008 papers, ACM, SIGGRAPH Asia
  </p>
  <p>
   08, 111:1111:9.
  </p>
  <p>
   KREAVOY, V., JULIUS, D., AND SHEFFER, A. 2007. Model composition
  </p>
  <p>
   from interchangeable components. In Proc. of Pacific
  </p>
  <p>
   Graphics, IEEE Computer Society, 129138.
  </p>
  <p>
   KRISHNAN, S., LEE, P. Y., MOORE, J. B., AND VENKATASUBRAMANIAN,
  </p>
  <p>
   S. 2005. Global registration of multiple 3d point
  </p>
  <p>
   sets via optimization-on-a-manifold. SGP 05.
  </p>
  <p>
   KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. 2012.
  </p>
  <p>
   Imagenet classification with deep convolutional neural networks.
  </p>
  <p>
   In Proc. NIPS.
  </p>
  <p>
   KUHN, H., AND TUCKER, A. 1951. Nonlinear programming. In
  </p>
  <p>
   Proceedings of the Second Berkeley Symposium on Mathematical
  </p>
  <p>
   Statistics and Probability, University of California Press,
  </p>
  <p>
   Berkeley, California, J. Neyman, Ed., 481492.
  </p>
  <p>
   KUMAR, M. P., KOLMOGOROV, V., AND TORR, P. H. S. 2009.
  </p>
  <p>
   An analysis of convex relaxations for MAP estimation of discrete
  </p>
  <p>
   MRFs. Journal of Machine Learning Research 10, 71106.
  </p>
  <p>
   LAFFERTY, J. D., MCCALLUM, A., AND PEREIRA, F. C. N.
  </p>
  <p>
   2001. Conditional random fields: Probabilistic models for segmenting
  </p>
  <p>
   and labeling sequence data. 282289.
  </p>
  <p>
   LAGA, H., MORTARA, M., AND SPAGNUOLO, M. 2013. Geometry
  </p>
  <p>
   and context for semantic correspondences and functionality
  </p>
  <p>
   recognition in man-made 3d shapes. ACM Trans. Graph. 32, 5.
  </p>
  <p>
   LAI, Y.-K., HU, S.-M., MARTIN, R. R., AND ROSIN, P. L. 2008.
  </p>
  <p>
   Fast mesh segmentation using random walks. In Proceedings
  </p>
  <p>
   of the 2008 ACM symposium on Solid and physical modeling,
  </p>
  <p>
   ACM, SPM 08, 183191.
  </p>
  <p>
   LAI, K., BO, L., AND FOX, D. 2013. Unsupervised feature learning
  </p>
  <p>
   for 3d scene labeling. In Proc. IEEE Int. Conf. on Rob. and
  </p>
  <p>
   Auto.
  </p>
  <p>
   LEE, J., AND FUNKHOUSER, T. 2008. Sketch-based search and
  </p>
  <p>
   composition of 3d models. In Proc. SBIM.
  </p>
  <p>
   LEE, Y. J., ZITNICK, L., AND COHEN, M. 2011. Shadowdraw:
  </p>
  <p>
   Real-time user guidance for freehand drawing. ACM Trans.
  </p>
  <p>
   Graph. 30, 4, 27:127:9.
  </p>
  <p>
   LENSCH, H. P. A., KAUTZ, J., GOESELE, M., HEIDRICH, W.,
  </p>
  <p>
   AND SEIDEL, H.-P. 2003. Image-based reconstruction of spatial
  </p>
  <p>
   appearance and geometric detail. ACM Trans. Graph. 22, 2, 234
  </p>
  <p>
   257.
  </p>
  <p>
   LEORDEANU, M., AND HEBERT, M. 2005. A spectral technique
  </p>
  <p>
   for correspondence problems using pairwise constraints. ICCV
  </p>
  <p>
   05, 14821489.
  </p>
  <p>
   LEORDEANU, M., AND HEBERT, M. 2006. Efficient map approximation
  </p>
  <p>
   for dense energy functions. 545552.
  </p>
  <p>
   LEVIN, A., FERGUS, R., DURAND, F., AND FREEMAN, W. T.
  </p>
  <p>
   2007. Image and depth from a conventional camera with a coded
  </p>
  <p>
   aperture. In ACM SIGGRAPH 2007 papers, ACM, SIGGRAPH
  </p>
  <p>
   07.
  </p>
  <p>
   LEVY, B., AND ZHANG, H. 2011. Elements of geometry processing.
  </p>
  <p>
   In Invited SIGGRAPH Asia Courses.
  </p>
  <p>
   LI, M., LANGBEIN, F. C., AND MARTIN, R. R. 2006. Constructing
  </p>
  <p>
   regularity feature trees for solid models. In Proc. Geometric
  </p>
  <p>
   Modeling and Processing; LNCS, Springer, 267286.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   LI, H., SUMNER, R. W., AND PAULY, M. 2008. Global correspondence
  </p>
  <p>
   optimization for non-rigid registration of depth scans.
  </p>
  <p>
   Computer Graphics Forum 27, 5, 14211430.
  </p>
  <p>
   LI, H., WEISE, T., AND PAULY, M. 2010. Example-based facial
  </p>
  <p>
   rigging. ACM Trans. Graph. 29, 4, 32:132:6.
  </p>
  <p>
   LI, Y., WU, X., CHRYSATHOU, Y., SHARF, A., COHEN-OR, D.,
  </p>
  <p>
   AND MITRA, N. J. 2011. Globfit: consistently fitting primitives
  </p>
  <p>
   by discovering global relations. In ACM SIGGRAPH 2011
  </p>
  <p>
   papers, ACM, SIGGRAPH 11, 52:152:12.
  </p>
  <p>
   LI, B., GODIL, A., AONO, M., BAI, X., FURUYA, T., LI, L.,
  </p>
  <p>
   LOPEZ-SASTRE, R., JOHAN, H., OHBUCHI, R., REDONDOCABRERA,
  </p>
  <p>
   C., TATSUMA, A., YANAGIMACHI, T., AND
  </p>
  <p>
   ZHANG, S. 2012. Shrec12 track: Generic 3d shape retrieval.
  </p>
  <p>
   Eurographics Workshop on 3D Object Retrieval.
  </p>
  <p>
   LI, H., YU, J., YE, Y., AND BREGLER, C. 2013. Realtime facial
  </p>
  <p>
   animation with on-the-fly correctives. ACM Trans. Graph. 32, 4,
  </p>
  <p>
   42:142:10.
  </p>
  <p>
   LI, B., LU, Y., LI, C., GODIL, A., SCHRECK, T., AONO, M.,
  </p>
  <p>
   CHEN, Q., CHOWDHURY, N. K., FANG, B., FURUYA, T., JOHAN,
  </p>
  <p>
   H., KOSAKA, R., KOYANAGI, H., OHBUCHI, R., AND
  </p>
  <p>
   TATSUMA, A. 2014. Shrec 14 track: Large scale comprehensive
  </p>
  <p>
   3d shape retrieval. Eurographics Workshop on 3D Object
  </p>
  <p>
   Retrieval, 131140.
  </p>
  <p>
   LI, Y., DAI, A., GUIBAS, L., AND NIESSNER, M. 2015.
  </p>
  <p>
   Database-assisted object retrieval for real-time 3d reconstruction.
  </p>
  <p>
   In Computer Graphics Forum, vol. 34.
  </p>
  <p>
   LI, Y., SU, H., QI, C. R., FISH, N., COHEN-OR, D., AND
  </p>
  <p>
   GUIBAS, L. 2015. Joint embeddings of shapes and images via
  </p>
  <p>
   cnn image purification. ACM Trans. Graph..
  </p>
  <p>
   LI, S. Z. 2009. Markov Random Field Modeling in Image Analysis,
  </p>
  <p>
   3rd ed. Springer Publishing Company, Incorporated.
  </p>
  <p>
   LIN, H.-Y. S., LIAO, H.-Y. M., AND LIN, J.-C. 2007. Visual
  </p>
  <p>
   salience-guided mesh decomposition. IEEE Transactions
  </p>
  <p>
   on Multimedia 9, 1, 4657.
  </p>
  <p>
   LIN, Z., CHEN, M., AND MA, Y. 2011. The augmented Lagrange
  </p>
  <p>
   multiplier method for exact recovery of corrupted low-rank matrices.
  </p>
  <p>
   LIPMAN, Y., AND FUNKHOUSER, T. 2009. Mobius voting for
  </p>
  <p>
   surface correspondence. ACM Trans. Graph. 28, 3.
  </p>
  <p>
   LIPMAN, Y., SORKINE, O., LEVIN, D., AND COHEN-OR, D.
  </p>
  <p>
   2005. Linear rotation-invariant coordinates for meshes. In ACM
  </p>
  <p>
   SIGGRAPH 2005 Papers, ACM, SIGGRAPH 05, 479487.
  </p>
  <p>
   LIPP, M.,WONKA, P., AND WIMMER, M. 2008. Interactive visual
  </p>
  <p>
   editing of grammars for procedural architecture. In ACM SIGGRAPH
  </p>
  <p>
   2008 papers, ACM, SIGGRAPH 08, 102:1102:10.
  </p>
  <p>
   LITMAN, R., BRONSTEIN, A. M., BRONSTEIN, M. M., AND
  </p>
  <p>
   CASTELLANI, U. 2014. Supervised learning of bag-of-features
  </p>
  <p>
   shape descriptors using sparse coding. SGP.
  </p>
  <p>
   LIU, R., AND 0002, H. Z. 2007. Mesh segmentation via spectral
  </p>
  <p>
   embedding and contour analysis. Comput. Graph. Forum 26, 3,
  </p>
  <p>
   385394.
  </p>
  <p>
   LIU, Y., LIN, W.-C., AND HAYS, J. 2004. Near-regular texture
  </p>
  <p>
   analysis and manipulation. In ACM SIGGRAPH 2004 Papers,
  </p>
  <p>
   ACM, New York, NY, USA, SIGGRAPH 04, 368376.
  </p>
  <p>
   LIU, Y., POTTMANN, H., WALLNER, J., YANG, Y.-L., AND
  </p>
  <p>
   WANG, W. 2006. Geometric modeling with conical meshes and
  </p>
  <p>
   developable surfaces. In ACM SIGGRAPH 2006 Papers, ACM,
  </p>
  <p>
   New York, NY, USA, SIGGRAPH 06, 681689.
  </p>
  <p>
   LIU, Y., JIN, R., AND YANG, L. 2006. Semi-supervised multilabel
  </p>
  <p>
   learning by constrained non-negative matrix factorization.
  </p>
  <p>
   AAAI06, 421426.
  </p>
  <p>
   LIU, S., MARTIN, R. R., LANGBEIN, F. C., AND ROSIN, P. L.
  </p>
  <p>
   2007. Segmenting periodic reliefs on triangle meshes. In IMA
  </p>
  <p>
   Conference on the Mathematics of Surfaces, 290306.
  </p>
  <p>
   LIU, W., WANG, J., AND CHANG, S.-F. 2012. Robust and scalable
  </p>
  <p>
   graph-based semisupervised learning. Proceedings of the
  </p>
  <p>
   IEEE 100, 9, 26242638.
  </p>
  <p>
   LIU, T., CHAUDHURI, S., KIM, V., HUANG, Q.-X., MITRA,
  </p>
  <p>
   N. J., AND FUNKHOUSER, T. 2014. Creating consistent scene
  </p>
  <p>
   graphs using a probabilistic grammar. ACM Trans. Graph. 33, 6,
  </p>
  <p>
   to appear.
  </p>
  <p>
   LIU, T., HERTZMANN, A., LI, W., AND FUNKHOUSER, T. 2015.
  </p>
  <p>
   Style compatibility for 3d furniture models. ACM Trans. Graph.
  </p>
  <p>
   34, 4, 85:185:9.
  </p>
  <p>
   LJUP CO TODOROVSKI, S. D. 2006. Integrating knowledge-driven
  </p>
  <p>
   and data-driven approaches to modeling. Ecological Modelling
  </p>
  <p>
   194, 13, 313.
  </p>
  <p>
   LLOYD, S. P. 1982. Least squares quantization in pcm. IEEE
  </p>
  <p>
   Transactions on Information Theory 28, 129137.
  </p>
  <p>
   LOEFF, N., FARHADI, A., ENDRES, I., AND FORSYTH, D. 2009.
  </p>
  <p>
   Unlabeled data improves word prediction. In ICCV09, 956962.
  </p>
  <p>
   LOPER, M.M., MAHMOOD, N., AND BLACK, M. J. 2014. MoSh:
  </p>
  <p>
   Motion and shape capture from sparse markers. ACM Trans.
  </p>
  <p>
   Graph. 33, 6, 220:1220:13.
  </p>
  <p>
   LUENBERGER, D. G., AND YE, Y. 2008. Linear and Nonlinear
  </p>
  <p>
   Programming, Third Edition. Springer.
  </p>
  <p>
   LUN, Z., KALOGERAKIS, E., AND SHEFFER, A. 2015. Elements
  </p>
  <p>
   of style: Learning perceptual shape style similarity. ACM Trans.
  </p>
  <p>
   Graph. 34, 4, 84:184:14.
  </p>
  <p>
   LV, J., CHEN, X., HUANG, J., AND BAO, H. 2012. Semisupervised
  </p>
  <p>
   mesh segmentation and labeling. Comp. Graph. Forum
  </p>
  <p>
   31, 7-2.
  </p>
  <p>
   MA, C., HUANG, H., SHEFFER, A., KALOGERAKIS, E., AND
  </p>
  <p>
   WANG, R. 2014. Analogy-driven 3d style transfer. Computer
  </p>
  <p>
   Graphics Forum 33, 2, 175184.
  </p>
  <p>
   MANGAN, A. P., AND WHITAKER, R. T. 1999. Partitioning 3d
  </p>
  <p>
   surface meshes using watershed segmentation. IEEE Trans. Vis.
  </p>
  <p>
   Comput. Graph. 5, 308321.
  </p>
  <p>
   MANNING, C. D., RAGHAVAN, P., AND SCH UTZE, H. 2008.
  </p>
  <p>
   Introduction to Information Retrieval. Cambridge University
  </p>
  <p>
   Press, New York, NY, USA.
  </p>
  <p>
   MARANDE, W., AND BURGER, G. 2007. Mitochondrial dna as a
  </p>
  <p>
   genomic jigsaw puzzle. Science 318, 415.
  </p>
  <p>
   MARKOVSKY, I. 2012. Low Rank Approximation: Algorithms,
  </p>
  <p>
   Implementation, Applications. Springer.
  </p>
  <p>
   MARTINET, A., SOLER, C., HOLZSCHUCH, N., AND SILLION,
  </p>
  <p>
   F. X. 2006. Accurate detection of symmetries in 3d shapes.
  </p>
  <p>
   ACM Trans. Graph. 25, 439464.
  </p>
  <p>
   MARTINET, A. 2007. Structuring 3D Geometry based on Symmetry
  </p>
  <p>
   and Instancing Information. These, Institut National Polytechnique
  </p>
  <p>
   de Grenoble - INPG.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   MATTAUSCH, O., PANOZZO, D., MURA, C., SORKINEHORNUNG,
  </p>
  <p>
   O., AND PAJAROLA, R. 2014. Object detection
  </p>
  <p>
   and classification from large-scale cluttered indoor scans. Computer
  </p>
  <p>
   Graphics Forum 33, 2.
  </p>
  <p>
   MCCLURE, S. G., AND D.E. 1987. Statistical methods for tomographic
  </p>
  <p>
   image reconstruction. In Proceedings of the 46th Session
  </p>
  <p>
   of the International Statistical Institute, Bulletin of the ISI, 52.
  </p>
  <p>
   MEMOLI, F., AND SAPIRO, G. 2005. A theoretical and computational
  </p>
  <p>
   framework for isometry invariant recognition of point
  </p>
  <p>
   cloud data. Foundations of Computational Mathematics 5, 3,
  </p>
  <p>
   313347.
  </p>
  <p>
   MERRELL, P., SCHKUFZA, E., LI, Z., AGRAWALA, M., AND
  </p>
  <p>
   KOLTUN, V. 2011. Interactive furniture layout using interior
  </p>
  <p>
   design guidelines. ACM Trans. Graph. 30, 4, 87:187:9.
  </p>
  <p>
   MERRELL, P. 2007. Example-based model synthesis. In Proc.
  </p>
  <p>
   I3D, 105112.
  </p>
  <p>
   MILLER, G. A. 1995. Wordnet: A lexical database for english.
  </p>
  <p>
   Communications of the ACM 38, 3941.
  </p>
  <p>
   MIN, P., HALDERMAN, A., KAZHDAN, M., AND FUNKHOUSER,
  </p>
  <p>
   T. 2003. Early experiences with a 3D model search engine. In
  </p>
  <p>
   Web3D Symposium.
  </p>
  <p>
   MIN, P., KAZHDAN, M., AND FUNKHOUSER, T. 2004. A comparison
  </p>
  <p>
   of text and shape matching for retrieval of online 3D
  </p>
  <p>
   models. In European Conference on Digital Libraries.
  </p>
  <p>
   MITCHELL, J. S. B., MOUNT, D. M., AND PAPADIMITRIOU,
  </p>
  <p>
   C. H. 1987. The discrete geodesic problem. SIAM J. Comput.
  </p>
  <p>
   16, 4.
  </p>
  <p>
   MITRA, N. J., GUIBAS, L. J., AND PAULY, M. 2006. Partial and
  </p>
  <p>
   approximate symmetry detection for 3d geometry. SIGGRAPH
  </p>
  <p>
   06, 560568.
  </p>
  <p>
   MITRA, N. J., BRONSTEIN, A. M., AND BRONSTEIN, M. M.
  </p>
  <p>
   2010. Intrinsic regularity detection in 3d geometry. In Computer
  </p>
  <p>
   Vision - ECCV 2010, 11th European Conference on Computer
  </p>
  <p>
   Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings,
  </p>
  <p>
   Part III, 398410.
  </p>
  <p>
   MITRA, N., WAND, M., ZHANG, H., COHEN-OR, D., KIM, V.,
  </p>
  <p>
   AND HUANG, Q.-X. 2014. Structure-aware shape processing.
  </p>
  <p>
   SIGGRAPH Course.
  </p>
  <p>
   MOREELS, P., AND PERONA, P. 2007. Evaluation of features
  </p>
  <p>
   detectors and descriptors based on 3d objects. Int. J. Comput.
  </p>
  <p>
   Vision 73, 3, 263284.
  </p>
  <p>
   MULLER, P., WONKA, P., HAEGLER, S., ULMER, A., AND
  </p>
  <p>
   VAN GOOL, L. 2006. Procedural modeling of buildings. ACM
  </p>
  <p>
   Trans. Graph., 614623.
  </p>
  <p>
   MULLER, P., ZENG, G.,WONKA, P., AND VAN GOOL, L. 2007.
  </p>
  <p>
   Image-based procedural modeling of facades. In ACM SIGGRAPH
  </p>
  <p>
   2007 papers, ACM, New York, NY, USA, SIGGRAPH
  </p>
  <p>
   07.
  </p>
  <p>
   MURRAY, S. O., KERSTEN, D., OLSHAUSEN, B. A., SCHRATER,
  </p>
  <p>
   P., AND WOODS, D. L. 2002. Shape perception reduces activity
  </p>
  <p>
   in human primary visual cortex. Proc. Natl. Acad. Sci. 99, 23,
  </p>
  <p>
   1516415169.
  </p>
  <p>
   NAN, L., XIE, K., AND SHARF, A. 2012. A search-classify approach
  </p>
  <p>
   for cluttered indoor scene understanding. ACM Trans.
  </p>
  <p>
   Graph. 31, 6, 137:1137:10.
  </p>
  <p>
   NATTERER, J., BURGER, N., AND MULLER, A. 2002. The roof
  </p>
  <p>
   structure Expodach at theworld exhibition Hannover. In Proc.
  </p>
  <p>
   5th Intl. Conf. Space Structures, 185193.
  </p>
  <p>
   NG, A. Y., JORDAN, M. I., AND WEISS, Y. 2001. On spectral
  </p>
  <p>
   clustering: Analysis and an algorithm. In ADVANCES
  </p>
  <p>
   IN NEURAL INFORMATION PROCESSING SYSTEMS, MIT
  </p>
  <p>
   Press, vol. 14, 849856.
  </p>
  <p>
   NG, A. Y. 2004. Feature selection, l1 vs. l2 regularization, and
  </p>
  <p>
   rotational invariance. In ICML.
  </p>
  <p>
   NGUYEN, G. P., AND WORRING, M. 2008. Interactive access to
  </p>
  <p>
   large image collections using similarity-based visualization. J.
  </p>
  <p>
   Visual Lang. Comp. 19, 2, 203224.
  </p>
  <p>
   NGUYEN, A., BEN-CHEN, M., WELNICKA, K., YE, Y., AND
  </p>
  <p>
   GUIBAS, L. 2011. An optimization approach to improving collections
  </p>
  <p>
   of shape maps. Computer Graphics Forum 30, 5, 1481
  </p>
  <p>
   1491.
  </p>
  <p>
   NOCEDAL, J., AND WRIGHT, S. J. 2006. Numerical Optimization.
  </p>
  <p>
   Springer.
  </p>
  <p>
   NOVOTNI, M., AND KLEIN, R. 2003. 3d zernike descriptors for
  </p>
  <p>
   content based shape retrieval. solid modeling.
  </p>
  <p>
   OHBUCHI, R., AND FURUYA, T. 2010. Distance metric learning
  </p>
  <p>
   and feature combination for shape-based 3d model retrieval.
  </p>
  <p>
   In Proceedings of the ACM Workshop on 3D Object Retrieval,
  </p>
  <p>
   ACM, New York, NY, USA, 3DOR 10, 6368.
  </p>
  <p>
   OSADA, R., FUNKHOUSER, T., CHAZELLE, B., AND DOBKIN,
  </p>
  <p>
   D. 2002. Shape distributions. ACM Trans. Graph. 21, 807832.
  </p>
  <p>
   OVSJANIKOV, M., SUN, J., AND GUIBAS, L. J. 2008. Global
  </p>
  <p>
   intrinsic symmetries of shapes. Comput. Graph. Forum 27, 5,
  </p>
  <p>
   13411348.
  </p>
  <p>
   OVSJANIKOV, M., MERIGOT, Q., MEMOLI, F., AND GUIBAS,
  </p>
  <p>
   L. J. 2010. One point isometric matching with the heat kernel.
  </p>
  <p>
   Computer Graphics Forum 29, 5, 15551564.
  </p>
  <p>
   OVSJANIKOV, M., LI, W., GUIBAS, L., AND MITRA, N. J. 2011.
  </p>
  <p>
   Exploration of continuous variability in collections of 3d shapes.
  </p>
  <p>
   ACM Trans. Graph. 30, 4, 33:133:10.
  </p>
  <p>
   OVSJANIKOV, M., BEN-CHEN, M., SOLOMON, J., BUTSCHER,
  </p>
  <p>
   A., AND GUIBAS, L. 2012. Functional maps: A flexible representation
  </p>
  <p>
   of maps between shapes. ACM Trans. Graph. 31, 4,
  </p>
  <p>
   30:130:11.
  </p>
  <p>
   OVSJANIKOV, M., BEN-CHEN, M., SOLOMON, J., BUTSCHER,
  </p>
  <p>
   A., AND GUIBAS, L. 2012. Functional maps: A flexible representation
  </p>
  <p>
   of maps between shapes. ACM Trans. Graph. 31,
  </p>
  <p>
   4.
  </p>
  <p>
   OVSJANIKOV, M., BEN-CHEN, M., CHAZAL, F., AND GUIBAS,
  </p>
  <p>
   L. J. 2013. Analysis and visualization of maps between shapes.
  </p>
  <p>
   Comput. Graph. Forum 32, 6, 135145.
  </p>
  <p>
   PACHAURI, D., KONDOR, R., AND SINGH, V. 2013. Solving the
  </p>
  <p>
   multi-way matching problem by permutation synchronization. In
  </p>
  <p>
   Proc. NIPS, 18601868.
  </p>
  <p>
   PALMER, S. E. 1999. Vision Science: Photons to Phenomenology.
  </p>
  <p>
   MIT Press.
  </p>
  <p>
   PAN, S. J., AND YANG, Q. 2010. A survey on transfer learning.
  </p>
  <p>
   IEEE Trans. Knowl. Data Eng. 22, 10, 13451359.
  </p>
  <p>
   PARK, M., BROCKLEHURST, K., COLLINS, R. T., AND LIU, Y.
  </p>
  <p>
   2009. Deformed lattice detection in real-world images using
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   mean-shift belief propagation. IEEE Trans. Pattern Anal. Mach.
  </p>
  <p>
   Intell. 31, 18041816.
  </p>
  <p>
   PAULOVICH, F., ELER, D., POCO, J., BOTHA, C., MINGHIM,
  </p>
  <p>
   R., AND NONATO, L. 2011. Piecewise laplacian-based projection
  </p>
  <p>
   for interactive data exploration and organization. Computer
  </p>
  <p>
   Graphics Forum 30, 3, 10911100.
  </p>
  <p>
   PAULY, M., KEISER, R., AND GROSS, M. 2003. Multi-scale
  </p>
  <p>
   feature extraction on point-sampled surfaces. Comput. Graph.
  </p>
  <p>
   Forum (Proc. Eurographics) 22, 3, 281290.
  </p>
  <p>
   PAULY, M., MITRA, N. J., GIESEN, J., GROSS, M., AND
  </p>
  <p>
   GUIBAS, L. J. 2005. Example-based 3d scan completion. In
  </p>
  <p>
   Proc. Symp. on Geom. Proc., SGP 05.
  </p>
  <p>
   PAULY, M., MITRA, N. J., GIESEN, J., GROSS, M., AND
  </p>
  <p>
   GUIBAS, L. J. 2005. Example-based 3d scan completion. In
  </p>
  <p>
   Proc. Symp. on Geom. Proc., SGP 05.
  </p>
  <p>
   PAULY, M., MITRA, N. J., WALLNER, J., POTTMANN, H., AND
  </p>
  <p>
   GUIBAS, L. J. 2008. Discovering structural regularity in 3d
  </p>
  <p>
   geometry. In ACM SIGGRAPH 2008 papers, ACM, New York,
  </p>
  <p>
   NY, USA, SIGGRAPH 08, 43:143:11.
  </p>
  <p>
   PEKELNY, YURI, GOTSMAN, AND CRAIG. 2008. Articulated object
  </p>
  <p>
   reconstruction and markerless motion capture from depth
  </p>
  <p>
   video. Computer Graphics Forum 27, 2, 399408.
  </p>
  <p>
   PIRAZZI, C., AND WEINAND, Y. 2006. Geodesic lines on freeform
  </p>
  <p>
   surfaces:optimized grids for timber rib shells. In Proc.
  </p>
  <p>
   World Conference on Timber Engineering. 7pp.
  </p>
  <p>
   PODOLAK, J., SHILANE, P., GOLOVINSKIY, A., RUSINKIEWICZ,
  </p>
  <p>
   S., , AND FUNKHOUSER, T. 2006. A planar-reflective symmetry
  </p>
  <p>
   transform for 3d shapes. ACM Trans. Graph. 25, 3.
  </p>
  <p>
   PODOLAK, J., SHILANE, P., GOLOVINSKIY, A., RUSINKIEWICZ,
  </p>
  <p>
   S., AND FUNKHOUSER, T. 2006. A planar-reflective symmetry
  </p>
  <p>
   transform for 3d shapes. In ACM SIGGRAPH 2006 Papers,
  </p>
  <p>
   ACM, New York, NY, USA, SIGGRAPH 06, 549559.
  </p>
  <p>
   PONS-MOLL, G., ROMERO, J., MAHMOOD, N., AND BLACK,
  </p>
  <p>
   M. 2015. Dyna: A model of dynamic human shape in motion.
  </p>
  <p>
   ACM Trans. Graph. 34, 4, to appear.
  </p>
  <p>
   POTTMANN, H., LIU, Y., WALLNER, J., BOBENKO, A., AND
  </p>
  <p>
   WANG, W. 2007. Geometry of multi-layer freeform structures
  </p>
  <p>
   for architecture. In ACM SIGGRAPH 2007 papers, ACM, New
  </p>
  <p>
   York, NY, USA, SIGGRAPH 07.
  </p>
  <p>
   POTTMANN, H., SCHIFTNER, A., BO, P., SCHMIEDHOFER, H.,
  </p>
  <p>
   WANG, W., BALDASSINI, N., AND WALLNER, J. 2008.
  </p>
  <p>
   Freeform surfaces from single curved panels. In ACM SIGGRAPH
  </p>
  <p>
   2008 papers, ACM, New York, NY, USA, SIGGRAPH
  </p>
  <p>
   08, 76:176:10.
  </p>
  <p>
   POTTMANN, H., WALLNER, J., HUANG, Q.-X., AND YANG, Y.-
  </p>
  <p>
   L. 2009. Integral invariants for robust geometry processing.
  </p>
  <p>
   Comput. Aided Geom. Des. 26, 3760.
  </p>
  <p>
   POTTMANN, H., HUANG, Q., DENG, B., SCHIFTNER, A., KILIAN,
  </p>
  <p>
   M., GUIBAS, L., AND WALLNER, J. 2010. Geodesic
  </p>
  <p>
   patterns. In ACM SIGGRAPH 2010 papers, ACM, New York,
  </p>
  <p>
   NY, USA, SIGGRAPH 10, 43:143:10.
  </p>
  <p>
   QUATTONI, A., AND TORRALBA, A. 2009. Recognizing indoor
  </p>
  <p>
   scenes. In Proc. CVPR, 413420.
  </p>
  <p>
   RAND, W. M. 1971. Objective criteria for the evaluation of clustering
  </p>
  <p>
   methods. Journal of the American Statistical Association
  </p>
  <p>
   66, 336, 846850.
  </p>
  <p>
   RAVIKUMAR, P., AGARWAL, A., AND WAINWRIGHT, M. J.
  </p>
  <p>
   2010. Message-passing for graph-structured linear programs:
  </p>
  <p>
   Proximal methods and rounding schemes. J. Mach. Learn. Res.
  </p>
  <p>
   11, 10431080.
  </p>
  <p>
   RAY, N., LI, W. C., LEVY, B., SHEFFER, A., AND ALLIEZ, P.
  </p>
  <p>
   2006. Periodic global parameterization. ACM Trans. Graph. 25,
  </p>
  <p>
   14601485.
  </p>
  <p>
   REN, X., AND MALIK, J. 2003. Learning a classification model
  </p>
  <p>
   for segmentation. In Proceedings of the Ninth IEEE International
  </p>
  <p>
   Conference on Computer Vision - Volume 2, IEEE Computer
  </p>
  <p>
   Society, Washington, DC, USA, ICCV 03, 1017.
  </p>
  <p>
   REN, X., AND RAMANAN, D. 2013. Histograms of sparse codes
  </p>
  <p>
   for object detection. In Proc. CVPR, 32463253.
  </p>
  <p>
   RESENDE, M. G. C., RAMAKRISHNAN, K. G., AND DREZNER,
  </p>
  <p>
   Z. 1995. Computing lower bounds for the quadratic assignment
  </p>
  <p>
   problem with an interior point algorithm for linear programming.
  </p>
  <p>
   Operations Research 43, 781791.
  </p>
  <p>
   RIFKIN, R., AND KLAUTAU, A. 2004. In defense of one-vs-all
  </p>
  <p>
   classification. J. Mach. Learn. Res. 5, 101141.
  </p>
  <p>
   RITCHIE, D. W., AND KEMP, G. J. L. 1999. Protein docking
  </p>
  <p>
   using spherical polar fourier correlations. Proteins 39, 178194.
  </p>
  <p>
   ROBERTS, R., SINHA, S. N., SZELISKI, R., AND STEEDLY, D.
  </p>
  <p>
   2011. Structure from motion for scenes with large duplicate
  </p>
  <p>
   structures. In Proc. CVPR, 31373144.
  </p>
  <p>
   ROTHER, C., MINKA, T., BLAKE, A., AND KOLMOGOROV, V.
  </p>
  <p>
   2006. Cosegmentation of image pairs by histogram matching -
  </p>
  <p>
   incorporating a global constraint into mrfs. In Proceedings of the
  </p>
  <p>
   2006 IEEE Computer Society Conference on Computer Vision
  </p>
  <p>
   and Pattern Recognition - Volume 1, IEEE Computer Society,
  </p>
  <p>
   Washington, DC, USA, 9931000.
  </p>
  <p>
   RUBNER, Y., TOMASI, C., AND GUIBAS, L. J. 2000. The earth
  </p>
  <p>
   movers distance as a metric for image retrieval. Int. J. Comput.
  </p>
  <p>
   Vision 40, 2, 99121.
  </p>
  <p>
   RUSSELL, B., EFROS, A., SIVIC, J., FREEMAN, W., AND ZISSERMAN,
  </p>
  <p>
   A. 2009. Segmenting scenes by matching image
  </p>
  <p>
   composites. Advances in Neural Information Processing Systems,
  </p>
  <p>
   19.
  </p>
  <p>
   RUSTAMOV, R. M., OVSJANIKOV, M., AZENCOT, O., BENCHEN,
  </p>
  <p>
   M., CHAZAL, F., AND GUIBAS, L. 2013. Map-based
  </p>
  <p>
   exploration of intrinsic shape differences and variability. ACM
  </p>
  <p>
   Trans. Graph. 32, 4, 72:172:12.
  </p>
  <p>
   SALAS-MORENO, R. F., NEWCOMBE, R. A., STRASDAT, H.,
  </p>
  <p>
   KELLY, P. H. J., AND DAVISON, A. J. 2013. SLAM++: simultaneous
  </p>
  <p>
   localisation and mapping at the level of objects. In
  </p>
  <p>
   Proc. CVPR, IEEE, 13521359.
  </p>
  <p>
   SATKIN, S., LIN, J., AND HEBERT, M. 2012. Data-driven scene
  </p>
  <p>
   understanding from 3d models. In Proc. BMVC, 128:1128:11.
  </p>
  <p>
   SAUPE, D., AND VRANIC, D. V. 2001. 3d model retrieval with
  </p>
  <p>
   spherical harmonics and moments. In Proceedings of the 23rd
  </p>
  <p>
   DAGM-Symposium on Pattern Recognition, 392397.
  </p>
  <p>
   SAXENA, A., SUN, M., AND NG, A. Y. 2009. Make3d: Learning
  </p>
  <p>
   3d scene structure from a single still image. IEEE Trans. Pattern
  </p>
  <p>
   Anal. Mach. Intell. 31, 5, 824840.
  </p>
  <p>
   SCHAEFER, S., AND YUKSEL, C. 2007. Example-based skeleton
  </p>
  <p>
   extraction. In Proc. Symp. on Geom. Proc., 153162.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   SCHAEFER, S., AND YUKSEL, C. 2007. Example-based skeleton
  </p>
  <p>
   extraction. In Proc. Symp. on Geom. Proc., 153162.
  </p>
  <p>
   SCHAFFALITZKY, F., AND ZISSERMAN, A. 1999. Geometric
  </p>
  <p>
   grouping of repeated elements within images. In Shape, Contour
  </p>
  <p>
   and Grouping in Computer Vision, Springer-Verlag, London,
  </p>
  <p>
   UK, 165181.
  </p>
  <p>
   SCHLESINGER, D., AND FLACH, B. 2006. Transforming an arbitrary
  </p>
  <p>
   minsum problem into a binary one. Dresden University of
  </p>
  <p>
   Technology Tech Rep, Technical Report TUD-FI06-01, 116.
  </p>
  <p>
   SCHNEIDMAN-DUHOVNY, D., INBAR, Y., NUSSINOV, R., AND
  </p>
  <p>
   WOLFSON, H. J. 2005. Geometry-based flexible and symmetric
  </p>
  <p>
   protein docking. Proteins 60, 2, 224231.
  </p>
  <p>
   SCHREINER, J., ASIRVATHAM, A., PRAUN, E., AND HOPPE, H.
  </p>
  <p>
   2004. Inter-surface mapping. In ACM SIGGRAPH 2004 Papers,
  </p>
  <p>
   ACM, New York, NY, USA, SIGGRAPH 04, 870877.
  </p>
  <p>
   SCHRIJVER, A. 1986. Theory of linear and integer programming.
  </p>
  <p>
   John Wiley &amp; Sons, Inc., New York, NY, USA.
  </p>
  <p>
   SCHULZ, A., SHAMIR, A., LEVIN, D. I.W., SITTHI-AMORN, P.,
  </p>
  <p>
   AND MATUSIK, W. 2014. Design and fabrication by example.
  </p>
  <p>
   ACM Trans. Graph. 33, 4.
  </p>
  <p>
   SECORD, A., LU, J., FINKELSTEIN, A., SINGH, M., AND
  </p>
  <p>
   NEALEN, A. 2011. Perceptual models of viewpoint preference.
  </p>
  <p>
   ACM Trans. Graph. 30, 5.
  </p>
  <p>
   SEDERBERG, T. W., AND PARRY, S. R. 1986. Free-form deformation
  </p>
  <p>
   of solid geometric models. SIGGRAPH 86, 151160.
  </p>
  <p>
   SHAKHNAROVICH, G., VIOLA, P., AND DARRELL, T. 2003. Fast
  </p>
  <p>
   pose estimation with parameter-sensitive hashing. In Proceedings
  </p>
  <p>
   of the Ninth IEEE International Conference on Computer
  </p>
  <p>
   Vision - Volume 2, IEEE Computer Society, Washington, DC,
  </p>
  <p>
   USA, ICCV 03, 750.
  </p>
  <p>
   SHAMIR, A. 2008. A survey on mesh segmentation techniques.
  </p>
  <p>
   Comput. Graph. Forum 27, 6, 15391556.
  </p>
  <p>
   SHAO, T., XU, W., ZHOU, K., WANG, J., LI, D., AND GUO, B.
  </p>
  <p>
   2012. An interactive approach to semantic modeling of indoor
  </p>
  <p>
   scenes with an rgbd camera. ACM Trans. Graph. 31, 6, 136:1
  </p>
  <p>
   136:11.
  </p>
  <p>
   SHAPIRA, L., SHAMIR, A., AND COHEN-OR, D. 2008. Consistent
  </p>
  <p>
   mesh partitioning and skeletonisation using the shape diameter
  </p>
  <p>
   function. Vis. Comput. 24, 249259.
  </p>
  <p>
   SHAPIRA, L., SHALOM, S., SHAMIR, A., ZHANG, R. H., AND
  </p>
  <p>
   COHEN-OR, D. 2010. Contextual Part Analogies in 3D Objects.
  </p>
  <p>
   International Journal of Computer Vision 89, 2, 309315.
  </p>
  <p>
   SHARF, A., LEWINER, T., SHKLARSKI, G., TOLEDO, S., AND
  </p>
  <p>
   COHEN-OR, D. 2007. Interactive topology-aware surface reconstruction.
  </p>
  <p>
   In ACM SIGGRAPH 2007 papers, ACM, SIGGRAPH
  </p>
  <p>
   07.
  </p>
  <p>
   SHEN, C.-H., FU, H., CHEN, K., AND HU, S.-M. 2012. Structure
  </p>
  <p>
   recovery by part assembly. ACM Trans. Graph. 31, 6, 180:1
  </p>
  <p>
   180:11.
  </p>
  <p>
   SHI, J., AND MALIK, J. 2000. Normalized cuts and image segmentation.
  </p>
  <p>
   IEEE Trans. Pattern Anal. Mach. Intell. 22, 888905.
  </p>
  <p>
   SHILANE, P., AND FUNKHOUSER, T. 2007. Distinctive regions of
  </p>
  <p>
   3d surfaces. ACM Trans. Graph. 26, 2.
  </p>
  <p>
   SHILANE, P., MIN, P., KAZHDAN, M., AND FUNKHOUSER, T.
  </p>
  <p>
   2004. The princeton shape benchmark. In Proc. SMI, IEEE
  </p>
  <p>
   Computer Society, Washington, DC, USA, 167178.
  </p>
  <p>
   SHILANE, P., MIN, P., KAZHDAN, M., AND FUNKHOUSER, T.
  </p>
  <p>
   2004. The Princeton shape benchmark. In Shape Modeling International.
  </p>
  <p>
   SHLAFMAN, S., TAL, A., AND KATZ, S. 2002. Metamorphosis
  </p>
  <p>
   of polyhedral surfaces using decomposition. Comput. Graph.
  </p>
  <p>
   Forum 21, 3.
  </p>
  <p>
   SHOTTON, J.,WINN, J., ROTHER, C., AND CRIMINISI, A. 2009.
  </p>
  <p>
   Textonboost for image understanding: Multi-class object recognition
  </p>
  <p>
   and segmentation by jointly modeling texture, layout, and
  </p>
  <p>
   context. Int. J. Comput. Vision 81, 223.
  </p>
  <p>
   SHOTTON, J., FITZGIBBON, A., COOK, M., SHARP, T., FINOCCHIO,
  </p>
  <p>
   M., MOORE, R., KIPMAN, A., AND BLAKE, A. 2011.
  </p>
  <p>
   Real-time human pose recognition in parts from single depth images.
  </p>
  <p>
   In Proc. CVPR.
  </p>
  <p>
   SIDI, O., VAN KAICK, O., KLEIMAN, Y., ZHANG, H., AND
  </p>
  <p>
   COHEN-OR, D. 2011. Unsupervised co-segmentation of a set
  </p>
  <p>
   of shapes via descriptor-space spectral clustering. ACM Trans.
  </p>
  <p>
   Graph. 30, 6, 126:1126:10.
  </p>
  <p>
   SILBERMAN, N., KOHLI, P., HOIEM, D., AND FERGUS, R. 2012.
  </p>
  <p>
   Indoor segmentation and support inference from rgbd images. In
  </p>
  <p>
   Proc. ECCV.
  </p>
  <p>
   SIMARI, P., KALOGERAKIS, E., AND SINGH, K. 2006. Folding
  </p>
  <p>
   meshes: hierarchical mesh segmentation based on planar symmetry.
  </p>
  <p>
   In Proc. Symp. on Geom. Proc., SGP 06, 111119.
  </p>
  <p>
   SIMARI, P. D., NOWROUZEZAHRAI, D., KALOGERAKIS, E.,
  </p>
  <p>
   AND SINGH, K. 2009. Multi-objective shape segmentation and
  </p>
  <p>
   labeling. Computer Graphics Forum 28, 5, 14151425.
  </p>
  <p>
   SIMS, K. 1991. Artificial evolution for computer graphics. In Proc.
  </p>
  <p>
   of SIGGRAPH, 319328.
  </p>
  <p>
   SINGER, A., AND WU, H.-T., 2011. Vector diffusion maps and
  </p>
  <p>
   the connection laplacian.
  </p>
  <p>
   SNAVELY, N., SEITZ, S. M., AND SZELISKI, R. 2006. Photo
  </p>
  <p>
   tourism: Exploring image collections in 3d. ACM Trans. Graph.
  </p>
  <p>
   25, 3, 835846.
  </p>
  <p>
   SNAVELY, N., SEITZ, S. M., AND SZELISKI, R. 2006. Photo
  </p>
  <p>
   tourism: Exploring photo collections in 3d. ACM Trans. Graph.
  </p>
  <p>
   25, 3, 835846.
  </p>
  <p>
   SOCHER, R., HUVAL, B., BHAT, B., MANNING, C. D., AND NG,
  </p>
  <p>
   A. Y. 2012. Convolutional-recursive deep learning for 3d object
  </p>
  <p>
   classification. In Proc. NIPS, 665673.
  </p>
  <p>
   SOLOMON, J., BEN-CHEN, M., BUTSCHER, A., AND GUIBAS,
  </p>
  <p>
   L. J. 2011. Discovery of intrinsic primitives on triangle meshes.
  </p>
  <p>
   Comput. Graph. Forum 30, 2, 365374.
  </p>
  <p>
   SOLOMON, J., NGUYEN, A., BUTSCHER, A., BEN-CHEN, M.,
  </p>
  <p>
   AND GUIBAS, L. 2012. Soft maps between surfaces. In Proc.
  </p>
  <p>
   SGP 2012.
  </p>
  <p>
   SOLOMON, J., NGUYEN, A., BUTSCHER, A., BEN-CHEN, M.,
  </p>
  <p>
   AND GUIBAS, L. 2012. Soft maps between surfaces. Computer
  </p>
  <p>
   Graphics Forum 31, 5, 16171626.
  </p>
  <p>
   SOLOMON, J. 2015. Numerical algorithms. AK Peters/CRC Press.
  </p>
  <p>
   SONG, S., AND XIAO, J. 2014. Sliding shapes for 3D object
  </p>
  <p>
   detection in depth images. In Proc. ECCV.
  </p>
  <p>
   SONTAG, D., AND JAAKKOLA, T. 2009. Tree block coordinate descent
  </p>
  <p>
   for MAP in graphical models. Journal of Machine Learning
  </p>
  <p>
   Research - Proceedings Track 5, 544551.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   SORKINE, O., AND ALEXA, M. 2007. As-rigid-as-possible surface
  </p>
  <p>
   modeling. In Proc. Symp. on Geom. Proc., 109116.
  </p>
  <p>
   SORKINE, O., COHEN-OR, D., LIPMAN, Y., ALEXA, M.,
  </p>
  <p>
   R OSSL, C., AND SEIDEL, H.-P. 2004. Laplacian surface editing.
  </p>
  <p>
   In Proc. Symp. on Geom. Proc., ACM, New York, NY, USA,
  </p>
  <p>
   SGP 04, 175184.
  </p>
  <p>
   SPUYBROEK, L. 2004. NOX: Machining Architecture. Thames &amp;
  </p>
  <p>
   Hudson.
  </p>
  <p>
   STAVA, O., BENES, B., MECH, R., ALIGA, D., AND KRISTOF,
  </p>
  <p>
   P. 2010. Inverse procedural modeling by automatic generation
  </p>
  <p>
   of L-systems. Computer Graphics Forum 29, 2, 665674.
  </p>
  <p>
   SU, H., HUANG, Q.-X., MITRA, N. J., LI, Y., AND GUIBAS,
  </p>
  <p>
   L. J. 2014. Estimating image depth using shape collections.
  </p>
  <p>
   ACM Trans. Graph. 33, 4, 37:137:11.
  </p>
  <p>
   SU, H., MAJI, S., KALOGERAKIS, E., AND LEARNED-MILLER,
  </p>
  <p>
   E. 2015. Multi-view convolutional neural networks for 3d shape
  </p>
  <p>
   recognition. In Proc. ICCV.
  </p>
  <p>
   SU, H., QI, C. R., LI, Y., AND GUIBAS, L. J. 2015. Render
  </p>
  <p>
   for cnn: Viewpoint estimation in images using cnns trained with
  </p>
  <p>
   rendered 3d model views. In The IEEE International Conference
  </p>
  <p>
   on Computer Vision (ICCV).
  </p>
  <p>
   SU, H., SAVVA, M., YI, L., CHANG, A. X., SONG, S.,
  </p>
  <p>
   YU, F., LI, Z., XIAO, J., HUANG, Q., SAVARESE, S.,
  </p>
  <p>
   FUNKHOUSER, T., HANRAHAN, P., AND GUIBAS, L. J.,
  </p>
  <p>
   2015. ShapeNet: An information-rich 3d model repository.
  </p>
  <p>
   http://shapenet.cs.stanford.edu.
  </p>
  <p>
   SU, H., QI, C., NIESSNER, M., DAI, A., YAN, M., AND GUIBAS,
  </p>
  <p>
   L. 2016. Volumetric and multi-view cnns for object classification
  </p>
  <p>
   on 3d data. CVPR.
  </p>
  <p>
   SUMNER, R. W., AND POPOVIC
  </p>
  <p>
   , J. 2004. Deformation transfer
  </p>
  <p>
   for triangle meshes. ACM Trans. Graph. 23, 3, 399405.
  </p>
  <p>
   SUMNER, R.W., SCHMID, J., AND PAULY, M. 2007. Embedded
  </p>
  <p>
   deformation for shape manipulation. In ACM SIGGRAPH 2007
  </p>
  <p>
   papers, ACM, New York, NY, USA, SIGGRAPH 07.
  </p>
  <p>
   SUN, J., OVSJANIKOV, M., AND GUIBAS, L. 2009. A concise and
  </p>
  <p>
   provably informative multi-scale signature based on heat diffusion.
  </p>
  <p>
   In SGP, 13831392.
  </p>
  <p>
   SUN, J., OVSJANIKOV, M., AND GUIBAS, L. 2009. A concise and
  </p>
  <p>
   provably informative multi-scale signature based on heat diffusion.
  </p>
  <p>
   In Computer Graphics Forum, 13831392.
  </p>
  <p>
   SUNDAR, H., SILVER, D., N., G., AND DICKENSON, S. 2004.
  </p>
  <p>
   Skeleton based shape matching and retrieval. shape modeling
  </p>
  <p>
   international, 130139.
  </p>
  <p>
   SUNG, M., KIM, V. G., ANGST, R., AND GUIBAS, L. 2015.
  </p>
  <p>
   Data-driven structural priors for shape completion. ACM Trans.
  </p>
  <p>
   Graph..
  </p>
  <p>
   SURAZHSKY, V., SURAZHSKY, T., KIRSANOV, D., GORTLER,
  </p>
  <p>
   S. J., AND HOPPE, H. 2005. Fast exact and approximate
  </p>
  <p>
   geodesics on meshes. In ACM SIGGRAPH 2005 Papers, ACM,
  </p>
  <p>
   New York, NY, USA, SIGGRAPH 05, 553560.
  </p>
  <p>
   SUTTON, C., AND MCCALLUM, A. 2005. Piecewise training of
  </p>
  <p>
   undirected models. In Proc. of UAI.
  </p>
  <p>
   SWADZBA, A., AND WACHSMUTH, S. 2010. Indoor scene classification
  </p>
  <p>
   using combined 3D and gist features. In Proc. ACCV,
  </p>
  <p>
   201215.
  </p>
  <p>
   SZEGEDY, C., LIU, W., JIA, Y., SERMANET, P., REED, S.,
  </p>
  <p>
   ANGUELOV, D., ERHAN, D., VANHOUCKE, V., AND RABINOVICH,
  </p>
  <p>
   A. 2015. Going deeper with convolutions. In Proc.
  </p>
  <p>
   CVPR.
  </p>
  <p>
   SZELISKI, R., ZABIH, R., SCHARSTEIN, D., VEKSLER, O.,
  </p>
  <p>
   KOLMOGOROV, V., AGARWALA, A., TAPPEN, M., AND
  </p>
  <p>
   ROTHER, C. 2008. A comparative study of energy minimization
  </p>
  <p>
   methods for Markov random fields with smoothness-based priors.
  </p>
  <p>
   IEEE Trans. Pattern Anal. Mach. Intell. 30, 6, 10681080.
  </p>
  <p>
   TALTON, J. O., GIBSON, D., YANG, L., HANRAHAN, P., AND
  </p>
  <p>
   KOLTUN, V. 2009. Exploratory modeling with collaborative
  </p>
  <p>
   design spaces. ACM Trans. Graph. 28, 5.
  </p>
  <p>
   TALTON, J. O., GIBSON, D., YANG, L., HANRAHAN, P., AND
  </p>
  <p>
   KOLTUN, V. 2009. Exploratory modeling with collaborative
  </p>
  <p>
   design spaces. ACM Trans. Graph. 28, 5, 167:1167:10.
  </p>
  <p>
   TALTON, J. O., LOU, Y., LESSER, S., DUKE, J.,MECH, R., AND
  </p>
  <p>
   KOLTUN, V. 2011. Metropolis procedural modeling. ACM
  </p>
  <p>
   Trans. Graph. 30, 11:111:14.
  </p>
  <p>
   TALTON, J., YANG, L., KUMAR, R., LIM, M., GOODMAN, N.,
  </p>
  <p>
   AND MECH, R. 2012. Learning design patterns with bayesian
  </p>
  <p>
   grammar induction. 6374.
  </p>
  <p>
   TANGELDER, J. W. H., AND VELTKAMP, R. C. 2008. A survey
  </p>
  <p>
   of content based 3d shape retrieval methods. Multimedia Tools
  </p>
  <p>
   and Applications 39, 3, 441471.
  </p>
  <p>
   TANGELDER, J. W., AND VELTKAMP, R. C. 2008. A survey of
  </p>
  <p>
   content based 3d shape retrieval methods. Multimedia Tools and
  </p>
  <p>
   Applications 39, 3, 441471.
  </p>
  <p>
   THOMPSON, D. W. 1945. On Growth and Form. Cambridge
  </p>
  <p>
   University Press.
  </p>
  <p>
   TONG, Y., ALLIEZ, P., COHEN-STEINER, D., AND DESBRUN,
  </p>
  <p>
   M. 2006. Designing quadrangulations with discrete harmonic
  </p>
  <p>
   forms. In Proc. Symp. on Geom. Proc., SGP 06, 201210.
  </p>
  <p>
   TORRALBA, A., MURPHY, K. P., AND FREEMAN, W. T. 2004.
  </p>
  <p>
   Sharing features: efficient boosting procedures for multiclass object
  </p>
  <p>
   detection. In Proceedings of the 2004 IEEE computer society
  </p>
  <p>
   conference on Computer vision and pattern recognition, IEEE
  </p>
  <p>
   Computer Society, Washington, DC, USA, CVPR04, 762769.
  </p>
  <p>
   TORRALBA, A., MURPHY, K. P., AND FREEMAN, W. T. 2004.
  </p>
  <p>
   Sharing features: efficient boosting procedures for multiclass object
  </p>
  <p>
   detection. CVPR04, 762769.
  </p>
  <p>
   TORRALBA, A., MURPHY, K. P., AND FREEMAN, W. T. 2007.
  </p>
  <p>
   Sharing Visual Features for Multiclass and Multiview Object Detection.
  </p>
  <p>
   IEEE Trans. Pattern Anal. Mach. Intell. 29, 5.
  </p>
  <p>
   TORRALBA, A., FERGUS, R., AND FREEMAN, W. T. 2008. 80
  </p>
  <p>
   million tiny images: a large database for non-parametric object
  </p>
  <p>
   and scene recognition. IEEE Trans. Pat. Ana. &amp; Mach. Int. 30,
  </p>
  <p>
   11, 19581970.
  </p>
  <p>
   TRIMBLE, 2014. Trimble 3D warehouse.
  </p>
  <p>
   http://sketchup.google.com/3Dwarehouse/.
  </p>
  <p>
   TROPP, J. A. 2012. User-friendly tail bounds for sums of random
  </p>
  <p>
   matrices. Found. Comput. Math. 12, 4, 389434.
  </p>
  <p>
   TSANG, S., BALAKRISHNAN, R., SINGH, K., AND RANJAN, A.
  </p>
  <p>
   2004. A suggestive interface for image guided 3d sketching. In
  </p>
  <p>
   CHI, 591598.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   TSOLI, A., MAHMOOD, N., AND BLACK, M. J. 2014. Breathing
  </p>
  <p>
   life into shape: Capturing, modeling and animating 3d human
  </p>
  <p>
   breathing. ACM Trans. Graph. 33, 4, 52:152:11.
  </p>
  <p>
   TSOUMAKAS, G., AND KATAKIS, I. 2007. Multi-label classification:
  </p>
  <p>
   An overview. Int J Data Ware. and Mining 2007, 113.
  </p>
  <p>
   TUYTELAARS, T., TURINA, A., AND VAN GOOL, L. 2003. Noncombinatorial
  </p>
  <p>
   detection of regular repetitions under perspective
  </p>
  <p>
   skew. IEEE Trans. Pattern Anal. Mach. Intell. 25, 418432.
  </p>
  <p>
   UMETANI, N., IGARASHI, T., AND MITRA, N. J. 2012. Guided
  </p>
  <p>
   exploration of physically valid shapes for furniture design. ACM
  </p>
  <p>
   Trans. Graph. 31, 4.
  </p>
  <p>
   VALIANT, L. G. 1984. A theory of the learnable. Commun. ACM
  </p>
  <p>
   27, 11.
  </p>
  <p>
   VAN KAICK, O., TAGLIASACCHI, A., SIDI, O., ZHANG, H.,
  </p>
  <p>
   COHEN-OR, D., WOLF, L., , AND HAMARNEH, G. 2011. Prior
  </p>
  <p>
   knowledge for part correspondence. Computer Graphics Forum
  </p>
  <p>
   30, 2, 553562.
  </p>
  <p>
   VAN KAICK, O., TAGLIASACCHI, A., SIDI, O., ZHANG, H.,
  </p>
  <p>
   COHEN-OR, D., WOLF, L., AND HAMARNEH, G. 2011. Prior
  </p>
  <p>
   knowledge for shape correspondence. Computer Graphics Forum
  </p>
  <p>
   30, 2, 553562.
  </p>
  <p>
   VAN KAICK, O., ZHANG, H., HAMARNEH, G., AND COHEN-OR,
  </p>
  <p>
   D. 2011. A survey on shape correspondence. Comput. Graph.
  </p>
  <p>
   Forum 30, 6, 16811707.
  </p>
  <p>
   VAN KAICK, O., XU, K., ZHANG, H., WANG, Y., SUN, S.,
  </p>
  <p>
   SHAMIR, A., AND COHEN-OR, D. 2013. Co-hierarchical analysis
  </p>
  <p>
   of shape structures. ACM Trans. Graph. 32, 4, 69:110.
  </p>
  <p>
   VICENTE, S., KOLMOGOROV, V., AND ROTHER, C. 2010. Cosegmentation
  </p>
  <p>
   revisited: models and optimization. In Proceedings
  </p>
  <p>
   of the 11th European conference on Computer vision: Part II,
  </p>
  <p>
   Springer-Verlag, Berlin, Heidelberg, ECCV10, 465479.
  </p>
  <p>
   WAINWRIGHT, M. J., JAAKKOLA, T., AND WILLSKY, A. S.
  </p>
  <p>
   2005. MAP estimation via agreement on trees: message-passing
  </p>
  <p>
   and linear programming. IEEE Transactions on Information
  </p>
  <p>
   Theory 51, 11, 36973717.
  </p>
  <p>
   WAND, M., JENKE, P., HUANG, Q., BOKELOH, M., GUIBAS,
  </p>
  <p>
   L., AND SCHILLING, A. 2007. Reconstruction of deforming
  </p>
  <p>
   geometry from time-varying point clouds. In Proc. Symp. on
  </p>
  <p>
   Geom. Proc., 4958.
  </p>
  <p>
   WAND, M., ADAMS, B., OVSJANIKOV, M., BERNER, A.,
  </p>
  <p>
   BOKELOH, M., JENKE, P., GUIBAS, L., SEIDEL, H.-P., AND
  </p>
  <p>
   SCHILLING, A. 2009. Efficient reconstruction of nonrigid shape
  </p>
  <p>
   and motion from real-time 3d scanner data. ACM Trans. Graph.
  </p>
  <p>
   28, 2, 15:115:15.
  </p>
  <p>
   WANG, R. Y., AND POPOVIC
  </p>
  <p>
   , J. 2009. Real-time hand-tracking
  </p>
  <p>
   with a color glove. ACM Trans. Graph., 63:163:8.
  </p>
  <p>
   WANG, L., AND SINGER, A. 2013. Exact and stable recovery of
  </p>
  <p>
   rotations for robust synchronization. Information and Inference
  </p>
  <p>
   2, 2, 145193.
  </p>
  <p>
   WANG, H., WEXLER, Y., OFEK, E., AND HOPPE, H. 2008.
  </p>
  <p>
   Factoring repeated content within and among images. In ACM
  </p>
  <p>
   Trans. Graph., 14:114:10.
  </p>
  <p>
   WANG, Y., ASAFI, S., VAN KAICK, O., ZHANG, H., COHEN-OR,
  </p>
  <p>
   D., AND CHEN, B. 2012. Active co-analysis of a set of shapes.
  </p>
  <p>
   ACM Trans. Graph. 31, 6, 165:1165:10.
  </p>
  <p>
   WANG, F., HUANG, Q., AND GUIBAS, L. 2013. Image cosegmentation
  </p>
  <p>
   via consistent functional maps. In Proc. ICCV.
  </p>
  <p>
   WANG, Y., GONG, M., WANG, T., COHEN-OR, D., ZHANG, H.,
  </p>
  <p>
   AND CHEN, B. 2013. Projective analysis for 3d shape segmentation.
  </p>
  <p>
   ACM Trans. Graph. 32, 6, 192:1192:12.
  </p>
  <p>
   WANG, F., HUANG, Q., OVSJANIKOV, M., AND GUIBAS, L.
  </p>
  <p>
   2014. Unsupervised multi-class joint image segmentation. In
  </p>
  <p>
   Proc. CVPR.
  </p>
  <p>
   WANG, F., HUANG, Q., OVSJANIKOV, M., AND GUIBAS, L.
  </p>
  <p>
   2014. Unsupervised multi-class joint image segmentation. In
  </p>
  <p>
   Proc. CVPR.
  </p>
  <p>
   WATANABE, K., AND BELYAEV, A. G. 2001. Detection of salient
  </p>
  <p>
   curvature features on polygonal surfaces. Comput. Graph. Forum
  </p>
  <p>
   (Proc. Eurographics) 20, 3.
  </p>
  <p>
   WEI, L.-Y., LEFEBVRE, S., KWATRA, V., AND TURK, G. 2009.
  </p>
  <p>
   State of the art in example-based texture synthesis. In Eurographics
  </p>
  <p>
   State of the Art Report, 93117.
  </p>
  <p>
   WEI, L., HUANG, Q., CEYLAN, D., VOUGA, E., AND LI, H.
  </p>
  <p>
   2016. Dense human body correspondence using convolutional
  </p>
  <p>
   network. CVPR.
  </p>
  <p>
   WEINBERGER, K. Q., AND SAUL, L. K. 2009. Distance metric
  </p>
  <p>
   learning for large margin nearest neighbor classification. J.
  </p>
  <p>
   Mach. Learn. Res. 10, 207244.
  </p>
  <p>
   WEISE, T., LI, H., VAN GOOL, L., AND PAULY, M. 2009.
  </p>
  <p>
   Face/off: Live facial puppetry. In Proc. of Symp. on Comp.
  </p>
  <p>
   Anim., 716.
  </p>
  <p>
   WEISE, T., BOUAZIZ, S., LI, H., AND PAULY, M. 2011. Realtime
  </p>
  <p>
   performance-based facial animation. ACM Trans. Graph. 30, 4,
  </p>
  <p>
   77:177:10.
  </p>
  <p>
   WEN, Z., GOLDFARB, D., AND YIN, W. 2010. Alternating direction
  </p>
  <p>
   augmented Lagrangian methods for semidefinite programming.
  </p>
  <p>
   Math. Prog. Comput. 2, 3-4, 203230.
  </p>
  <p>
   WERNER, T. 2007. A linear programming approach to max-sum
  </p>
  <p>
   problem: A review. IEEE Trans. Pattern Anal. Mach. Intell. 29,
  </p>
  <p>
   11651179.
  </p>
  <p>
   WESSEL, R., BL UMEL, I., AND KLEIN, R. 2009. A 3d shape
  </p>
  <p>
   benchmark for retrieval and automatic classification of architectural
  </p>
  <p>
   data. In Eurographics 2009 Workshop on 3D Object Retrieval,
  </p>
  <p>
   5356.
  </p>
  <p>
   WILEY, D. F., AMENTA, N., ALCANTARA, D. A., GHOSH, D.,
  </p>
  <p>
   KIL, Y. J., DELSON, E., HARCOURT-SMITH, W., ROHLF,
  </p>
  <p>
   F. J., JOHN, K. S., AND HAMANN, B. 2005. Evolutionary
  </p>
  <p>
   morphing. In Proceedings of IEEE Visualization 2005.
  </p>
  <p>
   WINN, J., AND JOJIC, N. 2005. Locus: Learning object classes
  </p>
  <p>
   with unsupervised segmentation. In Proceedings of the Tenth
  </p>
  <p>
   IEEE International Conference on Computer Vision (ICCV05)
  </p>
  <p>
   Volume 1 - Volume 01, IEEE Computer Society, Washington,
  </p>
  <p>
   DC, USA, 756763.
  </p>
  <p>
   WONKA, P., WIMMER, M., SILLION, F., AND RIBARSKY, W.
  </p>
  <p>
   2003. Instant architecture. In ACM SIGGRAPH 2003 Papers,
  </p>
  <p>
   ACM, New York, NY, USA, SIGGRAPH 03, 669677.
  </p>
  <p>
   WU, J., AND KOBBELT, L. 2005. Structure recovery via hybrid
  </p>
  <p>
   variational surface approximation. Comput. Graph. Forum 24,
  </p>
  <p>
   3, 277284.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   WU, T.-P., TANG, C.-K., BROWN, M. S., AND SHUM, H.-Y.
  </p>
  <p>
   2007. Shapepalettes: Interactive normal transfer via sketching.
  </p>
  <p>
   ACM Trans. Graph. 26, 3.
  </p>
  <p>
   WU, T.-P., SUN, J., TANG, C.-K., AND SHUM, H.-Y. 2008.
  </p>
  <p>
   Interactive normal reconstruction from a single image. In ACM
  </p>
  <p>
   SIGGRAPH Asia 2008 Papers, SIGGRAPH Asia 08, 119:1
  </p>
  <p>
   119:9.
  </p>
  <p>
   WU, H.,WANG, Y.-S., FENG, K.-C.,WONG, T.-T., LEE, T.-Y.,
  </p>
  <p>
   AND HENG, P.-A. 2010. Resizing by symmetry-summarization.
  </p>
  <p>
   In ACM SIGGRAPH Asia 2010 papers, ACM, New York, NY,
  </p>
  <p>
   USA, SIGGRAPH ASIA 10, 159:1159:10.
  </p>
  <p>
   WU, Z., SONG, S., KHOSLA, A., YU, F., ZHANG, L., TANG, X.,
  </p>
  <p>
   AND XIAO, J. 2015. 3d shapenets: A deep representation for
  </p>
  <p>
   volumetric shapes. In Proc. CVPR.
  </p>
  <p>
   XIANG, Y., MOTTAGHI, R., AND SAVARESE, S. 2014. Beyond
  </p>
  <p>
   pascal: A benchmark for 3d object detection in the wild. In IEEE
  </p>
  <p>
   Winter Conference on Applications of Computer Vision (WACV).
  </p>
  <p>
   XIANG, Y., SONG, C., MOTTAGHI, R., AND SAVARESE, S. 2014.
  </p>
  <p>
   Monocular multiview object tracking with 3d aspect parts. In
  </p>
  <p>
   European Conference on Computer Vision (ECCV).
  </p>
  <p>
   XIE, X., XU, K.,MITRA, N. J., COHEN-OR, D., GONG, W., SU,
  </p>
  <p>
   Q., AND CHEN, B. 2013. Sketch-to-design: Context-based part
  </p>
  <p>
   assembly. Comp. Graph. Forum 32, 8, 233245.
  </p>
  <p>
   XIE, Z., XU, K., LIU, L., AND XIONG, Y. 2014. 3d shape segmentation
  </p>
  <p>
   and labeling via extreme learning machine. Computer
  </p>
  <p>
   Graphics Forum 33, 5.
  </p>
  <p>
   XIE, Z., XU, K., SHAN, W., LIU, L., XIONG, Y., AND HUANG,
  </p>
  <p>
   H. 2015. Projective feature learning for 3d shapes with multiview
  </p>
  <p>
   depth images. to appear.
  </p>
  <p>
   XIN, S.-Q., AND WANG, G.-J. 2009. Improving chen and hans
  </p>
  <p>
   algorithm on the discrete geodesic problem. ACM Trans. Graph.
  </p>
  <p>
   28, 104:1104:8.
  </p>
  <p>
   XU, K., ZHANG, H., TAGLIASACCHI, A., LIU, L., LI, G.,
  </p>
  <p>
   MENG, M., AND XIONG, Y. 2009. Partial intrinsic reflectional
  </p>
  <p>
   symmetry of 3d shapes. In ACM SIGGRAPH Asia 2009 papers,
  </p>
  <p>
   ACM, New York, NY, USA, SIGGRAPH Asia 09, 138:1
  </p>
  <p>
   138:10.
  </p>
  <p>
   XU, W., WANG, J., YIN, K., ZHOU, K., VAN DE PANNE, M.,
  </p>
  <p>
   CHEN, F., AND GUO, B. 2009. Joint-aware manipulation of
  </p>
  <p>
   deformable models. In ACM SIGGRAPH 2009 papers, ACM,
  </p>
  <p>
   New York, NY, USA, SIGGRAPH 09, 35:135:9.
  </p>
  <p>
   XU, K., LI, H., ZHANG, H., COHEN-OR, D., XIONG, Y., AND
  </p>
  <p>
   CHENG, Z.-Q. 2010. Style-content separation by anisotropic
  </p>
  <p>
   part scales. ACM Trans. Graph. 29, 5, 184:1184:10.
  </p>
  <p>
   XU, K., ZHENG, H., ZHANG, H., COHEN-OR, D., LIU, L., AND
  </p>
  <p>
   XIONG, Y. 2011. Photo-inspired model-driven 3d object modeling.
  </p>
  <p>
   ACM Trans. Graph. 30, 4, 80:180:10.
  </p>
  <p>
   XU, K., ZHANG, H., COHEN-OR, D., AND CHEN, B. 2012. Fit
  </p>
  <p>
   and diverse: Set evolution for inspiring 3d shape galleries. ACM
  </p>
  <p>
   Trans. Graph. 31, 4, 57:157:10.
  </p>
  <p>
   XU, K., CHEN, K., FU, H., SUN, W.-L., AND HU, S.-M. 2013.
  </p>
  <p>
   Sketch2scene: Sketch-based co-retrieval and co-placement of 3d
  </p>
  <p>
   models. ACM Trans. Graph. 32, 4, 123:112.
  </p>
  <p>
   XU, K., CHEN, K., FU, H., SUN, W.-L., AND HU, S.-M. 2013.
  </p>
  <p>
   Sketch2scene: Sketch-based co-retrieval and co-placement of 3d
  </p>
  <p>
   models. ACM Trans. Graph. 32, 4, 123:1123:12.
  </p>
  <p>
   XU, K., MA, R., ZHANG, H., ZHU, C., SHAMIR, A., COHENOR,
  </p>
  <p>
   D., AND HUANG, H. 2014. Organizing heterogeneous
  </p>
  <p>
   scene collections through contextual focal points. ACM Trans.
  </p>
  <p>
   Graph. 33, 4, 35:112.
  </p>
  <p>
   XU, W., SHI, Z., XU, M., ZHOU, K., WANG, J., ZHOU, B.,
  </p>
  <p>
   WANG, J., AND YUAN, Z. 2014. Transductive 3d shape segmentation
  </p>
  <p>
   using sparse reconstruction. Comp. Graph. F. 33, 5.
  </p>
  <p>
   XU, K., HUANG, H., SHI, Y., LI, H., LONG, P., CAICHEN, J.,
  </p>
  <p>
   SUN, W., AND CHEN, B. 2015. Autoscanning for coupled scene
  </p>
  <p>
   reconstruction and proactive object analysis. ACM Trans. Graph.
  </p>
  <p>
   34, 6, 177:1177:14.
  </p>
  <p>
   XU, K., KIM, V., Q. HUANG, N. M., AND KALOGERAKIS, E.,
  </p>
  <p>
   2016. Wikipage: Data-driven shape analysis and processing.
  </p>
  <p>
   http://wp.cs.umass.edu/datadrivenshape/.
  </p>
  <p>
   YAN, D.-M., LIU, Y., AND WANG, W. 2006. Quadric surface
  </p>
  <p>
   extraction by variational shape approximation. In Proc. GMP,
  </p>
  <p>
   vol. 4077, 7386.
  </p>
  <p>
   YANG, L., AND JIN, R. 2006. Distance metric learning: A comprehensive
  </p>
  <p>
   survey.
  </p>
  <p>
   YAO, B., KHOSLA, A., AND FEI-FEI, L. 2011. Combining
  </p>
  <p>
   randomization and discrimination for fine-grained image categorization.
  </p>
  <p>
   In Proc. CVPR, 15771584.
  </p>
  <p>
   YAO, B., KHOSLA, A., AND FEI-FEI, L. 2011. Combining
  </p>
  <p>
   randomization and discrimination for fine-grained image categorization.
  </p>
  <p>
   In Proc. CVPR.
  </p>
  <p>
   YE, Y. 1991. An o(n3l) potential reduction algorithm for linear
  </p>
  <p>
   programming. Math. Program. 50, 239258.
  </p>
  <p>
   YI, L., KIM, V. G., CEYLAN, D., SHEN, I.-C., YAN, M., SU, H.,
  </p>
  <p>
   LU, C., HUANG, Q., SHEFFER, A., AND GUIBAS, L. 2016.
  </p>
  <p>
   A scalable active framework for region annotation in 3d shape
  </p>
  <p>
   collections. SIGGRAPH Asia.
  </p>
  <p>
   YU, K., AND NG, A. 2010. Feature learning for image classification.
  </p>
  <p>
   In ECCV tutorials.
  </p>
  <p>
   YUAN, M., AND LIN, Y. 2006. Model selection and estimation in
  </p>
  <p>
   regression with grouped variables. Journal of the Royal Statistical
  </p>
  <p>
   Society, Series B 68, 4967.
  </p>
  <p>
   YUAN, M., AND LIN, Y. 2006. Model selection and estimation in
  </p>
  <p>
   regression with grouped variables. Journal of the Royal Statistical
  </p>
  <p>
   Society, Series B 68, 4967.
  </p>
  <p>
   YUMER, M. E., AND KARA, L. B. 2012. Co-abstraction of shape
  </p>
  <p>
   collections. ACM Trans. Graph. 31, 6, 166:1166:11.
  </p>
  <p>
   YUMER, M. E., AND KARA, L. B. 2012. Co-abstraction of shape
  </p>
  <p>
   collections. ACM Trans. Graph. 31.
  </p>
  <p>
   YUMER, M., AND KARA, L. 2014. Co-constrained handles for
  </p>
  <p>
   deformation in shape collections. ACM Trans. Graph. 32, 6, to
  </p>
  <p>
   appear.
  </p>
  <p>
   YUMER, M. E., AND MITRA, N. J. 2016. Learning semantic
  </p>
  <p>
   deformation flows with 3d convolutional networks. In European
  </p>
  <p>
   Conference on Computer Vision (ECCV 2016), Springer, .
  </p>
  <p>
   YUMER, M. E., CHUN, W., AND MAKADIA, A. 2014. Cosegmentation
  </p>
  <p>
   of textured 3d shapes with sparse annotations. In
  </p>
  <p>
   Proc. CVPR.
  </p>
  <p>
   YUMER, M. E., CHAUDHURI, S., HODGINS, J. K., AND KARA,
  </p>
  <p>
   L. B. 2015. Semantic shape editing using deformation handles.
  </p>
  <p>
   ACM Trans. Graph. 34.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
   ZACH, C., KLOPSCHITZ, M., AND POLLEFEYS, M. 2010. Disambiguating
  </p>
  <p>
   visual relations using loop constraints. In Proc. CVPR.
  </p>
  <p>
   ZAHARESCU, A., BOYER, E., VARANASI, K., AND HORAUD, R.
  </p>
  <p>
   2009. Surface feature detection and description with applications
  </p>
  <p>
   to mesh matching. In Proc. CVPR [DBL 2009], 373380.
  </p>
  <p>
   ZEILER, M. D., AND FERGUS, R. 2014. Visualizing and understanding
  </p>
  <p>
   convolutional neural networks. In Proc. ECCV.
  </p>
  <p>
   ZHA, Z.-J., MEI, T., WANG, J., WANG, Z., AND HUA, X.-S.
  </p>
  <p>
   2009. Graph-based semi-supervised learning with multiple labels.
  </p>
  <p>
   J. Vis. Comun. Image Represent. 20, 2, 97103.
  </p>
  <p>
   ZHANG, R., TSAI, P.-S., CRYER, J. E., AND SHAH, M. 1999.
  </p>
  <p>
   Shape from shading: A survey. IEEE Trans. Pattern Anal. Mach.
  </p>
  <p>
   Intell. 21, 8, 690706.
  </p>
  <p>
   ZHANG, E., MISCHAIKOW, K., AND TURK, G. 2005. Featurebased
  </p>
  <p>
   Surface Parameterization and Texture Mapping. ACM
  </p>
  <p>
   Trans. Graph. 24, 1.
  </p>
  <p>
   ZHANG, H., SHEFFER, A., COHEN-OR, D., ZHOU, Q., VAN
  </p>
  <p>
   KAICK, O., AND TAGLIASACCHI, A. 2008. Deformationdriven
  </p>
  <p>
   shape correspondence. Comput. Graph. Forum 27, 5,
  </p>
  <p>
   14311439.
  </p>
  <p>
   ZHANG, Y., XU, W., TONG, Y., AND ZHOU, K. 2014. Online
  </p>
  <p>
   structure analysis for real-time indoor scene reconstruction.
  </p>
  <p>
   ACM Trans. Graph..
  </p>
  <p>
   ZHAO, X., WANG, H., AND KOMURA, T. 2014. Indexing 3d
  </p>
  <p>
   scenes using the interaction bisector surface. ACM Trans. Graph.
  </p>
  <p>
   33, 3, 22:122:15.
  </p>
  <p>
   ZHOU, K., HUANG, X., WANG, X., TONG, Y., DESBRUN, M.,
  </p>
  <p>
   GUO, B., AND SHUM, H.-Y. 2006. Mesh quilting for geometric
  </p>
  <p>
   texture synthesis. In ACM SIGGRAPH 2006 Papers, ACM, New
  </p>
  <p>
   York, NY, USA, SIGGRAPH 06, 690697.
  </p>
  <p>
   ZHOU, S., FU, H., LIU, L., COHEN-OR, D., AND HAN, X. 2010.
  </p>
  <p>
   Parametric reshaping of human bodies in images. ACM Trans.
  </p>
  <p>
   Graph. 29, 4, 126:1126:10.
  </p>
  <p>
   ZHU, L., ZHOU, Z., AND HU, D. 2008. Globally consistent reconstruction
  </p>
  <p>
   of ripped-up documents. IEEE Trans. Pattern Anal.
  </p>
  <p>
   Mach. Intell. 30, 1, 113.
  </p>
  <p>
   ZHU, X. 2006. Semi-supervised learning literature survey. Computer
  </p>
  <p>
   Sciences TR 1530, University of Wisconsin Madison.
  </p>
  <p>
   ZIA, M. Z., STARK, M., SCHIELE, B., AND SCHINDLER, K.
  </p>
  <p>
   2013. Detailed 3d representations for object recognition and
  </p>
  <p>
   modeling. IEEE Trans. Pat. Ana. &amp; Mach. Int. 35, 11, 2608
  </p>
  <p>
   2623.
  </p>
  <p>
  </p>
  <p>
  </p>
 </body>
</html>
